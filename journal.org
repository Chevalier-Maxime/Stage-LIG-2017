# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:       Journal de bord
#+AUTHOR:      Maxime Chevalier 
#+LANGUAGE:    fr
#+TAGS: LIG(L) SimGrid(S) PSI3(P) CODES(C) ROSS(O) Space(A) Time(T)
#+TAGS: R(R) OrgMode(M) Deprecated(D) DUMPI(U) HPL(H)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* 2016
** 2016-03 March
*** 2016-03-07 Monday
**** [[https://github.com/alegrand/RR_webinars/blob/master/1_replicable_article_laboratory_notebook/index.org][First webinar on reproducible research: litterate programming]]
***** Emacs shortcuts
Here are a few convenient emacs shortcuts for those that have never
used emacs. In all of the emacs shortcuts, =C=Ctrl=, =M=Alt/Esc= and
=S=Shift=.  Note that you may want to use two hours to follow the emacs
tutorial (=C-h t=). In the configuration file CUA keys have been
activated and allow you to use classical copy/paste (=C-c/C-v=)
shortcuts. This can be changed from the Options menu.
  - =C-x C-c= exit
  - =C-x C-s= save buffer
  - =C-g= panic mode ;) type this whenever you want to exit an awful
    series of shortcuts
  - =C-Space= start selection marker although selection with shift and
    arrows should work as well
  - =C-l= reposition the screen
  - =C-_= (or =C-z= if CUA keys have been activated)
  - =C-s= search
  - =M-%= replace
  - =C-x C-h= get the list of emacs shortcuts
  - =C-c C-h= get the list of emacs shortcuts considering the mode you are
    currently using (e.g., C, Lisp, org, ...)
  There are a bunch of cheatsheets also available out there (e.g.,
  [[http://www.shortcutworld.com/en/linux/Emacs_23.2.1.html][this one for emacs]] and [[http://orgmode.org/orgcard.txt][this one for org-mode]] or this [[http://sachachua.com/blog/wp-content/uploads/2013/05/How-to-Learn-Emacs-v2-Large.png][graphical one]]).
***** Org-mode                                                  :OrgMode:
  Many emacs shortcuts start by =C-x=. Org-mode's shortcuts generaly
  start with =C-c=.
  - =Tab= fold/unfold
  - =C-c c= capture (finish capturing with =C-c C-c=, this is explained on
    the top of the buffer that just opened)
  - =C-c C-c= do something useful here (tag, execute, ...)
  - =C-c C-o= open link
  - =C-c C-t= switch todo
  - =C-c C-e= export
  - =M-Enter= new item/section
  - =C-c a= agenda (try the =L= option)
  - =C-c C-a= attach files
  - =C-c C-d= set a deadl1ine (use =S-arrows= to navigate in the dates)
  - =A-arrows= move subtree (add shift for the whole subtree)
***** Org-mode Babel (for literate programming)                 :OrgMode:
  - =<s + tab= template for source bloc. You can easily adapt it to get this:
      : #+BEGIN_SRC sh
      : ls
      : #+END_SRC
    Now if you =C-c C-c=, it will execute the block.
    #+BEGIN_EXAMPLE
  #+RESULTS:
  | #journal.org# |
  | journal.html  |
  | journal.org   |
  | journal.org~  |
    #+END_EXAMPLE
  
  - Source blocks have many options (formatting, arguments, names,
    sessions,...), which is why I have my own shortcuts =<b + tab= bash
    block (or =B= for sessions).
    #+BEGIN_EXAMPLE 
  #+begin_src sh :results output :exports both
  ls /tmp/*201*.pdf
  #+end_src

  #+RESULTS:
  : /tmp/2015_02_bordeaux_otl_tutorial.pdf
  : /tmp/2015-ASPLOS.pdf
  : /tmp/2015-Europar-Threadmap.pdf
  : /tmp/europar2016-1.pdf
  : /tmp/europar2016.pdf
  : /tmp/M2-PDES-planning-examens-janvier2016.pdf
    #+END_EXAMPLE
  - I have defined many such templates in my configuration. You can
    give a try to =<r=, =<R=, =<RR=, =<g=, =<p=, =<P=, =<m= ...
  - Some of these templates are not specific to babel: e.g., =<h=, =<l=,
    =<L=, =<c=, =<e=, ...
***** In case you want to play with ipython on a recent debian   :Python:
Here is what you should install:
#+begin_src sh :results output :exports both
sudo apt-get install python3-pip ipython3 ipython3-notebook python3-numpy python3-matplotlib
#+end_src

The ipython notebook can then be run with the following command:
#+begin_src sh :results output :exports both
ipython3 notebook
#+end_src

The latest version of this notebook is called [[http://jupyter.org/][Jupyter]] and is polyglot
like babel. Playing with it is easy as it's deployed on the cloud but
as I'm not a python expert I'm not sure to know how to deploy it locally.
***** In case you want to play with R/knitR/rstudio:                  :R:
Here is what you should install on debian:
#+BEGIN_SRC sh
sudo apt-get install r-base r-cran-ggplot2
#+END_SRC

Rstudio and knitr are unfortunately not packaged within debian so the
easiest is to download the corresponding debian package on the [[http://www.rstudio.com/ide/download/desktop][Rstudio
webpage]] and then to install it manually (depending on when you do
this, you can obviously change the version number).
#+BEGIN_SRC sh
wget https://download1.rstudio.org/rstudio-0.99.887-amd64.deb
sudo dpkg -i rstudio-0.99.887-amd64.deb
sudo apt-get -f install # to fix possibly missing dependencies
#+END_SRC
You will also need to install knitr. To this end, you should simply
run R (or Rstudio) and use the following command.
#+BEGIN_SRC R
install.packages("knitr")
#+END_SRC
If =r-cran-ggplot2= could not be installed for some reason, you can also
install it through R by doing:
#+BEGIN_SRC R
install.packages("ggplot2")
#+END_SRC

As you will experience, knitr is polyglot but not Rstudio, which
makes its use not as fluid when using other languages than R.
* 2017
** 2017-05 mai
*** 2017-05-09 mardi
**** DONE Lecture papiers [3/3]                                   :ATTACH:
:PROPERTIES:
:Attachments: PDS_fujimoto2015.pdf
:ID:       a3acf95b-21d5-4621-a955-41bab99b38f6
:END:
- [X] simulation «[[file:Papiers/Timeparallelsimulation.pdf][temps parallèle]]» (extrait du livre de
      Fujimoto, 2000)
  - *Space parallel approch* : C'est la decomposition horizontale d'un
    programme, où chaque process maintient ses variables d'état tout au long de la simulation.
    - Plus flexible, et plus applicable (dans la plupart des cas).
  - *Time parallel approch* :
    - Propriétés : Parallélisme massif, indépendance des processeurs
      logiques (moins de synchronisation)
    - Approches : comment déterminer l'état initial des process (sauf le premier process), il faut corriger la trajectoire.
      - *Fix-up computation* : Etat initial random, qui ne match presque
        jamais, donc il faut faire une correction avec une seconde
        simulation (et ca fonctionne grâce à la contraction des
        ensembles de trajectoires) prenant en état initial l'état
        final du process précédant. Et quand ca match, c'est gagné!
        Fonction le mieux lorsque l'état final ne dépend pas de l'état
        initial (2 itérations et c'est fini). Lorsque ce n'est pas le
        cas, (pire cas) il faut pour N intervals, N itérations, soit
        du sequentiel avec des ressources pour du parallèle
        (bad!). *Exemple ou ca fonctionne* cache LRU.
      - *Precomputation of state at specific time division points* : Il
        se peut que dans certains programmes on puisse déterminer des
        points spécifiques (débordement de buffer par exemple) qui
        vont arriver et donc en faire des états de départ. *Exemple* :
        buffer avec N entrées de capacité 1 et une sortie de capacité C.
        - Soit il y a overflow
        - Soit il y a underflow
        - Du coup chaque processus détermine si il part d'un
          over/under et effectue sa simulation pour X services. Quand
          il a fini, il envoie le reste de sa file d'attente au
          processus d'après qui se charge de simuler l'interval entre
          l'état d'arrêt du processus précédent, et son postulat de départ.
        - À la fin, on peut calculer le nombre de service et de perte
          en additionnant les résultats de chaque processus.
        - Cependant ca pet échouer car les processus peuvent ne pas
          atteindre l'objectif (overflow souvent car c'est très
          rare). Il est donc très difficile d'identifier les points de division.
      - *Parallel prefix computations* :
        - Utilisation de récurence car il existe de très bon
          algorithmes pour faire du calcul prefix sur des systèmes parallèles.
        - Il faut faire attention au ratio nbCalculs/nbProcesseurs. En
          effet, si ce ratio tend vers 1, il y a des pertes à cause du
          temps de communication entre les processeurs (qui est du
          temps de calcul perdu).
        - /!\ Je n'ai pas compris l'exemple de cette partie (page 9 et 10).
    - Plutôt utilisé pour développer des algo de simulation parallèle
      spécifique.
    - Permet de parralléliser des cas où le space parralélisme ne
      fonctionne pas (ou peu), comme dans les exemples.
  - Dans certain cas, il est possible de mélanger les deux approches.

- [X] Et un [[file:Papiers/PDS_fujimoto2015.pdf][autre]] Fujimoto (2015) pour avoir une vision d’ensemble sur la
      simulation parallèle (sauf sur la technique «  temps-parallèle »  qui
      n’est pas mentionnée dans cet article)
  - Lecture :
    - *PDES* : Parallel Discrete Event Simulation. But : accelerer
      l'execution des simulation. Les "pas" de simulations sont
      irrégulier. Peut être vu comme une collection d'evenements
      sequentiels discret, qui interagissent par des messages
      datés. Un PDES doit fournir les mêmes résutats que le programme
      en séquentiel (ou parfois aproximativement les mêmes). Le
      principal problèlme c'est la synchrinisation entre les
      differents processus.

    - *Simulation distribué* : concerne l'execution de simulation sur
      des machines séparé "géographiquement", où l'agrégation de
      simulation pour un environnement de simulation (*exemple*
      simulateur de vol,...). Synchronisation = time management. De
      gros efforts ont été fait pour developer des standards
      d'interconnection de simulation (DIS,HLA) qui, entre autre
      réduisent le nombre de message envoyé.

    - *Synchronisation conservative* : Permet d'avoir le même résultat
      qu'une exécution séquentielle sur un processeur monocœur. Cette
      propriété peut être montré en s'assurant que chaque processus
      logique traite les messages par leurs dates.

      - Première génération : Pour qu'il y ait du parallèlisme, il
        faut cependant que certains LP (processus logique) puissent
        prendre de l'avance. Pour cela il faut s'assurer que l'on ne
        recevera pas d'event avec un horadatage plus petit que celui
        qu'on va executer (*ALGO CMB). *Contraintes* : réseau FIFO. Tant qu'il y a
        des messages dans les buffers le LP à l'information. Si le
        buffer est vide, il doit se bloquer /!\ il peut y avoir des
        interblocages. Pour les éviter les LPs envoient des messages
        nulls qui garantissent les horodatages. Les LP ont aussi un
        *lookahead* qui defini le temps min avant l'envoie d'un autre
        message (l'horadatage du prochain evenement doit être bien
        plus grand que le temps actuel). Cependant, si il est trop petit, de nombreux messages
        inutiles peuvent être envoyé si il y a blocage (cf exemple
        fin page 4).

      - Seconde génération : Dans un premiers temps, les nouveaux algo
        ont tenté de supprimer le problème de lookahead en utilisant
        le plus petit horadatage d'event dans le système, pour passer
        directement à la suite. D'autres approche complétement
        différentes de CMB ont été tenté, avec par exemple YAWNS qui
        utilise des points globaux de synchronisation (des
        barrières). *Schéma lookahead* question : Je suis pas sûr de
        comprendre comment ca marche... *Réponse* : SimGrid ne
        fonctionne pas sur ce modèle. C'est juste dire aux autres LPs
        que le LP ne va pas envoyer de message avant T+L, et qu'ils
        peuvent donc process jusque là.

    - *Synchronisation optimiste* : Les events ne sont plus éxécutés
      strictement dans l'ordre.(*TIME WARP*)contrairement aux algorithmes
      précédents, les erreurs sont possible, mais un mécanisme de
      rollback permet de retourner dans un état sain grâce à deu
      mécanismes :

      - *Mécanisme de control local* : Si un LP recoit un event daté de
        50 et qu'il en est à 100, il va faire un rollback
        jusqu'a 50. Cependant, si il y a eu modification des variables
        d'état, ou envoie de message ca ne peut pas être annulé. Pour
        les variables d'état il est possible de les sauvegarder avant
        modification (*copy state saving*) qui a un *cout* en temps de
        copie et en mémoire) ou faire de *l'incremental state saving*
        qui sauvegarder seulement les infos qu'on a modifié mais qui
        nécessite d'avoir les adresses des variables (à
        modifier/backup). Une autre technique est le *reverse
        computation* (si un event fait +1, on fait -1) qui est la plus
        avantageuse des 3, mais ca peut de
        pas être réalisable. Pour les messages, un système anti
        message récursif a été mis au point. On r'envoie le message
        identique avec un marqueur, si il a pasété traité, il est
        supprimé de la file, sinon il y a rollback pour le LP qui
        recoit l'anti-message (et ainsi de suite, pouvant conduire à
        des rollbacks en cascade /!\)

      - *Mécanisme de contrôle global* : GVT : Pas compris. C'est un
        état global, mais j'ai pas compris comment il est utilisé/ce
        qui est sauvegardé. Mécanisme de coupe comme vu en algo
        distribué (RICM4). Ok donc GVT c'est une barrière dans le
        temps avant laquelle on ne peut pas revenir lors d'un
        rollback. Du coup, il est possible de libérer les ressources
        utilisé pour sauvegarder les états avant cette barrière.
        https://www.acm-sigsim-mskr.org/Courseware/Fujimoto/Slides/FujimotoSlides-12-ComputingGlobalVirtualTime.pdf

        - D'autres pistes sont envisagées, comme limiter dans une
          fenêtre de temps les LPs (ce qui empêche certains LP d'être
          trop en avance et de faire beaucoup de rollback). Une autre
          technique et de retarder les envoies de messages jusqu'a ce
          que ce soit garantie que l'envoie ne va pas être rollback
          plus tard.

- [X] Les outils : 
  - [X] SimGrid un simulateur d’applications parallèles 
    http://simgrid.gforge.inria.fr (présentation générale http://simgrid.gforge.inria.fr/tutorials/simgrid-101.pdf)

    - SimDag : Framework pour application parralèles. Les tâches avec
      dépendances limitent la parralèlisme. On fourni à SimDag le
      graphe des tâches et ça retourne des informations sur les
      machines qui ont fait tels taches.

    - MPI : couche d'abstracton adapté à la programation parallèle
      (partie communication).

    - SMPI : Simule des application // avec MPI. Réalise le calcul
      pour de vrais en simulant l'architecture, mais en séquentiel. LE
      but c'est pas forcément le résultat, mais plutôt de voir si
      l'algorithme fonctionne bien sur l'architecture choisie.

    - MSG : Visualisation de données

    - Fonctionne comme en RO pour simuler les architechtures.

    - Permet plusieurs approches :

      - On-line : simulation sur sa propre machine. Mais /!\ à la
        topologie logique (grille par ex) qui est ensuite mappé sur
        une topologie physique différente. SimGrid peut également
        simuler cette différence.

      - Off-line : Execution du programme sur un super-calculateur
        puis récupération des traces. Avec ces trâces, rejouer avec
        des topologies différentes. Cependant cette approche nécéssite
        d'avoir accès à un super-calculateur, mais est limité pas la
        taille initiale de la topologie (on peut difficilement
        extrapoler). De plus, pour pouvoir jouer avec les trâce sur
        des configurations différentes, il faut que le code soit
        déterministe, hors il ne l'est pas nécessairement.

  - [X] PSi3 : Perfect simulator. S'arrête à la districbution
    stationnaire /!\ coût élevé
    - [X] un simulateur [développé par Inria] contenant une version
      parallèle « à la Fujimoto/Nicol » 
      https://gforge.inria.fr/projects/psi/ dédié aux files d'attentes
      réseaux (en temps discret). Le logiciel fournis des traces
      d'éxecution qui sont ensuite utilisable pour analyse (avec R par
      exemple). Ressemble beaucoup aux TD/DM de EP (les exemples).
    - [X]  [[file:Papiers/PDS_fujimoto2015.pdf][slides de présentation ci-joints (Briot/Vincent)]] Simulation
      de chaine de Markov, et utilisation des processus de vie et de
      mort. Le nombre d'etat doit être inferieur à 10⁷ pour les
      calculs numériques. Pour les simulations le cout est
      linéaire. Le but est de générer beaucoup d'exemples grâce un
      script de simulation pour ensuite les analyser. Psi3 permet de
      faire des statistiques sur les réseaux à file finie sur des
      évènements rare : rejet, blocage, ... avec la garantie
      d'indépendance entre les traces. La simulation utilises
      plusieurs coeurs (framework OpenMP cf RICM3). Il y a plusieurs
      approches pour la simulation :
      - Space //, Time // et les deux

      - Backward simulation : chaque coeur fait une trace.

      - Forward simulation *?? question*

      - Peret la //isation des entrées sorties.
  - [X] CODES : un simulateur [développé par Argonne] un simulateur
                parallèle d’applications parallèles
                https://press3.mcs.anl.gov/codes/ basé sur le
    framework ROSS. Ce simulateur a pour but d'augmenter la
    parallélisation sur du stackage de masse et pour les applications
    qui utilisent beaucoups de données. Dans le but de tester des
    architectures Exascale qui n'existe pas encore. Ca permet de tester des
    algorithmes avec une simulation de réseau faite par ROSS et des
    simulations d'I/O modélisé par CODE. Avec ce simulateur on peut
    mettre en évidence des goulots, évaluer la tolérance aux fautes,
    la mise à l'échelle et les techniques de
    "recovery". L'architechture TORUS (architecture réseau à plusieurs
    dimmentions) a été directement ajouté dans CODES et non dans
    ROSS. On peut donc ajouter nos propres modèles. Voici en
    attachement 2 articles présentant CODES
    -[X] [[file:Papiers/P1884.pdf][1]] 
    - 4.2 - Figure 3 : pas compris les résultats de l'expérience. 
    -[X] [[file:Papiers/LiuCarothers2012.pdf][2]] Possibilité de s'inspirer de ce papier pour la démarche sur
    des études
  - [X] ROSS :  simulateur parallèle, lui-même basé sur le
                mécanisme Time Warp 
                http://carothersc.github.io/ROSS/about.html . La
    simulation est basé sur des LPs, chacun modélisant un composant du
    système. Pour communiquer, les LPs utilisent des message horodaté.
                https://github.com/carothersc/ROSS. 
                
  - [X] Time Warp : Pour Time Warp il y a l’[[file:Papiers/Jefferson87.pdf][article]] de Jefferson et
                    al. qui résume à peu près. Utilisation d'un temps
    virtuel pour la synchronisation (Jefferson, cf Cours AD
    RICM4). Principe du rollback cité plus haut. Time Warp est à la
    base un OS, car le faire au dessus d'un autre OS n'ayant pas la
    même logique doublé tous les éléments (scheduling, I/O, ...). Time
    Warp est destiné aux applications utilisant un temps logique ou un
    temps de simulation dans leur modèle. La condition FIFO n'est plus
    nécessaire même si elle est préférable. Il y a régulierement des
    sauvegarde globales du système (de type GVT je suppose).
    - Anti-message : Comme vu plus haut, mais plus détailé. Chaque
      message envoyé engendre un anti-message dans le "buffer"
      d'emission qui n'est pas envoyé. Cependant, si il y a un
      rollback, le LP reprend son exécution et à chaque envoie de
      message, il compare avec les anti-message : si il y a un même
      message anti + nouveau, les deux sont supprimé (on a déjà envoyé
      le message), si il n'y a pas correspondance, un anti-message du
      nouveau est conservé. Tous les anti-message non représenté
      durant la réexecution doivent être annulé chez les autres
      LPs. Ensuite c'est l'algo en cascade.

    - La communication entre les processus ne se fait pas via des
      pipe,... il n'est donc pas nécéssaire de déclarer quel process
      communique avec quel process. Il suffi de faire un
      send. Question : != entre QueryMessage et EventMessage

    - Structure d'un process : cf page 10

    - Les processus s'executent qu'a la réception de message et font
      le traitement, ils ne font pas de calcul en dehors. Ils peuvent
      cependant s'envoyer eux-même des messages. Les processus doivent
      être deterministe (l'execution d'une même entrée doit donner la
      même sortie pour eviter les rollback en cascade. Pour les nombre
      aléa jouer avec la seed).

    - Utilisation de beaucoup de mémoire pour le GVT et pour la
      sauvegarde des anti-messages. Si il y a plus de place, le plus
      grand message (T dans le système, le plus loin dans le futur) est désenvoyé ce qui cause un rollback quelque
      part et donc libère de la place. Le méssage sera réenvoyé plus tard.

  - [X] Slide d'Arnaud sur les infrastructures de calcul :
   http://mescal.imag.fr/membres/arnaud.legrand/blog/2015/03/25/intervention_isn-gz.pdf
    - Tentative de mélanger les calculs et les communication pour
      réduire les barrières (MUMPS)

    - Explication simple d'un calcul de pivot sur une matrice
      diagonale.



**** Réunion Maxime Chevalier, simulation parallèle
- Lu chapitre simulation time // de Fujimoto
- A lu papier sur SG, PSI, CODES/ROSS (torus)
- A commencé à intégrer les notions de simulation time //, space //,
  simulation parfaite, simulation de Monte Carlo, à évènements
  discrets (pas forcément de chaine de Markov).
- À faire:
  - Journal:
    - Créer un repos github privé et nous donner les droits
      (alegrand, fperronnin).
    - On vérifie la config org-mode
  - Simulation:
    - Installer CODES/ROSS
    - Récupérer une trace de HPL (générée "en vrai" ou avec SimGrid)
    - Rejouer cette trace au dessus de CODES en séquentiel
    - Rejouer cette trace au dessus de SimGrid (en séquentiel donc)
    - Rejouer cette trace au dessus de CODES en multi-core
    - Rejouer cette trace au dessus de CODES en distribué (MPI)
    - Puis on avise pour le time //
**** Premiers pas avec org-mode
#+begin_src sh :results output :exports both
ls /tmp/
#+end_src

#+RESULTS:
: babel-58401Ra
: babel-8460yua
: emacs8460hfp
: systemd-private-309cd01bea0a4d7cbb3d3706e339b36a-colord.service-6SHoQ5
: systemd-private-309cd01bea0a4d7cbb3d3706e339b36a-rtkit-daemon.service-DraqAW
: tracker-extract-files.1000

#+begin_src python :results output :exports both
print("Hello")
#+end_src

#+RESULTS:
: Hello

#+begin_src R :results output :session *R* :exports both
summary(cars)
#+end_src

#+BEGIN_EXAMPLE
This seminar took place on March 7, 2016. The link to the video is below. Do not forget to check the section on software installation if you want to test yourself the demoed tools.

Table of Contents
#+END_EXAMPLE

**** Réunion avec Florence et Arnaud
***** DONE Mettre en place mon cahier de laboratoire [3/3]
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-05-09 mar. 15:05]
- State "TODO"       from "TODO"       [2017-05-09 mar. 15:05]
- State "TODO"       from "TODO"       [2017-05-09 mar. 13:54]
- State "TODO"       from "STARTED"    [2017-05-09 mar. 13:54]
- State "STARTED"    from "TODO"       [2017-05-09 mar. 13:52]
- State "TODO"       from              [2017-05-09 mar. 13:52]
:END:

- [X] github
- [X] org-mode
- [X] intégrer le mail

Entered on [2017-05-09 mar. 13:45]
******
*** 2017-05-11 jeudi
**** Fin des lectures
**** Installation CODES/ROSS sur ma machine
***** DEFERRED ROSS                                                :ROSS:
:LOGBOOK:
- State "DEFERRED"   from              [2017-05-15 lun. 13:35]
- State "DEFERRED"   from "CANCELLED"  [2017-05-15 lun. 13:35]
- State "CANCELLED"  from "DONE"       [2017-05-15 lun. 13:35]
- State "DONE"       from "APPT"       [2017-05-15 lun. 13:35]
- State "APPT"       from "WAITING"    [2017-05-15 lun. 13:35]
- State "STARTED"    from "TODO"       [2017-05-11 jeu. 17:51]
- State "TODO"       from              [2017-05-11 jeu. 17:51]
:END:
Lien Git : https://github.com/carothersc/ROSS
Requirement  : 
- C compiler (C11 prefered but not required)
- CMake : https://cmake.org (2.8 min) -> 3.6.2
#+begin_src sh
sudo yum install cmake
#+end_src
- Implémentation MPI : MPICH recommandé -> 3.2
#+begin_src sh
sudo yum install mpich
#+end_src
Machine : Fedora 25, gcc version 6.3.1, intel core i5, 8Go de RAM
****** Tentative installation 1
Ca ne fonctionne pas en suivant les étapes du git, je supprime le
dossier et je passe par un autre tuto.
****** Tentative d'installation 2
https://github.com/carothersc/ROSS/wiki/Installation
En fait j'ai une erreur dans la configuration de cmake...
#+BEGIN_EXAMPLE
CMake Error at /usr/share/cmake/Modules/CMakeDetermineCCompiler.cmake:57 (message):
  Could not find compiler set in environment variable CC:

  mpicc.
Call Stack (most recent call first):
  CMakeLists.txt:1 (PROJECT)


CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage
-- Configuring incomplete, errors occurred!
See also "/home/chevamax/Documents/ross-build/CMakeFiles/CMakeOutput.log".
#+END_EXAMPLE

(lundi 15 mai)
En fait mpich n'était pas dans mon PATH...
On retante.
Ca ne marche toujours pas, mais je viens de
trouver. https://ask.fedoraproject.org/en/question/32864/mpirun-command-not-found/
Il faut charger le module pour pouvoir l'utiliser. Du coup il faut
ajouter cette ligne au fichier de conf : 

#+BEGIN_EXAMPLE
module load mpi/mpich-x86_64 
#+END_EXAMPLE

#+begin_src
cd
gedit .bash_profile
#+end_src

J'y ai cru mais en fait non...

*** 2017-05-15 lundi
**** DONE ROSS                                                      :ROSS:
:LOGBOOK:
- State "DONE"       from ""       [2017-05-15 lun. 14:35]
:END:
***** intallation                                            :Deprecated:
CF CODES pour l'instalation
Reprise de l'installation, sur un nouvel environnement, Ubuntu cette
foi-ci (17.04)
On repart du début.
https://github.com/carothersc/ROSS
Version gcc : 6.3.0 20170406
Version cmake : 3.7.2-1
Version MPICH : 3.2-7
Version Doxygen 1.8.13

C'est bon tout fonctionne !
#+BEGIN_EXAMPLE
git clone -b master --single-branch git@github.com:carothersc/ROSS.git
mkdir ROSS-build
cd ROSS-build
export ARCH=x86_64
export CC=mpicc
cmake -DROSS_BUILD_MODELS=ON ../ROSS
make
make test
#+END_EXAMPLE

Avec ces commandes, on peut tester si tout fonctionne bien (car on a
des modèles et des tests). Si tout passe c'est OK. Merci ubuntu.

**** DONE CODES                                                    :CODES:
:LOGBOOK:
- State "DONE"       from "APPT"       [2017-05-15 lun. 17:17]
- State "APPT"       from "WAITING"    [2017-05-15 lun. 17:17]
- State "STARTED"    from "TODO"       [2017-05-15 lun. 17:17]
- State "TODO"       from              [2017-05-11 jeu. 17:51]
:END:
Sources : https://press3.mcs.anl.gov/codes/downloads/
Version : 0.5.2
Entered on [2017-05-15 lun. 09:34]
Mainenant que ROSS a l'air de fonctionner on passe à CODES.

En fait pour installer les deux il suffi de suivre les consignes dans
le fichier d'installation de CODES.
/!\ insallation dumpi
**** Traces HPL                                                      :HPL:
Maintenant que tout est installé et que je commence à comprendre, il
faut se diriger vers le premier objectif.
Je lis donc la doc GETTING STARTED, pour comprendre comment se compose
le logiciel, et avoir un point d'entrée.
***** Meeting avec Florence
CODES utilise DUMPI pour les traces. Il faut donc que je génère des
trâces avec mon PC grace à un logiciel HPL en MPI, que je pourrais
réinjecter. On tente de voir Arnaud demain.
*** 2017-05-16 mardi
**** DONE DUMPI                                         :Deprecated:DUMPI:
http://sst.sandia.gov/about_dumpi.html
J'ai l'impression qu'il faut installer tout le simulateur(SST) pour pouvoir
profiter de DUMPI. Cependant ils parlent le package, alors c'est peut
être un standalone 
profiter de DUMPI. 
**** Meeting avec Florence et Arnaud
- Les sources de DUMPI sont en fait sur [[https://github.com/sstsimulator/sst-dumpi][github]].
- D'après la [[https://github.com/sstsimulator/sst-dumpi/blob/master/docs/traceformat.dox][documentation]], c'est un format binaire et je ne suis pas
  sûr qu'il y ait moyen de générer de telles traces sans passer par
  leur bibliothèque. Si on veut de grosses traces (pour donner à
  manger à CODES), ça va être compliqué. Il aurait été pratique de les
  obtenir avec SimGrid (et le travail de Tom) mais il faudrait que
  SimGrid puisse générer du DUMPI.
- Après discussion rapide avec Lucas Schnorr, il est possible que
  DUMPI fonctionne avec SMPI puisque SMPI implémente PMPI sur lequel
  DUMPI se branche. Lucas avait réussi avec akypuera
  (http://github.com/schnorr/akypuera). On verra plus tard de toutes
  façons, il faut commencer par une petite trace.
- On n'a aucune idée de comment visualiser ou analyser les traces
  DUMPI. Je ne sais pas comment les convertir vers un autre
  format. Une possibilité serait de passer par [[https://github.com/hkustliqi/DUMPI_parser][ce parseur]] mais ça
  reste un pari.
- Il existe deux types de trâces, profiling (info de bases) ou une
  trâce complète (avec des outils comme DUMPI, EXTRAE, TAU/SCALASCA,
  OTF2 (format vers lequel tout le monde tend mais encore incomplet),
  POTI, ...
- DUMPI fait parti du projet SST, qui rassemble plusieurs projets
  - SST-MACRO : ressemble à SimGrid mais fonctionne en C++, et a des
    contraintes différentes. Il est encore en activité.
  - SST-Micro : pour simuler des tous petits systèmes de manière très
    détaillée.

À faire (sur ta machine pour commencer):
- Lancer HPL avec 12 processus (par exemple) et une matrice de taille
  20,000. Ne pas hésiter à aller voir Tom ou Christian si tu veux un
  fichier d'entrée pour HPL qui correspond à ceci.
- Suivre les informations de traçage de DUMPI (probablement à base de
  =LD_PRELOAD= et de =-ldumpi=) pour tracer HPL.
- Essayer de la rejouer sur ROSS.
**** Instalation DUMPI                                        :Deprecated:
Après le clonage du repo, il faut s'assurer d'avoir les bons outils :
- libtool
- m4
- automake
- autoconf
Une foi dans le dossier, il faut faire les commandes suivantes :
#+begin_src sh
./bootstrap.sh
mkdir build
cd build
../configure --enable-libdumpi --enable-test --prefix=$HOME/dumpi_inst CC=mpicc CXX=mpiCC
make
make install
make doc
#+end_src
J'ai cependant un problème, de nombreuses erreurs surviennent. En
effet, les signatures entre les .h de mpi et l'implémentation de DUMPI
diffèrent. Je vais donc voir si je dois prendre une version moins
récente de MPICH.
*** 2017-05-17 mercredi
**** DONE Installation de DUMPI/CODES             :CODES:Deprecated:DUMPI:
En reregardant la doc de CODES, il y a justement un avertissement sur
DUMPI. 
"To enable network tracing with dumpi
    (http://sst.sandia.gov/about_dumpi.html), use the option
    --with-dumpi=/path/to/dumpi/install with configure.

    NOTE: we only require libundumpi for trace processing. Hence, if building
    dumpi from source you may configure with --disable-libdumpi and
    --enable-libundumpi (this is especially useful if you have mpich3, which
    breaks libdumpi's function wrappers through const'ifying the MPI
    interface)."

Du coup pour l'installation ca donne ca :
#+begin_src sh
./bootstrap.sh
mkdir build
cd build
../configure --disable-libdumpi --enable-libundumpi --prefix=/home/chevamax/Documents/Stage_LIG_2017/sst-dumpi/dumpi_inst CC=mpicc CXX=mpiCC
make
make install
make doc
#+end_src
EDIT : il faut ensuite ajouter le chemin vers ~/dumpi-inst/lib dans le
fichier /etc/ld.so.conf et faire un ldconfig

Du coup il faut que je réinstalle CODES avec les le chemin vers dumpi.

#+begin_src sh
./prepare.sh
mkdir build
cd build
apt-get install flex
apt-get install bison
apt-get install pkg-config
../configure --with-dumpi=/home/chevamax/Documents/Stage_LIG_2017/sst-dumpi/dumpi_inst --prefix=/home/chevamax/Documents/Stage_LIG_2017/CODES/install CC=mpicc PKG_CONFIG_PATH=../../ROSS/install/lib/pkgconfig
make && make install
make tests && make check
#+end_src

Maintenant les tests ne passent plus ...
#+BEGIN_EXAMPLE
tests/workload/codes-workload-test: error while loading shared libraries: libundumpi.so.3: cannot open shared object file: No such file or directory
FAIL tests/workload/codes-workload-test.sh (exit status: 127)
#+END_EXAMPLE

J'ai du louper une étape.
En cherchant sur internet j'ai vu qu'on pouvait ajouter des librairies
dans le fichier /etc/ld.so.conf En ajoutant le chemin vers la
librairie dumpi ca fonctionne.

**** Installation de HPL                                  :Deprecated:HPL:
lien : http://www.netlib.org/benchmark/hpl/software.html
inspiration de : [[https://www.howtoforge.com/tutorial/hpl-high-performance-linpack-benchmark-raspberry-pi/][installation raspberry pi]] [[http://www.crc.nd.edu/~rich/CRC_Summer_Scholars_2014/HPL-HowTo.pdf][How to sur cluster]]

#+begin_src sh
sudo apt-get install libatlas-base-dev libmpich2-dev gfortran
#+end_src

le paquet libmpich2-dev n'existe plus dans les repos, j'ai donc pris
libmpich-dev qui la remplace (il y a cependant une version :i386 qui
existe, je sais pas si elle optimise certain processeurs).

MPdir = /usr/lib/mpich

En suivant le tuto pour raspberry pi, j'ai un problème (surement de
configuration). Voici la trace de l'erreur pour la partie 4 du tuto :

#+BEGIN_EXAMPLE
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/hpl-2.2/testing/ptest/linux »
mpif77  -o /home/chevamax/Documents/Stage_LIG_2017/hpl-2.2/bin/linux/xhpl HPL_pddriver.o         HPL_pdinfo.o           HPL_pdtest.o /home/chevamax/Documents/Stage_LIG_2017/hpl-2.2/lib/linux/libhpl.a  /usr/lib/atlas-base/libf77blas.a /usr/lib/atlas-base/libatlas.a -lblas /usr/lib/mpich/lib/libmpich.a
/usr/lib/mpich/lib/libmpich.a(lib_libmpich_la-initthread.o) : Dans la fonction « MPIR_Init_thread » :
(.text+0xb8) : référence indéfinie vers « pthread_mutexattr_init »
/usr/lib/mpich/lib/libmpich.a(lib_libmpich_la-initthread.o) : Dans la
fonction « MPIR_Init_thread » :
#+END_EXAMPLE

En ajoutant -pthread dans les flag on arrive à compiler.

Maintenant nouveau problème : 
#+Begin_Example
mpif77 -pthread -o /home/chevamax/Documents/Stage_LIG_2017/hpl-2.2/bin/linux/xhpl HPL_pddriver.o         HPL_pdinfo.o           HPL_pdtest.o /home/chevamax/Documents/Stage_LIG_2017/hpl-2.2/lib/linux/libhpl.a  /usr/lib/atlas-base/libf77blas.a /usr/lib/atlas-base/libatlas.a -lblas /usr/lib/mpich/lib/libmpich.a
/usr/lib/mpich/lib/libmpich.a(lib_libmpich_la-mpid_nem_ckpt.o) : Dans la fonction « ckpt_cb » :
(.text+0x342) : référence indéfinie vers « cr_checkpoint »
/usr/lib/mpich/lib/libmpich.a(lib_libmpich_la-mpid_nem_ckpt.o) : Dans la fonction « ckpt_cb » :
(.text+0x3e3) : référence indéfinie vers « cr_get_restart_info »
/usr/lib/mpich/lib/libmpich.a(lib_libmpich_la-mpid_nem_ckpt.o) : Dans la fonction « MPIDI_nem_ckpt_init » :
(.text+0xca5) : référence indéfinie vers « cr_init »
#+END_EXAMPLE

Je sais pas du tout qu'est ce qui manque. Est-ce que ça a un lien avec
BLCR ? Si oui il va falloir que je me lance dans l'installation.

*** 2017-05-18 jeudi

**** Mail Arnaud

1. Quand tu indiques "J'ai cependant un problème, de nombreuses
   erreurs surviennent. En effet, les signatures entre les .h de mpi
   et l'implémentation de DUMPI diffèrent. Je vais donc voir si je
   dois prendre une version moins récente de MPICH." Ça vaut le coup
   que tu indiques les erreurs (au moins les premières) dans une
   balise #+BEGIN_EXAMPLE   #+END_EXAMPLE
2. À propos de
   #+BEGIN_EXAMPLE
   tests/workload/codes-workload-test: error while loading shared
libraries: libundumpi.so.3:
   cannot open shared object file: No such file or directory FAIL
tests/workload/codes-workload-test.sh (exit status: 127)
   #+END_EXAMPLE
   Mettons que tu ais installé DUMPI dans /home/maxime/toto/lib (i.e.,
   le libundumpi.so est dans ce répertoire là).
   #+BEGIN_EXAMPLE
   export LD_LIBRARY_PATH=/home/maxime/toto/lib:$LD_LIBRARY_PATH
   #+END_EXAMPLE
   Ça devrait permettre au linker de trouver la bibliothèque. Modifier
   /etc/ld/so.conf me parait une mauvaise idée...
3. "Est-ce que ça a un lien avec BLCR ?" Ouh là, non, tu n'as pas
   besoin de ça. C'est une librairie de checkpoint qui permet de
   redémarer le code même quand on perd un noeud. Je ne sais pas d'où
   ça sort c'est anormal, tu ne devrais pas en avoir besoin.

**** Reprise installation HPL                             :Deprecated:HPL:
En regardant sur internet, je vois que les fonctions font parties d'un
package blcr-util. Je tente donc de l'installer pour voir.
Effectivement ca ne résoud pas le soucis...
Et mpiexec prend en charge bclr c'est étrange.

***** Tentative d'installation sur une machine virtuelle
Je vais tenter d'installer HPL sur un linux vierge, pour voir si il
n'y a pas conflit avec autre chose de déjà installé.
J'ai exactement la même erreure...

Après plusieurs tests de configurations trouvé sur plusieurs tuto,
j'en ai finalement trouvé un qui fonctionne ! (sur la machine
virtuelle pour le moment). [[http://jahanzebnotes.blogspot.fr/2013/06/how-to-run-hpl-benchmark-with-atlas.html][lien]]

*** 2017-05-19 vendredi

**** Changement des paramètres sur ma machine
#+BEGIN_EXAMPLE
#  
#  -- High Performance Computing Linpack Benchmark (HPL)                
#     HPL - 2.2 - February 24, 2016                          
#     Antoine P. Petitet                                                
#     University of Tennessee, Knoxville                                
#     Innovative Computing Laboratory                                 
#     (C) Copyright 2000-2008 All Rights Reserved                       
#                                                                       
#  -- Copyright notice and Licensing terms:                             
#                                                                       
#  Redistribution  and  use in  source and binary forms, with or without
#  modification, are  permitted provided  that the following  conditions
#  are met:                                                             
#                                                                       
#  1. Redistributions  of  source  code  must retain the above copyright
#  notice, this list of conditions and the following disclaimer.        
#                                                                       
#  2. Redistributions in binary form must reproduce  the above copyright
#  notice, this list of conditions,  and the following disclaimer in the
#  documentation and/or other materials provided with the distribution. 
#                                                                       
#  3. All  advertising  materials  mentioning  features  or  use of this
#  software must display the following acknowledgement:                 
#  This  product  includes  software  developed  at  the  University  of
#  Tennessee, Knoxville, Innovative Computing Laboratory.             
#                                                                       
#  4. The name of the  University,  the name of the  Laboratory,  or the
#  names  of  its  contributors  may  not  be used to endorse or promote
#  products  derived   from   this  software  without  specific  written
#  permission.                                                          
#                                                                       
#  -- Disclaimer:                                                       
#                                                                       
#  THIS  SOFTWARE  IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
#  ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,  INCLUDING,  BUT NOT
#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY
#  OR  CONTRIBUTORS  BE  LIABLE FOR ANY  DIRECT,  INDIRECT,  INCIDENTAL,
#  SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL DAMAGES  (INCLUDING,  BUT NOT
#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
#  DATA OR PROFITS; OR BUSINESS INTERRUPTION)  HOWEVER CAUSED AND ON ANY
#  THEORY OF LIABILITY, WHETHER IN CONTRACT,  STRICT LIABILITY,  OR TORT
#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
# ######################################################################
#  
# ----------------------------------------------------------------------
# - shell --------------------------------------------------------------
# ----------------------------------------------------------------------
#
SHELL        = /bin/sh
#
CD           = cd
CP           = cp
LN_S         = ln -s
MKDIR        = mkdir
RM           = /bin/rm -f
TOUCH        = touch
#
# ----------------------------------------------------------------------
# - Platform identifier ------------------------------------------------
# ----------------------------------------------------------------------
#
ARCH         = linux
#
# ----------------------------------------------------------------------
# - HPL Directory Structure / HPL library ------------------------------
# ----------------------------------------------------------------------
#
TOPdir       = $(HOME)/Documents/Stage_LIG_2017/hpl
INCdir       = $(TOPdir)/include
BINdir       = $(TOPdir)/bin/$(ARCH)
LIBdir       = $(TOPdir)/lib/$(ARCH)
#
HPLlib       = $(LIBdir)/libhpl.a 
#
# ----------------------------------------------------------------------
# - Message Passing library (MPI) --------------------------------------
# ----------------------------------------------------------------------
# MPinc tells the  C  compiler where to find the Message Passing library
# header files,  MPlib  is defined  to be the name of  the library to be 
# used. The variable MPdir is only used for defining MPinc and MPlib.
#
MPdir        = /usr/lib/mpich
MPinc        = -I $(MPdir)/include
MPlib        = -L $(MPdir)/lib
#
# ----------------------------------------------------------------------
# - Linear Algebra library (BLAS or VSIPL) -----------------------------
# ----------------------------------------------------------------------
# LAinc tells the  C  compiler where to find the Linear Algebra  library
# header files,  LAlib  is defined  to be the name of  the library to be 
# used. The variable LAdir is only used for defining LAinc and LAlib.
#
LAdir        = /usr/lib/atlas-base
LAinc        = 
LAlib        = $(LAdir)/libf77blas.a $(LAdir)/libatlas.a -lblas
#
# ----------------------------------------------------------------------
# - F77 / C interface --------------------------------------------------
# ----------------------------------------------------------------------
# You can skip this section  if and only if  you are not planning to use
# a  BLAS  library featuring a Fortran 77 interface.  Otherwise,  it  is
# necessary  to  fill out the  F2CDEFS  variable  with  the  appropriate
# options.  **One and only one**  option should be chosen in **each** of
# the 3 following categories:
#
# 1) name space (How C calls a Fortran 77 routine)
#
# -DAdd_              : all lower case and a suffixed underscore  (Suns,
#                       Intel, ...),                           [default]
# -DNoChange          : all lower case (IBM RS6000),
# -DUpCase            : all upper case (Cray),
# -DAdd__             : the FORTRAN compiler in use is f2c.
#
# 2) C and Fortran 77 integer mapping
#
# -DF77_INTEGER=int   : Fortran 77 INTEGER is a C int,         [default]
# -DF77_INTEGER=long  : Fortran 77 INTEGER is a C long,
# -DF77_INTEGER=short : Fortran 77 INTEGER is a C short.
#
# 3) Fortran 77 string handling
#
# -DStringSunStyle    : The string address is passed at the string loca-
#                       tion on the stack, and the string length is then
#                       passed as  an  F77_INTEGER  after  all  explicit
#                       stack arguments,                       [default]
# -DStringStructPtr   : The address  of  a  structure  is  passed  by  a
#                       Fortran 77  string,  and the structure is of the
#                       form: struct {char *cp; F77_INTEGER len;},
# -DStringStructVal   : A structure is passed by value for each  Fortran
#                       77 string,  and  the  structure is  of the form:
#                       struct {char *cp; F77_INTEGER len;},
# -DStringCrayStyle   : Special option for  Cray  machines,  which  uses
#                       Cray  fcd  (fortran  character  descriptor)  for
#                       interoperation.
#
F2CDEFS      = -DAdd_ -DF77_INTEGER=int -DStringSunStyle
#
# ----------------------------------------------------------------------
# - HPL includes / libraries / specifics -------------------------------
# ----------------------------------------------------------------------
#
HPL_INCLUDES = -I$(INCdir) -I$(INCdir)/$(ARCH) $(LAinc) $(MPinc)
HPL_LIBS     = $(HPLlib) $(LAlib) $(MPlib) -lmpl
#
# - Compile time options -----------------------------------------------
#
# -DHPL_COPY_L           force the copy of the panel L before bcast;
# -DHPL_CALL_CBLAS       call the cblas interface;
# -DHPL_CALL_VSIPL       call the vsip  library;
# -DHPL_DETAILED_TIMING  enable detailed timers;
#
# By default HPL will:
#    *) not copy L before broadcast,
#    *) call the BLAS Fortran 77 interface,
#    *) not display detailed timing information.
#
HPL_OPTS     = -DHPL_CALL_CBLAS
# 
# ----------------------------------------------------------------------
#
HPL_DEFS     = $(F2CDEFS) $(HPL_OPTS) $(HPL_INCLUDES) 
#
# ----------------------------------------------------------------------
# - Compilers / linkers - Optimization flags ---------------------------
# ----------------------------------------------------------------------
#
CC           = /usr/bin/mpicc
CCNOOPT      = $(HPL_DEFS) 
CCFLAGS      = $(HPL_DEFS) -fomit-frame-pointer -O3 -funroll-loops
#
LINKER       = /usr/bin/mpicc
LINKFLAGS    = $(CCFLAGS) -pthread
#
ARCHIVER     = ar
ARFLAGS      = r
RANLIB       = echo
#
# ----------------------------------------------------------------------
#+END_EXAMPLE

Le soucis avait l'air de se trouver dans la configuration de LPlib. En
effet, dans tous les exemple vu c'était de la sorte :
~$(MPdir)/lib/libmpich.a~ hors en le changeant en ~MPlib= -L $(MPdir)/lib~
l'erreur disparait et le programme tourne.

**** Lancement de HPL                                                :HPL:
Je vais utiliser ce [[http://www.advancedclustering.com/act_kb/tune-hpl-dat-file/][site]] pour m'aider à faire les paramètres sur
ma machine.

***** Premier test
Pour voir si tout fonctionne je fais sur un seul thread.

****** HPL.dat
#+begin_example
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any)
6            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
14208        Ns
1            # of NBs
192          NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
1            Ps
1            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
1            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
#+end_example

avec la sortie suivante :
#+begin_example
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       14208   192     1     1             201.68              9.482e+00
HPL_pdgesv() start time Fri May 19 10:17:35 2017

HPL_pdgesv() end time   Fri May 19 10:20:57 2017

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=        0.0018179 ...... PASSED
================================================================================

Finished      1 tests with the following results:
              1 tests completed and passed residual checks,
              0 tests completed and failed residual checks,
              0 tests skipped because of illegal input values.
--------------------------------------------------------------------------------

End of Tests.
================================================================================
#+end_example

***** Second test

#+begin_src sh :results output :exports both
lscpu
#+end_src

#+RESULTS:
#+begin_example
Architecture:          x86_64
Mode(s) opératoire(s) des processeurs :32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                4
On-line CPU(s) list:   0-3
Thread(s) par cœur : 2
Cœur(s) par socket : 2
Socket(s):             1
Nœud(s) NUMA :       1
Identifiant constructeur :GenuineIntel
Famille de processeur :6
Modèle :             69
Model name:            Intel(R) Core(TM) i5-4210U CPU @ 1.70GHz
Révision :           1
Vitesse du processeur en MHz :1853.613
CPU max MHz:           2700,0000
CPU min MHz:           800,0000
BogoMIPS:              4789.03
Virtualisation :      VT-x
Cache L1d :           32K
Cache L1i :           32K
Cache L2 :            256K
Cache L3 :            3072K
NUMA node0 CPU(s):     0-3
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts
#+end_example

J'ai donc un socket, avec deux coeurs et 2 threads par coeurs. Je
pense donc pouvoir faire peut être du 2x2

J'ai 8Go de RAM. Ils conseillent d'en alouer au maximum 80% (le reste
pour l'OS &co).

Je vais donc faire le test avec ce fichier :
#+BEGIN_EXAMPLE
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any) 
6            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
18048         Ns
1            # of NBs
192           NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
2            Ps
2            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
1            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
#+END_EXAMPLE

#+begin_src sh :results output :exports both
mpiexec -n 4 ./xhpl 
#+end_src

#+RESULTS:
#+BEGIN_EXAMPLE
================================================================================
HPLinpack 2.2  --  High-Performance Linpack benchmark  --   February 24, 2016
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :   18048 
NB     :     192 
PMAP   : Row-major process mapping
P      :       2 
Q      :       2 
PFACT  :   Right 
NBMIN  :       4 
NDIV   :       2 
RFACT  :   Crout 
BCAST  :  1ringM 
DEPTH  :       1 
SWAP   : Mix (threshold = 64)
L1     : transposed form
U      : transposed form
EQUIL  : yes
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0

================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       18048   192     2     2             309.73              1.265e+01
HPL_pdgesv() start time Fri May 19 10:59:50 2017

HPL_pdgesv() end time   Fri May 19 11:04:59 2017

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=        0.0014193 ...... PASSED
================================================================================

Finished      1 tests with the following results:
              1 tests completed and passed residual checks,
              0 tests completed and failed residual checks,
              0 tests skipped because of illegal input values.
--------------------------------------------------------------------------------

End of Tests.
================================================================================
#+END_EXAMPLE

C'est la première fois que je vois mes 4 "CPU" à 100% !

Entre les deux tests je suis passé de 9.482 à 12.65 Gflops.
Je pense qu'il est possible d'augmenter encoreun peu, mais le but est
de faire le lien avec DUMPI.


**** Lien avec DUMPI                                               :DUMPI:
Il semblerais que j'ai installé seulement undumpi, vu que dumpi ne
fonctionne pas avec les dernières version de mpich...
J'ai peut être trouvé un moyen : [[https://xgitlab.cels.anl.gov/codes/codes/blob/db787d9f77786b8682ab8746942e3e6037cacfd7/doc/BUILD_STEPS][ici]]
- DUMPI :
Je décide de l'installer sur ma machine virtuelle (vu que mon dumpi
est installé seulement avec undumpi sur ma machine et qu'il est lié à CODES).

#+begin_src sh
../configure --enable-libdumpi --prefix=/home/chevamax/sst-dumpi/install CC=mpicc CXX=mpiCC CFLAGS="-DMPICH_SUPPRESS_PROTOTYPES=1 -DHAVE_PRAGMA_HP_SEC_DEF=1"
#+end_src
Permet de passe le make

DONE : faire un test avec mpi tuto helloworld.
#+BEGIN_EXAMPLE
Linking with C/C++

We can use libdumpi that we generated above:

user@machine:~/play/mpi$ mpicc hello.c -c
user@machine:~/play/mpi$ mpicc hello.o -L$HOME/dumpi_inst/lib -ldumpi -o hello
user@machine:~/play/mpi$ LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/dumpi_inst/lib ./hello 
Hello!
user@machine:~/play/mpi$ ls
dumpi-2011.01.25.15.46.10-0000.bin  hello    hello.o
dumpi-2011.01.25.15.46.10.meta      hello.c

In this instance there is a single MPI rank created. There are two files created: a metafile and a file for our MPI rank zero. The date and time are encoded into the filename to prevent accidental overwrite of DUMPI trace files.

We can print the trace file using dumpi2ascii to verify a simple 'hello world' that does, an init, comm_rank, comm_size, and a finalize as its only MPI calls:

user@machine:~/play/mpi$ $HOME/dumpi_inst/bin/dumpi2ascii dumpi-2011.01.25.15.46.10-0000.bin 
MPI_Init entering at walltime 20120.589738643, cputime 0.023382031 seconds in thread 0.
int argc=1
string argv[1]=["./hello"]
MPI_Init returning at walltime 20120.589752472, cputime 0.023385336 seconds in thread 0.
MPI_Comm_rank entering at walltime 20120.589793260, cputime 0.023437127 seconds in thread 0.
MPI_Comm comm=2 (MPI_COMM_WORLD)
int rank=0
MPI_Comm_rank returning at walltime 20120.589796683, cputime 0.023440692 seconds in thread 0.
MPI_Comm_size entering at walltime 20120.589810022, cputime 0.023453983 seconds in thread 0.
MPI_Comm comm=2 (MPI_COMM_WORLD)
int size=1
MPI_Comm_size returning at walltime 20120.589813095, cputime 0.023457038 seconds in thread 0.
MPI_Finalize entering at walltime 20120.589841172, cputime 0.023484981 seconds in thread 0.
MPI_Finalize returning at walltime 20120.590067322, cputime 0.023710703 seconds in thread 0.

Libdumpi uses the PMPI interface, so you shouldn't have to use any included header files, or make any subroutine/function calls to DUMPI for most use cases of DUMPI. Of course, when you use libdumpi, you can't use or link against any other tools that use the PMPI interface in the same executable.

By default, DUMPI will emit trace information for every MPI call, with a timestamp for entry and exit to each routine.
#+END_EXAMPLE

En suivant la méthode du dessus, j'ai été capable de générer des
traces dumpi pour un programme helloWorld disponible [[http://mpitutorial.com/tutorials/mpi-hello-world/][ici]].

***** HPL + DUMPI
En ajoutant ~-L path/to/dumpi_inst/lib -ldumpi~ à la fin de la ligne
MPlib dans le Make.linux de HPL, j'ai réussi à générer une trâce!
/!\ Ca ne fonctionne pas si on met cette ligne sur *HPL_LIBS* ou sur le *LINKFLAGS*

Je n'ai pas encore compris comment on utilise =LD_PRELOAD= j'ai donc
compilé directement HPL avec le lien DUMPI. Je pense donc installer
deux versions, une avec les TRACES et l'autre sans. Christian m'a dit
de faire un lien entre les deux dossiers pour s'assurer que le HPL.dat
soit toujours le même.

***** Rencontre avec Christian
Florence m'a présenté Christian aujourd'hui qui travail sur
SimGrid. Il est très intéréssé par l'objectif de mon stage et aimerait
ajouter SST à la comparaison. En effet, il a écrit un papier sur ses
traveaux et on lui a reproché de ne pas avoir de comparaison avec
SST. J'espère avancer vite pour qu'il puisse se servir de mon travail!

***** Recherche
En faisant des recherche pour installer dumpi, j'ai vu qu'il y a un
convertisseur vers l'ASCII, mais aussi un convertisseur (experimental)
vers du DOT.

*** 2017-05-22 lundi

**** Configuration du system (how to)
Maintenant que tout marche séparément il faut que je rassemble tout
ça. Je pense faire un HowTo pour tout installer dans le bon
ordre. J'aurais du commencer par installer HPL et DUMPI avant de faire
le reste. Let's start again !

***** apt-get + versions
- gcc => 6.3.020170406
- mpich => 3.2-7
- cmake => 3.7.2-1
- doxygen => 1.8.13
- libtool => 2.4.6-2
- m4 => 1.4.18-1
- automake => 1:1.15-5ubuntu1
- autoconf => 2.69-10
- libatlas-base-dev => 3.10.3-1ubuntu1
- libmpich-dev => 3.2-7 build1
- gfortran 4 => 6.3.0-2ubuntu1
- flex => 2.6.2-1.3
- bison => 2:3.0.4.dfsg-1build1
- pkg-config => 0.29.1-0ubuntu1
- blcr-util => 0.8.5-2.3 (ce dernier et peut-être inutile, mais
  présent sur ma machine lors de l'installation)

***** DUMPI                                                       :DUMPI:

#+begin_src sh :session foo
mkdir logiciels
cd logiciels
#+end_src

#+begin_src sh :session foo :results output :exports both 
git clone https://github.com/sstsimulator/sst-dumpi.git
cd sst-dumpi
./bootstrap.sh
mkdir build
cd build
../configure --enable-libdumpi --prefix=/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install CC=mpicc CXX=mpiCC CFLAGS="-DMPICH_SUPPRESS_PROTOTYPES=1 -DHAVE_PRAGMA_HP_SEC_DEF=1"
make
make install
make doc
#+end_src

#+RESULTS:
#+begin_example
Clonage dans 'sst-dumpi'...
remote: Counting objects: 329, done.
(1/3)           
remote: Compressing objects:  66% (2/3)           
remote: Compressing objects: 100% (3/3)           
remote: Compressing objects: 100% (3/3), done.
(1/329)   
Réception d'objets:   1% (4/329)   
Réception d'objets:   2% (7/329)   
Réception d'objets:   3% (10/329)   
Réception d'objets:   4% (14/329)   
Réception d'objets:   5% (17/329)   
Réception d'objets:   6% (20/329)   
Réception d'objets:   7% (24/329)   
Réception d'objets:   8% (27/329)   
Réception d'objets:   9% (30/329)   
Réception d'objets:  10% (33/329)   
Réception d'objets:  11% (37/329)   
Réception d'objets:  12% (40/329)   
Réception d'objets:  13% (43/329)   
Réception d'objets:  14% (47/329)   
Réception d'objets:  15% (50/329)   
Réception d'objets:  16% (53/329)   
Réception d'objets:  17% (56/329)   
Réception d'objets:  18% (60/329)   
Réception d'objets:  19% (63/329)   
Réception d'objets:  20% (66/329)   
Réception d'objets:  21% (70/329)   
Réception d'objets:  22% (73/329)   
Réception d'objets:  23% (76/329)   
Réception d'objets:  24% (79/329)   
Réception d'objets:  25% (83/329)   
Réception d'objets:  26% (86/329)   
Réception d'objets:  27% (89/329)   
Réception d'objets:  28% (93/329)   
Réception d'objets:  29% (96/329)   
Réception d'objets:  30% (99/329)   
Réception d'objets:  31% (102/329)   
Réception d'objets:  32% (106/329)   
Réception d'objets:  33% (109/329)   
Réception d'objets:  34% (112/329)   
Réception d'objets:  35% (116/329)   
Réception d'objets:  36% (119/329)   
Réception d'objets:  37% (122/329)   
Réception d'objets:  38% (126/329)   
Réception d'objets:  39% (129/329)   
Réception d'objets:  40% (132/329)   
Réception d'objets:  41% (135/329)   
Réception d'objets:  42% (139/329)   
Réception d'objets:  43% (142/329)   
Réception d'objets:  44% (145/329)   
Réception d'objets:  45% (149/329)   
Réception d'objets:  46% (152/329)   
Réception d'objets:  47% (155/329)   
Réception d'objets:  48% (158/329)   
Réception d'objets:  49% (162/329)   
Réception d'objets:  50% (165/329)   
Réception d'objets:  51% (168/329)   
Réception d'objets:  52% (172/329)   
Réception d'objets:  53% (175/329)   
Réception d'objets:  54% (178/329)   
Réception d'objets:  55% (181/329)   
Réception d'objets:  56% (185/329)   
Réception d'objets:  57% (188/329)   
Réception d'objets:  58% (191/329)   
Réception d'objets:  59% (195/329)   
Réception d'objets:  60% (198/329)   
Réception d'objets:  61% (201/329)   
Réception d'objets:  62% (204/329)   
Réception d'objets:  63% (208/329)   
Réception d'objets:  64% (211/329)   
Réception d'objets:  65% (214/329)   
Réception d'objets:  66% (218/329)   
Réception d'objets:  67% (221/329)   
Réception d'objets:  68% (224/329)   
Réception d'objets:  69% (228/329)   
Réception d'objets:  70% (231/329)   
Réception d'objets:  71% (234/329)   
Réception d'objets:  72% (237/329)   
Réception d'objets:  73% (241/329)   
Réception d'objets:  74% (244/329)   
Réception d'objets:  75% (247/329)   
Réception d'objets:  76% (251/329)   
Réception d'objets:  77% (254/329)   
Réception d'objets:  78% (257/329)   
Réception d'objets:  79% (260/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  80% (264/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  81% (267/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  82% (270/329), 492.01 KiB | 951.00 KiB/s   
remote: Total 329 (delta 0), reused 1 (delta 0), pack-reused 326
(274/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  84% (277/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  85% (280/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  86% (283/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  87% (287/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  88% (290/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  89% (293/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  90% (297/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  91% (300/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  92% (303/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  93% (306/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  94% (310/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  95% (313/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  96% (316/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  97% (320/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  98% (323/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets:  99% (326/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets: 100% (329/329), 492.01 KiB | 951.00 KiB/s   
Réception d'objets: 100% (329/329), 536.61 KiB | 951.00 KiB/s, fait.
(0/147)   
Résolution des deltas:   1% (2/147)   
Résolution des deltas:   2% (4/147)   
Résolution des deltas:   4% (6/147)   
Résolution des deltas:   5% (8/147)   
Résolution des deltas:   7% (11/147)   
Résolution des deltas:   8% (12/147)   
Résolution des deltas:  10% (15/147)   
Résolution des deltas:  11% (17/147)   
Résolution des deltas:  12% (18/147)   
Résolution des deltas:  14% (21/147)   
Résolution des deltas:  15% (23/147)   
Résolution des deltas:  17% (25/147)   
Résolution des deltas:  18% (27/147)   
Résolution des deltas:  19% (28/147)   
Résolution des deltas:  20% (30/147)   
Résolution des deltas:  21% (31/147)   
Résolution des deltas:  22% (33/147)   
Résolution des deltas:  23% (35/147)   
Résolution des deltas:  25% (38/147)   
Résolution des deltas:  27% (41/147)   
Résolution des deltas:  30% (45/147)   
Résolution des deltas:  31% (47/147)   
Résolution des deltas:  34% (50/147)   
Résolution des deltas:  35% (52/147)   
Résolution des deltas:  37% (55/147)   
Résolution des deltas:  38% (56/147)   
Résolution des deltas:  40% (59/147)   
Résolution des deltas:  42% (63/147)   
Résolution des deltas:  44% (66/147)   
Résolution des deltas:  45% (67/147)   
Résolution des deltas:  46% (68/147)   
Résolution des deltas:  47% (70/147)   
Résolution des deltas:  48% (71/147)   
Résolution des deltas:  49% (73/147)   
Résolution des deltas:  51% (75/147)   
Résolution des deltas:  52% (77/147)   
Résolution des deltas:  53% (78/147)   
Résolution des deltas:  55% (81/147)   
Résolution des deltas:  56% (83/147)   
Résolution des deltas:  57% (84/147)   
Résolution des deltas:  59% (87/147)   
Résolution des deltas:  60% (89/147)   
Résolution des deltas:  61% (91/147)   
Résolution des deltas:  62% (92/147)   
Résolution des deltas:  64% (95/147)   
Résolution des deltas:  65% (96/147)   
Résolution des deltas:  66% (98/147)   
Résolution des deltas:  68% (100/147)   
Résolution des deltas:  69% (102/147)   
Résolution des deltas:  70% (103/147)   
Résolution des deltas:  71% (105/147)   
Résolution des deltas:  72% (107/147)   
Résolution des deltas:  73% (108/147)   
Résolution des deltas:  74% (109/147)   
Résolution des deltas:  76% (113/147)   
Résolution des deltas:  77% (114/147)   
Résolution des deltas:  78% (115/147)   
Résolution des deltas:  81% (120/147)   
Résolution des deltas:  82% (121/147)   
Résolution des deltas:  83% (123/147)   
Résolution des deltas:  84% (124/147)   
Résolution des deltas:  85% (125/147)   
Résolution des deltas:  86% (127/147)   
Résolution des deltas:  87% (128/147)   
Résolution des deltas:  90% (133/147)   
Résolution des deltas:  91% (134/147)   
Résolution des deltas:  92% (136/147)   
Résolution des deltas:  94% (139/147)   
Résolution des deltas:  95% (141/147)   
Résolution des deltas:  96% (142/147)   
Résolution des deltas:  97% (143/147)   
Résolution des deltas:  98% (145/147)   
Résolution des deltas: 100% (147/147)   
Résolution des deltas: 100% (147/147), fait.
$ ./bootstrap.sh: 14: ./bootstrap.sh: glibtoolize: not found
libtoolize: putting auxiliary files in AC_CONFIG_AUX_DIR, 'bin'.
libtoolize: linking file 'bin/ltmain.sh'
libtoolize: putting macros in AC_CONFIG_MACRO_DIRS, 'acinclude'.
libtoolize: linking file 'acinclude/libtool.m4'
libtoolize: linking file 'acinclude/ltoptions.m4'
libtoolize: linking file 'acinclude/ltsugar.m4'
libtoolize: linking file 'acinclude/ltversion.m4'
libtoolize: linking file 'acinclude/lt~obsolete.m4'
libtoolize: putting auxiliary files in AC_CONFIG_AUX_DIR, 'bin'.
libtoolize: copying file 'bin/ltmain.sh'
libtoolize: putting macros in AC_CONFIG_MACRO_DIRS, 'acinclude'.
libtoolize: copying file 'acinclude/libtool.m4'
libtoolize: copying file 'acinclude/ltoptions.m4'
libtoolize: copying file 'acinclude/ltsugar.m4'
libtoolize: copying file 'acinclude/ltversion.m4'
libtoolize: copying file 'acinclude/lt~obsolete.m4'
configure.ac:53: installing 'bin/ar-lib'
configure.ac:53: installing 'bin/compile'
configure.ac:51: installing 'bin/config.guess'
configure.ac:51: installing 'bin/config.sub'
configure.ac:52: installing 'bin/install-sh'
configure.ac:52: installing 'bin/missing'
dumpi/bin/Makefile.am: installing 'bin/depcomp'
parallel-tests: installing 'bin/test-driver'
checking build system type... x86_64-pc-linux-gnu
checking host system type... x86_64-pc-linux-gnu
checking target system type... x86_64-pc-linux-gnu
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a thread-safe mkdir -p... /bin/mkdir -p
checking for gawk... no
checking for mawk... mawk
(MAKE)... yes
checking whether make supports nested variables... yes
checking for style of include used by make... GNU
checking for gcc... mpicc
checking whether the C compiler works... yes
checking for C compiler default output file name... a.out
checking for suffix of executables... 
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether mpicc accepts -g... yes
checking for mpicc option to accept ISO C89... none needed
checking whether mpicc understands -c and -o together... yes
checking dependency style of mpicc... gcc3
checking for ar... ar
checking the archiver (ar) interface... ar
checking whether make supports nested variables... (cached) yes
checking for gcc... (cached) mpicc
checking whether we are using the GNU C compiler... (cached) yes
checking whether mpicc accepts -g... (cached) yes
checking for mpicc option to accept ISO C89... (cached) none needed
checking whether mpicc understands -c and -o together... (cached) yes
checking dependency style of mpicc... (cached) gcc3
checking whether we are using the GNU C++ compiler... yes
checking whether mpiCC accepts -g... yes
checking dependency style of mpiCC... gcc3
checking how to print strings... printf
checking for a sed that does not truncate output... /bin/sed
checking for grep that handles long lines and -e... /bin/grep
checking for egrep... /bin/grep -E
checking for fgrep... /bin/grep -F
checking for ld used by mpicc... /usr/bin/ld
checking if the linker (/usr/bin/ld) is GNU ld... yes
checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B
checking the name lister (/usr/bin/nm -B) interface... BSD nm
checking whether ln -s works... yes
checking the maximum length of command line arguments... 1635000
checking how to convert x86_64-pc-linux-gnu file names to x86_64-pc-linux-gnu format... func_convert_file_noop
checking how to convert x86_64-pc-linux-gnu file names to toolchain format... func_convert_file_noop
checking for /usr/bin/ld option to reload object files... -r
checking for objdump... objdump
checking how to recognize dependent libraries... pass_all
checking for dlltool... no
s\n
checking for archiver @FILE support... @
checking for strip... strip
checking for ranlib... ranlib
checking command to parse /usr/bin/nm -B output from mpicc object... ok
checking for sysroot... no
checking for a working dd... /bin/dd
checking how to truncate binary pipes... /bin/dd bs=4096 count=1
checking for mt... mt
checking if mt is a manifest tool... no
checking how to run the C preprocessor... mpicc -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking for dlfcn.h... yes
checking for objdir... .libs
checking if mpicc supports -fno-rtti -fno-exceptions... no
checking for mpicc option to produce PIC... -fPIC -DPIC
checking if mpicc PIC flag -fPIC -DPIC works... yes
checking if mpicc static flag -static works... no
checking if mpicc supports -c -o file.o... yes
checking if mpicc supports -c -o file.o... (cached) yes
checking whether the mpicc linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes
checking whether -lc should be explicitly linked in... no
checking dynamic linker characteristics... GNU/Linux ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... yes
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... yes
checking whether to build static libraries... yes
checking how to run the C++ preprocessor... mpiCC -E
checking for ld used by mpiCC... /usr/bin/ld -m elf_x86_64
checking if the linker (/usr/bin/ld -m elf_x86_64) is GNU ld... yes
checking whether the mpiCC linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes
checking for mpiCC option to produce PIC... -fPIC -DPIC
checking if mpiCC PIC flag -fPIC -DPIC works... yes
checking if mpiCC static flag -static works... no
checking if mpiCC supports -c -o file.o... yes
checking if mpiCC supports -c -o file.o... (cached) yes
checking whether the mpiCC linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes
checking dynamic linker characteristics... (cached) GNU/Linux ld.so
checking how to hardcode library paths into programs... immediate
checking whether POSIX timers should be used... available without linking -lrt
"enable_test=no"
checking wheter OTF library and header file can be found... no
checking whether mpicc can compile and link an MPI program... yes
checking whether mpi.h is found... yes
checking whether pthreads provide thread-local storage... no
checking whether HOST_NAME_MAX is declared... no
checking for gettimeofday... yes
checking for getrusage... yes
checking papi support... no
checking instrumentation settings... instrumentation not enabled
checking for dot... no
checking that generated files are newer than configure... done
configure: creating ./config.status
config.status: creating Makefile
config.status: creating dumpi/Makefile
config.status: creating dumpi/common/Makefile
config.status: creating dumpi/libdumpi/Makefile
config.status: creating dumpi/libundumpi/Makefile
config.status: creating dumpi/bin/Makefile
config.status: creating dumpi/test/Makefile
config.status: creating tests/Makefile
config.status: creating docs/doxygen.cfg
config.status: creating dumpi/dumpiconfig-generated.h
config.status: executing depfiles commands
config.status: executing libtool commands
Making all in dumpi
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
make  all-recursive
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
Making all in common
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/common »
  CC       types.lo
  CC       funcs.lo
  CC       io.lo
  CC       dumpiio.lo
  CC       funclabels.lo
  CC       gettime.lo
  CC       constants.lo
  CC       perfctrs.lo
  CC       perfctrtags.lo
  CC       iodefs.lo
  CC       debugflags.lo
  CCLD     libdumpi_common.la
ar: `u' modifier ignored since `D' is the default (see `U')
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/common »
Making all in libdumpi
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libdumpi »
  CC       data.lo
  CC       init.lo
  CC       libdumpi.lo
  CC       callprofile.lo
  CC       callprofile-addrset.lo
  CC       mpibindings-utils.lo
  CC       mpibindings-maps.lo
  CC       mpibindings2.lo
../../../dumpi/libdumpi/mpibindings2.c: In function ‘MPI_Pcontrol’:
llu’ expects argument of type ‘long long unsigned int’, but argument 3 has type ‘uint64_t {aka long unsigned int}’ [-Wformat=]
         fprintf(stderr, "WARNING:  DUMPI:  MPI_Pcontrol(3, ...):  "
                         ^
  CCLD     libdumpi.la
ar: `u' modifier ignored since `D' is the default (see `U')
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libdumpi »
Making all in libundumpi
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libundumpi »
  CC       libundumpi.lo
  CC       callbacks.lo
  CC       bindings.lo
  CCLD     libundumpi.la
ar: `u' modifier ignored since `D' is the default (see `U')
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libundumpi »
Making all in bin
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/bin »
  CC       dumpi2ascii.o
../../../dumpi/bin/dumpi2ascii.c: In function ‘print_addresses’:
llu’ expects argument of type ‘long long unsigned int’, but argument 3 has type ‘uint64_t {aka const long unsigned int}’ [-Wformat=]
llu has label %s\n", addresses[i], names[i]);
                                          ^
  CC       dumpi2ascii-callbacks.o
  CCLD     dumpi2ascii
  CC       dumpi2dumpi.o
  CC       dumpi2dumpi-opts.o
  CC       dumpi2dumpi-help.o
  CC       dumpi2dumpi-meta.o
  CC       dumpi2dumpi-callbacks.o
  CCLD     dumpi2dumpi
  CXX      dumpistats.o
  CXX      dumpistats-timebin.o
  CXX      dumpistats-gatherbin.o
  CXX      dumpistats-callbacks.o
  CXX      dumpistats-handlers.o
  CXX      trace.o
  CXX      metadata.o
  CXX      sharedstate.o
  CXX      sharedstate-commconstruct.o
  CXXLD    dumpistats
  CC       ascii2dumpi.o
../../../dumpi/bin/ascii2dumpi.c: In function ‘save_old_starttime’:
../../../dumpi/bin/ascii2dumpi.c:449:3: warning: implicit declaration of function ‘strptime’ [-Wimplicit-function-declaration]
a %b %d %T %Z %Y", &tmo);
   ^~~~~~~~
  CCLD     ascii2dumpi
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/bin »
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
Making all in tests
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/tests »
make[1]: rien à faire pour « all ».
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/tests »
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build »
make[1]: rien à faire pour « all-am ».
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build »
Making install in dumpi
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
Making install in common
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/common »
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/common »
make[3]: rien à faire pour « install-exec-am ».
 /bin/mkdir -p '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/include/dumpi/common'
 /usr/bin/install -c -m 644 ../../../dumpi/common/argtypes.h ../../../dumpi/common/debugflags.h ../../../dumpi/common/funclabels.h ../../../dumpi/common/gettime.h ../../../dumpi/common/io.h ../../../dumpi/common/perfctrs.h ../../../dumpi/common/settings.h ../../../dumpi/common/constants.h ../../../dumpi/common/dumpiio.h ../../../dumpi/common/funcs.h ../../../dumpi/common/hashmap.h ../../../dumpi/common/iodefs.h ../../../dumpi/common/perfctrtags.h ../../../dumpi/common/types.h ../../../dumpi/common/byteswap.h '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/include/dumpi/common'
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/common »
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/common »
Making install in libdumpi
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libdumpi »
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libdumpi »
 /bin/mkdir -p '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib'
 /bin/bash ../../libtool   --mode=install /usr/bin/install -c   libdumpi.la '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib'
libtool: install: /usr/bin/install -c .libs/libdumpi.so.7.0.0 /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libdumpi.so.7.0.0
libtool: install: (cd /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib && { ln -s -f libdumpi.so.7.0.0 libdumpi.so.7 || { rm -f libdumpi.so.7 && ln -s libdumpi.so.7.0.0 libdumpi.so.7; }; })
libtool: install: (cd /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib && { ln -s -f libdumpi.so.7.0.0 libdumpi.so || { rm -f libdumpi.so && ln -s libdumpi.so.7.0.0 libdumpi.so; }; })
libtool: install: /usr/bin/install -c .libs/libdumpi.lai /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libdumpi.la
libtool: install: /usr/bin/install -c .libs/libdumpi.a /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libdumpi.a
libtool: install: chmod 644 /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libdumpi.a
libtool: install: ranlib /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libdumpi.a
libtool: finish: PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/sbin" ldconfig -n /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib
----------------------------------------------------------------------
Libraries have been installed in:
   /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib

If you ever happen to want to link against installed libraries
in a given directory, LIBDIR, you must either use libtool, and
specify the full pathname of the library, or use the '-LLIBDIR'
flag during linking and do at least one of the following:
   - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable
     during execution
   - add LIBDIR to the 'LD_RUN_PATH' environment variable
     during linking
   - use the '-Wl,-rpath -Wl,LIBDIR' linker flag
   - have your system administrator add LIBDIR to '/etc/ld.so.conf'

See any operating system documentation about shared libraries for
more information, such as the ld(1) and ld.so(8) manual pages.
----------------------------------------------------------------------
 /bin/mkdir -p '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/include/dumpi/libdumpi'
 /usr/bin/install -c -m 644 ../../../dumpi/libdumpi/callprofile-addrset.h ../../../dumpi/libdumpi/callprofile.h ../../../dumpi/libdumpi/data.h ../../../dumpi/libdumpi/fused-bindings.h ../../../dumpi/libdumpi/init.h ../../../dumpi/libdumpi/libdumpi.h ../../../dumpi/libdumpi/mpibindings-maps.h ../../../dumpi/libdumpi/mpibindings.h ../../../dumpi/libdumpi/mpibindings-utils.h ../../../dumpi/libdumpi/tof77.h '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/include/dumpi/libdumpi'
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libdumpi »
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libdumpi »
Making install in libundumpi
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libundumpi »
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libundumpi »
 /bin/mkdir -p '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib'
 /bin/bash ../../libtool   --mode=install /usr/bin/install -c   libundumpi.la '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib'
libtool: install: /usr/bin/install -c .libs/libundumpi.so.7.0.0 /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libundumpi.so.7.0.0
libtool: install: (cd /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib && { ln -s -f libundumpi.so.7.0.0 libundumpi.so.7 || { rm -f libundumpi.so.7 && ln -s libundumpi.so.7.0.0 libundumpi.so.7; }; })
libtool: install: (cd /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib && { ln -s -f libundumpi.so.7.0.0 libundumpi.so || { rm -f libundumpi.so && ln -s libundumpi.so.7.0.0 libundumpi.so; }; })
libtool: install: /usr/bin/install -c .libs/libundumpi.lai /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libundumpi.la
libtool: install: /usr/bin/install -c .libs/libundumpi.a /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libundumpi.a
libtool: install: chmod 644 /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libundumpi.a
libtool: install: ranlib /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libundumpi.a
libtool: finish: PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/sbin" ldconfig -n /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib
----------------------------------------------------------------------
Libraries have been installed in:
   /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib

If you ever happen to want to link against installed libraries
in a given directory, LIBDIR, you must either use libtool, and
specify the full pathname of the library, or use the '-LLIBDIR'
flag during linking and do at least one of the following:
   - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable
     during execution
   - add LIBDIR to the 'LD_RUN_PATH' environment variable
     during linking
   - use the '-Wl,-rpath -Wl,LIBDIR' linker flag
   - have your system administrator add LIBDIR to '/etc/ld.so.conf'

See any operating system documentation about shared libraries for
more information, such as the ld(1) and ld.so(8) manual pages.
----------------------------------------------------------------------
 /bin/mkdir -p '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/include/dumpi/libundumpi'
 /usr/bin/install -c -m 644 ../../../dumpi/libundumpi/bindings.h ../../../dumpi/libundumpi/callbacks.h ../../../dumpi/libundumpi/dumpistate.h ../../../dumpi/libundumpi/freedefs.h ../../../dumpi/libundumpi/libundumpi.h '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/include/dumpi/libundumpi'
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libundumpi »
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/libundumpi »
Making install in bin
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/bin »
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/bin »
 /bin/mkdir -p '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/bin'
  /bin/bash ../../libtool   --mode=install /usr/bin/install -c dumpi2ascii dumpi2dumpi dumpistats ascii2dumpi '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/bin'
libtool: install: /usr/bin/install -c .libs/dumpi2ascii /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/bin/dumpi2ascii
libtool: install: /usr/bin/install -c .libs/dumpi2dumpi /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/bin/dumpi2dumpi
libtool: install: /usr/bin/install -c .libs/dumpistats /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/bin/dumpistats
libtool: install: /usr/bin/install -c .libs/ascii2dumpi /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/bin/ascii2dumpi
make[3]: rien à faire pour « install-data-am ».
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/bin »
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi/bin »
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
make[3]: rien à faire pour « install-exec-am ».
 /bin/mkdir -p '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/include/dumpi'
 /usr/bin/install -c -m 644 ../../dumpi/dumpiconfig.h dumpiconfig-generated.h '/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/include/dumpi'
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/dumpi »
Making install in tests
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/tests »
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/tests »
make[2]: rien à faire pour « install-exec-am ».
make[2]: rien à faire pour « install-data-am ».
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/tests »
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build/tests »
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build »
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build »
make[2]: rien à faire pour « install-exec-am ».
make[2]: rien à faire pour « install-data-am ».
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build »
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/build »
cd docs && doxygen doxygen.cfg
warning: Tag `USE_WINDOWS_ENCODING' at line 54 of file `doxygen.cfg' has become obsolete.
         To avoid this warning please remove this line from your configuration file or upgrade it using "doxygen -u"
warning: Tag `DETAILS_AT_TOP' at line 136 of file `doxygen.cfg' has become obsolete.
         To avoid this warning please remove this line from your configuration file or upgrade it using "doxygen -u"
warning: Tag `HTML_ALIGN_MEMBERS' at line 579 of file `doxygen.cfg' has become obsolete.
         To avoid this warning please remove this line from your configuration file or upgrade it using "doxygen -u"
warning: Tag `XML_SCHEMA' at line 811 of file `doxygen.cfg' has become obsolete.
         To avoid this warning please remove this line from your configuration file or upgrade it using "doxygen -u"
warning: Tag `XML_DTD' at line 817 of file `doxygen.cfg' has become obsolete.
         To avoid this warning please remove this line from your configuration file or upgrade it using "doxygen -u"
warning: Tag `MAX_DOT_GRAPH_WIDTH' at line 1078 of file `doxygen.cfg' has become obsolete.
         To avoid this warning please remove this line from your configuration file or upgrade it using "doxygen -u"
warning: Tag `MAX_DOT_GRAPH_HEIGHT' at line 1086 of file `doxygen.cfg' has become obsolete.
         To avoid this warning please remove this line from your configuration file or upgrade it using "doxygen -u"
Notice: Output directory `../docs/dumpi' does not exist. I have created it for you.
Searching for include files...
Searching for example files...
Searching for images...
Searching for dot files...
Searching for msc files...
Searching for dia files...
Searching for files to exclude
Searching INPUT for files to process...
Searching for files in directory /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi
Searching for files in directory /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin
Searching for files in directory /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common
Searching for files in directory /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi
Searching for files in directory /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi
Searching for files in directory /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test
Searching for files in directory /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs
Reading and parsing tag files
Parsing files
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/comm.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/comm.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2ascii-callbacks.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2ascii-callbacks.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2ascii-defs.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2ascii-defs.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2dumpi.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2dumpi.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-binbase.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-binbase.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-callbacks.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-callbacks.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-gatherbin.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-gatherbin.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-handlers.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-handlers.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-timebin.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-timebin.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/group.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/group.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/metadata.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/metadata.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/otfcomplete.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/otfcomplete.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/otfwriter.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/otfwriter.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/sharedstate-commconstruct.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/sharedstate-commconstruct.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/sharedstate.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/sharedstate.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/timeutils.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/timeutils.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/trace.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/trace.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/type.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/type.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/argtypes.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/argtypes.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/byteswap.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/byteswap.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/constants.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/constants.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/debugflags.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/debugflags.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/dumpiio.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/dumpiio.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/funclabels.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/funclabels.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/funcs.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/funcs.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/gettime.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/gettime.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/hashmap.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/hashmap.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/io.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/io.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/iodefs.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/iodefs.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/perfctrs.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/perfctrs.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/perfctrtags.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/perfctrtags.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/settings.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/settings.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/types.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/types.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/dumpiconfig.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/dumpiconfig.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/callprofile-addrset.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/callprofile-addrset.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/callprofile.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/callprofile.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/data.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/data.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/fused-bindings.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/fused-bindings.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/init.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/init.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/libdumpi.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/libdumpi.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings-maps.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings-maps.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings-utils.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings-utils.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/tof77.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/tof77.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/bindings.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/bindings.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/callbacks.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/callbacks.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/dumpistate.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/dumpistate.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/freedefs.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/freedefs.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/libundumpi.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/libundumpi.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/coll.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/coll.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/manip.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/manip.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/p2p.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/p2p.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/probe.h...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/probe.h...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/compiling.dox...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/compiling.dox...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/dumpi.dox...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/dumpi.dox...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/issues.dox...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/issues.dox...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/oaq.dox...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/oaq.dox...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/tools.dox...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/tools.dox...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/traceformat.dox...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/traceformat.dox...
Preprocessing /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox...
Parsing file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox...
Building group list...
Building directory list...
Building namespace list...
Building file list...
Building class list...
Associating documentation with classes...
Computing nesting relations for classes...
Building example list...
Searching for enumerations...
Searching for documented typedefs...
Searching for members imported via using declarations...
Searching for included using directives...
Searching for documented variables...
Building interface member list...
Building member list...
Searching for friends...
Searching for documented defines...
Computing class inheritance relations...
Computing class usage relations...
Flushing cached template relations that have become invalid...
Computing class relations...
Add enum values to enums...
Searching for member function documentation...
Creating members for template instances...
Building page list...
Search for main page...
Computing page relations...
Determining the scope of groups...
Sorting lists...
Freeing entry tree
Determining which enums are documented
Computing member relations...
Building full member lists recursively...
Adding members to member groups.
Computing member references...
Inheriting documentation...
Generating disk names...
Adding source references...
Adding xrefitems...
Sorting member lists...
Computing dependencies between directories...
Generating citations page...
Counting data structures...
Resolving user defined references...
Finding anchors and sections in the documentation...
Transferring function references...
Combining using relations...
Adding members to index pages...
Generating style sheet...
Generating search indices...
Generating example documentation...
Generating file sources...
Parsing code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/compiling.dox...
Parsing code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/dumpi.dox...
Parsing code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/issues.dox...
Parsing code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/oaq.dox...
Parsing code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/tools.dox...
Parsing code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/traceformat.dox...
Parsing code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/comm.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2ascii-callbacks.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2ascii-defs.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2dumpi.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-binbase.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-callbacks.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-gatherbin.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-handlers.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-timebin.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/group.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/metadata.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/otfcomplete.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/otfwriter.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/sharedstate-commconstruct.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/sharedstate.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/timeutils.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/trace.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/type.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/argtypes.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/byteswap.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/constants.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/debugflags.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/dumpiio.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/funclabels.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/funcs.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/gettime.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/hashmap.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/io.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/iodefs.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/perfctrs.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/perfctrtags.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/settings.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/types.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/dumpiconfig.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/callprofile-addrset.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/callprofile.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/data.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/fused-bindings.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/init.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/libdumpi.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings-maps.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings-utils.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/tof77.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/bindings.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/callbacks.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/dumpistate.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/freedefs.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/libundumpi.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/coll.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/manip.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/p2p.h...
Generating code for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/probe.h...
Generating file documentation...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/compiling.dox...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/dumpi.dox...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/issues.dox...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/oaq.dox...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/tools.dox...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/traceformat.dox...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/comm.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2ascii-callbacks.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2ascii-defs.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpi2dumpi.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-binbase.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-callbacks.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-gatherbin.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-handlers.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-timebin.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/group.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/metadata.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/otfcomplete.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/otfwriter.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/sharedstate-commconstruct.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/sharedstate.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/timeutils.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/trace.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/type.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/argtypes.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/byteswap.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/constants.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/debugflags.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/dumpiio.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/funclabels.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/funcs.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/gettime.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/hashmap.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/io.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/iodefs.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/perfctrs.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/perfctrtags.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/settings.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/common/types.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/dumpiconfig.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/callprofile-addrset.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/callprofile.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/data.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/fused-bindings.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/init.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/libdumpi.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings-maps.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings-utils.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/mpibindings.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libdumpi/tof77.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/bindings.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/callbacks.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/dumpistate.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/freedefs.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/libundumpi.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/coll.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/manip.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/p2p.h...
Generating docs for file /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/test/probe.h...
Generating page documentation...
Generating docs for page compiling...
Generating docs for page known_issues...
Generating docs for page oaq...
Generating docs for page tools...
Generating docs for page traceformat...
Generating docs for page using...
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:77: warning: Found unknown command `\machine'
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:88: warning: Found unknown command `\machine'
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:89: warning: Found unknown command `\machine'
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:90: warning: Found unknown command `\machine'
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:92: warning: explicit link request to 'PBS' could not be resolved
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:93: warning: explicit link request to 'PBS' could not be resolved
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:94: warning: explicit link request to 'PBS' could not be resolved
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:95: warning: explicit link request to 'PBS' could not be resolved
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:96: warning: explicit link request to 'PBS' could not be resolved
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:101: warning: Found unknown command `\machine'
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:106: warning: Found unknown command `\machine'
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/docs/user.dox:109: warning: Found unknown command `\machine'
found
Generating group documentation...
found
found
found
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/libundumpi/libundumpi.h:112: warning: The following parameters of undumpi_read_stream_full(const char *metaname, dumpi_profile *profile, const libundumpi_callbacks *callback, void *userarg, bool print_progress, double percent_terminate) are not documented:
  parameter 'metaname'
  parameter 'print_progress'
  parameter 'percent_terminate'
Generating class documentation...
Generating docs for compound d2a_addrmap...
Generating docs for compound d2dmeta...
Generating docs for compound d2dopts...
Generating docs for compound dumpi_abort...
Generating docs for compound dumpi_accumulate...
Generating docs for compound dumpi_add_error_class...
Generating docs for compound dumpi_add_error_code...
Generating docs for compound dumpi_add_error_string...
Generating docs for compound dumpi_addr_iterator...
Generating docs for compound dumpi_address...
Generating docs for compound dumpi_allgather...
Generating docs for compound dumpi_allgatherv...
Generating docs for compound dumpi_alloc_mem...
Generating docs for compound dumpi_allreduce...
Generating docs for compound dumpi_alltoall...
Generating docs for compound dumpi_alltoallv...
Generating docs for compound dumpi_alltoallw...
Generating docs for compound dumpi_attr_delete...
Generating docs for compound dumpi_attr_get...
Generating docs for compound dumpi_attr_put...
Generating docs for compound dumpi_barrier...
Generating docs for compound dumpi_bcast...
Generating docs for compound dumpi_bsend...
Generating docs for compound dumpi_bsend_init...
Generating docs for compound dumpi_buffer_attach...
Generating docs for compound dumpi_buffer_detach...
Generating docs for compound dumpi_cancel...
Generating docs for compound dumpi_cart_coords...
Generating docs for compound dumpi_cart_create...
Generating docs for compound dumpi_cart_get...
Generating docs for compound dumpi_cart_map...
Generating docs for compound dumpi_cart_rank...
Generating docs for compound dumpi_cart_shift...
Generating docs for compound dumpi_cart_sub...
Generating docs for compound dumpi_cartdim_get...
Generating docs for compound dumpi_clock...
Generating docs for compound dumpi_clock_pair...
Generating docs for compound dumpi_close_port...
Generating docs for compound dumpi_comm_accept...
Generating docs for compound dumpi_comm_call_errhandler...
Generating docs for compound dumpi_comm_compare...
Generating docs for compound dumpi_comm_connect...
Generating docs for compound dumpi_comm_create...
Generating docs for compound dumpi_comm_create_errhandler...
Generating docs for compound dumpi_comm_create_keyval...
Generating docs for compound dumpi_comm_delete_attr...
Generating docs for compound dumpi_comm_disconnect...
Generating docs for compound dumpi_comm_dup...
Generating docs for compound dumpi_comm_free...
Generating docs for compound dumpi_comm_free_keyval...
Generating docs for compound dumpi_comm_get_attr...
Generating docs for compound dumpi_comm_get_errhandler...
Generating docs for compound dumpi_comm_get_name...
Generating docs for compound dumpi_comm_get_parent...
Generating docs for compound dumpi_comm_group...
Generating docs for compound dumpi_comm_join...
Generating docs for compound dumpi_comm_rank...
Generating docs for compound dumpi_comm_remote_group...
Generating docs for compound dumpi_comm_remote_size...
Generating docs for compound dumpi_comm_set_attr...
Generating docs for compound dumpi_comm_set_errhandler...
Generating docs for compound dumpi_comm_set_name...
Generating docs for compound dumpi_comm_size...
Generating docs for compound dumpi_comm_spawn...
Generating docs for compound dumpi_comm_spawn_multiple...
Generating docs for compound dumpi_comm_split...
Generating docs for compound dumpi_comm_test_inter...
Generating docs for compound dumpi_dims_create...
Generating docs for compound dumpi_errhandler_create...
Generating docs for compound dumpi_errhandler_free...
Generating docs for compound dumpi_errhandler_get...
Generating docs for compound dumpi_errhandler_set...
Generating docs for compound dumpi_error_class...
Generating docs for compound dumpi_error_string...
Generating docs for compound dumpi_exscan...
Generating docs for compound dumpi_file_call_errhandler...
Generating docs for compound dumpi_file_close...
Generating docs for compound dumpi_file_create_errhandler...
Generating docs for compound dumpi_file_delete...
Generating docs for compound dumpi_file_get_amode...
Generating docs for compound dumpi_file_get_atomicity...
Generating docs for compound dumpi_file_get_byte_offset...
Generating docs for compound dumpi_file_get_errhandler...
Generating docs for compound dumpi_file_get_group...
Generating docs for compound dumpi_file_get_info...
Generating docs for compound dumpi_file_get_position...
Generating docs for compound dumpi_file_get_position_shared...
Generating docs for compound dumpi_file_get_size...
Generating docs for compound dumpi_file_get_type_extent...
Generating docs for compound dumpi_file_get_view...
Generating docs for compound dumpi_file_iread...
Generating docs for compound dumpi_file_iread_at...
Generating docs for compound dumpi_file_iread_shared...
Generating docs for compound dumpi_file_iwrite...
Generating docs for compound dumpi_file_iwrite_at...
Generating docs for compound dumpi_file_iwrite_shared...
Generating docs for compound dumpi_file_open...
Generating docs for compound dumpi_file_preallocate...
Generating docs for compound dumpi_file_read...
Generating docs for compound dumpi_file_read_all...
Generating docs for compound dumpi_file_read_all_begin...
Generating docs for compound dumpi_file_read_all_end...
Generating docs for compound dumpi_file_read_at...
Generating docs for compound dumpi_file_read_at_all...
Generating docs for compound dumpi_file_read_at_all_begin...
Generating docs for compound dumpi_file_read_at_all_end...
Generating docs for compound dumpi_file_read_ordered...
Generating docs for compound dumpi_file_read_ordered_begin...
Generating docs for compound dumpi_file_read_ordered_end...
Generating docs for compound dumpi_file_read_shared...
Generating docs for compound dumpi_file_seek...
Generating docs for compound dumpi_file_seek_shared...
Generating docs for compound dumpi_file_set_atomicity...
Generating docs for compound dumpi_file_set_errhandler...
Generating docs for compound dumpi_file_set_info...
Generating docs for compound dumpi_file_set_size...
Generating docs for compound dumpi_file_set_view...
Generating docs for compound dumpi_file_sync...
Generating docs for compound dumpi_file_write...
Generating docs for compound dumpi_file_write_all...
Generating docs for compound dumpi_file_write_all_begin...
Generating docs for compound dumpi_file_write_all_end...
Generating docs for compound dumpi_file_write_at...
Generating docs for compound dumpi_file_write_at_all...
Generating docs for compound dumpi_file_write_at_all_begin...
Generating docs for compound dumpi_file_write_at_all_end...
Generating docs for compound dumpi_file_write_ordered...
Generating docs for compound dumpi_file_write_ordered_begin...
Generating docs for compound dumpi_file_write_ordered_end...
Generating docs for compound dumpi_file_write_shared...
Generating docs for compound dumpi_finalize...
Generating docs for compound dumpi_finalized...
Generating docs for compound dumpi_footer...
Generating docs for compound dumpi_free_mem...
Generating docs for compound dumpi_func_call...
Generating docs for compound dumpi_gather...
Generating docs for compound dumpi_gatherv...
Generating docs for compound dumpi_get...
Generating docs for compound dumpi_get_address...
Generating docs for compound dumpi_get_count...
Generating docs for compound dumpi_get_elements...
Generating docs for compound dumpi_get_processor_name...
Generating docs for compound dumpi_get_version...
Generating docs for compound dumpi_global_t...
Generating docs for compound dumpi_graph_create...
Generating docs for compound dumpi_graph_get...
Generating docs for compound dumpi_graph_map...
Generating docs for compound dumpi_graph_neighbors...
Generating docs for compound dumpi_graph_neighbors_count...
Generating docs for compound dumpi_graphdims_get...
Generating docs for compound dumpi_grequest_complete...
Generating docs for compound dumpi_grequest_start...
Generating docs for compound dumpi_group_compare...
Generating docs for compound dumpi_group_difference...
Generating docs for compound dumpi_group_excl...
Generating docs for compound dumpi_group_free...
Generating docs for compound dumpi_group_incl...
Generating docs for compound dumpi_group_intersection...
Generating docs for compound dumpi_group_range_excl...
Generating docs for compound dumpi_group_range_incl...
Generating docs for compound dumpi_group_rank...
Generating docs for compound dumpi_group_size...
Generating docs for compound dumpi_group_translate_ranks...
Generating docs for compound dumpi_group_union...
Generating docs for compound dumpi_header...
Generating docs for compound dumpi_ibsend...
Generating docs for compound dumpi_info_create...
Generating docs for compound dumpi_info_delete...
Generating docs for compound dumpi_info_dup...
Generating docs for compound dumpi_info_free...
Generating docs for compound dumpi_info_get...
Generating docs for compound dumpi_info_get_nkeys...
Generating docs for compound dumpi_info_get_nthkey...
Generating docs for compound dumpi_info_get_valuelen...
Generating docs for compound dumpi_info_set...
Generating docs for compound dumpi_init...
Generating docs for compound dumpi_init_thread...
Generating docs for compound dumpi_initialized...
Generating docs for compound dumpi_intercomm_create...
Generating docs for compound dumpi_intercomm_merge...
Generating docs for compound dumpi_iprobe...
Generating docs for compound dumpi_irecv...
Generating docs for compound dumpi_irsend...
Generating docs for compound dumpi_is_thread_main...
Generating docs for compound dumpi_isend...
Generating docs for compound dumpi_issend...
Generating docs for compound dumpi_keyval_create...
Generating docs for compound dumpi_keyval_entry...
Generating docs for compound dumpi_keyval_free...
Generating docs for compound dumpi_keyval_record...
Generating docs for compound dumpi_lookup_name...
Generating docs for compound dumpi_op_create...
Generating docs for compound dumpi_op_free...
Generating docs for compound dumpi_open_port...
Generating docs for compound dumpi_outputs...
Generating docs for compound dumpi_pack...
Generating docs for compound dumpi_pack_external...
Generating docs for compound dumpi_pack_external_size...
Generating docs for compound dumpi_pack_size...
Generating docs for compound dumpi_perfinfo...
Generating docs for compound dumpi_probe...
Generating docs for compound dumpi_profile...
Generating docs for compound dumpi_publish_name...
Generating docs for compound dumpi_put...
Generating docs for compound dumpi_query_thread...
Generating docs for compound dumpi_recv...
Generating docs for compound dumpi_recv_init...
Generating docs for compound dumpi_reduce...
Generating docs for compound dumpi_reduce_scatter...
Generating docs for compound dumpi_register_datarep...
Generating docs for compound dumpi_request_free...
Generating docs for compound dumpi_request_get_status...
Generating docs for compound dumpi_rsend...
Generating docs for compound dumpi_rsend_init...
Generating docs for compound dumpi_scan...
Generating docs for compound dumpi_scatter...
Generating docs for compound dumpi_scatterv...
Generating docs for compound dumpi_send...
Generating docs for compound dumpi_send_init...
Generating docs for compound dumpi_sendrecv...
Generating docs for compound dumpi_sendrecv_replace...
Generating docs for compound dumpi_sizeof...
Generating docs for compound dumpi_ssend...
Generating docs for compound dumpi_ssend_init...
Generating docs for compound dumpi_start...
Generating docs for compound dumpi_startall...
Generating docs for compound dumpi_status...
Generating docs for compound dumpi_status_set_cancelled...
Generating docs for compound dumpi_status_set_elements...
Generating docs for compound dumpi_test...
Generating docs for compound dumpi_test_cancelled...
Generating docs for compound dumpi_testall...
Generating docs for compound dumpi_testany...
Generating docs for compound dumpi_testsome...
Generating docs for compound dumpi_time...
Generating docs for compound dumpi_topo_test...
Generating docs for compound dumpi_type_commit...
Generating docs for compound dumpi_type_contiguous...
Generating docs for compound dumpi_type_create_darray...
Generating docs for compound dumpi_type_create_hindexed...
Generating docs for compound dumpi_type_create_hvector...
Generating docs for compound dumpi_type_create_indexed_block...
Generating docs for compound dumpi_type_create_keyval...
Generating docs for compound dumpi_type_create_resized...
Generating docs for compound dumpi_type_create_struct...
Generating docs for compound dumpi_type_create_subarray...
Generating docs for compound dumpi_type_delete_attr...
Generating docs for compound dumpi_type_dup...
Generating docs for compound dumpi_type_extent...
Generating docs for compound dumpi_type_free...
Generating docs for compound dumpi_type_free_keyval...
Generating docs for compound dumpi_type_get_attr...
Generating docs for compound dumpi_type_get_contents...
Generating docs for compound dumpi_type_get_envelope...
Generating docs for compound dumpi_type_get_extent...
Generating docs for compound dumpi_type_get_name...
Generating docs for compound dumpi_type_get_true_extent...
Generating docs for compound dumpi_type_hindexed...
Generating docs for compound dumpi_type_hvector...
Generating docs for compound dumpi_type_indexed...
Generating docs for compound dumpi_type_lb...
Generating docs for compound dumpi_type_match_size...
Generating docs for compound dumpi_type_set_attr...
Generating docs for compound dumpi_type_set_name...
Generating docs for compound dumpi_type_size...
Generating docs for compound dumpi_type_struct...
Generating docs for compound dumpi_type_ub...
Generating docs for compound dumpi_type_vector...
Generating docs for compound dumpi_unpack...
Generating docs for compound dumpi_unpack_external...
Generating docs for compound dumpi_unpublish_name...
Generating docs for compound dumpi_wait...
Generating docs for compound dumpi_waitall...
Generating docs for compound dumpi_waitany...
Generating docs for compound dumpi_waitsome...
Generating docs for compound dumpi_win_call_errhandler...
Generating docs for compound dumpi_win_complete...
Generating docs for compound dumpi_win_create...
Generating docs for compound dumpi_win_create_errhandler...
Generating docs for compound dumpi_win_create_keyval...
Generating docs for compound dumpi_win_delete_attr...
Generating docs for compound dumpi_win_fence...
Generating docs for compound dumpi_win_free...
Generating docs for compound dumpi_win_free_keyval...
Generating docs for compound dumpi_win_get_attr...
Generating docs for compound dumpi_win_get_errhandler...
Generating docs for compound dumpi_win_get_group...
Generating docs for compound dumpi_win_get_name...
Generating docs for compound dumpi_win_lock...
Generating docs for compound dumpi_win_post...
Generating docs for compound dumpi_win_set_attr...
Generating docs for compound dumpi_win_set_errhandler...
Generating docs for compound dumpi_win_set_name...
Generating docs for compound dumpi_win_start...
Generating docs for compound dumpi_win_test...
Generating docs for compound dumpi_win_unlock...
Generating docs for compound dumpi_win_wait...
Generating docs for compound dumpi_wtick...
Generating docs for compound dumpi_wtime...
Generating docs for compound dumpio_test...
Generating docs for compound dumpio_testall...
Generating docs for compound dumpio_testany...
Generating docs for compound dumpio_testsome...
Generating docs for compound dumpio_wait...
Generating docs for compound dumpio_waitall...
Generating docs for compound dumpio_waitany...
Generating docs for compound dumpio_waitsome...
Generating docs for compound libundumpi_callbacks...
Generating docs for compound libundumpi_cbpair...
Generating namespace index...
Generating docs for namespace dumpi
Generating docs for compound dumpi::binbase...
Generating docs for compound dumpi::callbacks...
Generating docs for compound dumpi::comm...
Generating docs for compound dumpi::counter...
Generating docs for compound dumpi::exchanger...
Generating docs for compound dumpi::gatherbin...
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-gatherbin.h:138: warning: argument 'start_patt' of command @param is not found in the argument list of dumpi::gatherbin::gatherbin(const std::string &expression, bool accumulate)
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-gatherbin.h:138: warning: argument 'end_patt' of command @param is not found in the argument list of dumpi::gatherbin::gatherbin(const std::string &expression, bool accumulate)
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/dumpi/bin/dumpistats-gatherbin.h:144: warning: The following parameters of dumpi::gatherbin::gatherbin(const std::string &expression, bool accumulate) are not documented:
  parameter 'expression'
found
found
found
Generating docs for compound dumpi::group...
Generating docs for compound dumpi::handlerbase...
Generating docs for compound dumpi::lumper...
Generating docs for compound dumpi::metadata...
Generating docs for compound dumpi::otfcomplete...
Generating docs for nested compound dumpi::otfcomplete::args...
Generating docs for compound dumpi::otfwriter...
Generating docs for compound dumpi::perfcounter...
Generating docs for compound dumpi::recver...
Generating docs for compound dumpi::sender...
Generating docs for compound dumpi::sharedstate...
Generating docs for nested compound dumpi::sharedstate::commconstruct...
Generating docs for nested compound dumpi::sharedstate::commcreate...
Generating docs for nested compound dumpi::sharedstate::commdup...
Generating docs for nested compound dumpi::sharedstate::commsplit...
Generating docs for nested compound dumpi::sharedstate::entrant...
Generating docs for compound dumpi::timebin...
found
found
found
Generating docs for compound dumpi::timer...
Generating docs for compound dumpi::trace...
Generating docs for nested compound dumpi::trace::commentry...
Generating docs for nested compound dumpi::trace::groupentry...
Generating docs for nested compound dumpi::trace::typeentry...
Generating docs for compound dumpi::type...
Generating graph info page...
Generating directory documentation...
Generating index page...
Generating page index...
Generating module index...
Generating namespace index...
Generating namespace member index...
Generating annotated compound index...
Generating alphabetical compound index...
Generating hierarchical class index...
Generating member index...
Generating file index...
Generating file member index...
Generating example index...
finalizing index lists...
writing tag file...
lookup cache used 8226/65536 hits=64609 misses=8674
finished...
#+end_example

Il faut ensuite ajouter le chemin de la librairie dans
=LD_LIBRARY_PATH=.

***** HPL                                                           :HPL:

Téléchargez le programme [[http://www.netlib.org/benchmark/hpl/hpl-2.2.tar.gz][ici]], ou en ligne de commande
#+begin_src sh :session foo
cd ~/Documents/Stage_LIG_2017/logiciels
wget http://www.netlib.org/benchmark/hpl/hpl-2.2.tar.gz
tar -xvf hpl-2.2.tar.gz
mv hpl-2.2 hpl
cd hpl/setup
sh make_generic
cp Make.UNKNOWN ../Make.linux
cd ..
#+end_src

Puis remplacer le contenu du fichier par celui ci dessous, en
remplacent les lignes avec commentaire #path/to.

#+BEGIN_EXAMPLE
#  
#  -- High Performance Computing Linpack Benchmark (HPL)                
#     HPL - 2.2 - February 24, 2016                          
#     Antoine P. Petitet                                                
#     University of Tennessee, Knoxville                                
#     Innovative Computing Laboratory                                 
#     (C) Copyright 2000-2008 All Rights Reserved                       
#                                                                       
#  -- Copyright notice and Licensing terms:                             
#                                                                       
#  Redistribution  and  use in  source and binary forms, with or without
#  modification, are  permitted provided  that the following  conditions
#  are met:                                                             
#                                                                       
#  1. Redistributions  of  source  code  must retain the above copyright
#  notice, this list of conditions and the following disclaimer.        
#                                                                       
#  2. Redistributions in binary form must reproduce  the above copyright
#  notice, this list of conditions,  and the following disclaimer in the
#  documentation and/or other materials provided with the distribution. 
#                                                                       
#  3. All  advertising  materials  mentioning  features  or  use of this
#  software must display the following acknowledgement:                 
#  This  product  includes  software  developed  at  the  University  of
#  Tennessee, Knoxville, Innovative Computing Laboratory.             
#                                                                       
#  4. The name of the  University,  the name of the  Laboratory,  or the
#  names  of  its  contributors  may  not  be used to endorse or promote
#  products  derived   from   this  software  without  specific  written
#  permission.                                                          
#                                                                       
#  -- Disclaimer:                                                       
#                                                                       
#  THIS  SOFTWARE  IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
#  ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,  INCLUDING,  BUT NOT
#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY
#  OR  CONTRIBUTORS  BE  LIABLE FOR ANY  DIRECT,  INDIRECT,  INCIDENTAL,
#  SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL DAMAGES  (INCLUDING,  BUT NOT
#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
#  DATA OR PROFITS; OR BUSINESS INTERRUPTION)  HOWEVER CAUSED AND ON ANY
#  THEORY OF LIABILITY, WHETHER IN CONTRACT,  STRICT LIABILITY,  OR TORT
#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
# ######################################################################
#  
# ----------------------------------------------------------------------
# - shell --------------------------------------------------------------
# ----------------------------------------------------------------------
#
SHELL        = /bin/sh
#
CD           = cd
CP           = cp
LN_S         = ln -s
MKDIR        = mkdir
RM           = /bin/rm -f
TOUCH        = touch
#
# ----------------------------------------------------------------------
# - Platform identifier ------------------------------------------------
# ----------------------------------------------------------------------
#
ARCH         = linux #ici mettre le même nom que l'extension donnée au fichier
#
# ----------------------------------------------------------------------
# - HPL Directory Structure / HPL library ------------------------------
# ----------------------------------------------------------------------
#
TOPdir       = $(HOME)/Documents/Stage_LIG_2017/logiciels/hpl #path/to/hpl 
INCdir       = $(TOPdir)/include
BINdir       = $(TOPdir)/bin/$(ARCH)
LIBdir       = $(TOPdir)/lib/$(ARCH)
#
HPLlib       = $(LIBdir)/libhpl.a 
#
# ----------------------------------------------------------------------
# - Message Passing library (MPI) --------------------------------------
# ----------------------------------------------------------------------
# MPinc tells the  C  compiler where to find the Message Passing library
# header files,  MPlib  is defined  to be the name of  the library to be 
# used. The variable MPdir is only used for defining MPinc and MPlib.
#
MPdir        = /usr/lib/mpich      #path/to/mpich
MPinc        = -I $(MPdir)/include
MPlib        = -L $(MPdir)/lib -L /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib -ldumpi #path/to/dumpi
#
# ----------------------------------------------------------------------
# - Linear Algebra library (BLAS or VSIPL) -----------------------------
# ----------------------------------------------------------------------
# LAinc tells the  C  compiler where to find the Linear Algebra  library
# header files,  LAlib  is defined  to be the name of  the library to be 
# used. The variable LAdir is only used for defining LAinc and LAlib.
#
LAdir        = /usr/lib/atlas-base #path/to/lib-atlas
LAinc        = 
LAlib        = $(LAdir)/libf77blas.a $(LAdir)/libatlas.a -lblas
#
# ----------------------------------------------------------------------
# - F77 / C interface --------------------------------------------------
# ----------------------------------------------------------------------
# You can skip this section  if and only if  you are not planning to use
# a  BLAS  library featuring a Fortran 77 interface.  Otherwise,  it  is
# necessary  to  fill out the  F2CDEFS  variable  with  the  appropriate
# options.  **One and only one**  option should be chosen in **each** of
# the 3 following categories:
#
# 1) name space (How C calls a Fortran 77 routine)
#
# -DAdd_              : all lower case and a suffixed underscore  (Suns,
#                       Intel, ...),                           [default]
# -DNoChange          : all lower case (IBM RS6000),
# -DUpCase            : all upper case (Cray),
# -DAdd__             : the FORTRAN compiler in use is f2c.
#
# 2) C and Fortran 77 integer mapping
#
# -DF77_INTEGER=int   : Fortran 77 INTEGER is a C int,         [default]
# -DF77_INTEGER=long  : Fortran 77 INTEGER is a C long,
# -DF77_INTEGER=short : Fortran 77 INTEGER is a C short.
#
# 3) Fortran 77 string handling
#
# -DStringSunStyle    : The string address is passed at the string loca-
#                       tion on the stack, and the string length is then
#                       passed as  an  F77_INTEGER  after  all  explicit
#                       stack arguments,                       [default]
# -DStringStructPtr   : The address  of  a  structure  is  passed  by  a
#                       Fortran 77  string,  and the structure is of the
#                       form: struct {char *cp; F77_INTEGER len;},
# -DStringStructVal   : A structure is passed by value for each  Fortran
#                       77 string,  and  the  structure is  of the form:
#                       struct {char *cp; F77_INTEGER len;},
# -DStringCrayStyle   : Special option for  Cray  machines,  which  uses
#                       Cray  fcd  (fortran  character  descriptor)  for
#                       interoperation.
#
F2CDEFS      = -DAdd_ -DF77_INTEGER=int -DStringSunStyle
#
# ----------------------------------------------------------------------
# - HPL includes / libraries / specifics -------------------------------
# ----------------------------------------------------------------------
#
HPL_INCLUDES = -I$(INCdir) -I$(INCdir)/$(ARCH) $(LAinc) $(MPinc)
HPL_LIBS     = $(HPLlib) $(LAlib) $(MPlib) -lmpl
#
# - Compile time options -----------------------------------------------
#
# -DHPL_COPY_L           force the copy of the panel L before bcast;
# -DHPL_CALL_CBLAS       call the cblas interface;
# -DHPL_CALL_VSIPL       call the vsip  library;
# -DHPL_DETAILED_TIMING  enable detailed timers;
#
# By default HPL will:
#    *) not copy L before broadcast,
#    *) call the BLAS Fortran 77 interface,
#    *) not display detailed timing information.
#
HPL_OPTS     = -DHPL_CALL_CBLAS
# 
# ----------------------------------------------------------------------
#
HPL_DEFS     = $(F2CDEFS) $(HPL_OPTS) $(HPL_INCLUDES) 
#
# ----------------------------------------------------------------------
# - Compilers / linkers - Optimization flags ---------------------------
# ----------------------------------------------------------------------
#
CC           = /usr/bin/mpicc
CCNOOPT      = $(HPL_DEFS) 
CCFLAGS      = $(HPL_DEFS) -fomit-frame-pointer -O3 -funroll-loops
#
LINKER       = /usr/bin/mpicc
LINKFLAGS    = $(CCFLAGS) -pthread
#
ARCHIVER     = ar
ARFLAGS      = r
RANLIB       = echo
#
# ----------------------------------------------------------------------
#+END_EXAMPLE

#+begin_src sh :session foo :results output :exports both 
make arch=linux
#+end_src

#+RESULTS:
#+begin_example
make -f Make.top startup_dir     arch=linux
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
mkdir include/linux
mkdir: impossible de créer le répertoire «include/linux»: Le fichier existe
Make.top:74 : la recette pour la cible « startup_dir » a échouée
make[1]: [startup_dir] Erreur 1 (ignorée)
mkdir lib
mkdir: impossible de créer le répertoire «lib»: Le fichier existe
Make.top:74 : la recette pour la cible « startup_dir » a échouée
make[1]: [startup_dir] Erreur 1 (ignorée)
mkdir lib/linux
mkdir: impossible de créer le répertoire «lib/linux»: Le fichier existe
Make.top:74 : la recette pour la cible « startup_dir » a échouée
make[1]: [startup_dir] Erreur 1 (ignorée)
mkdir bin
mkdir: impossible de créer le répertoire «bin»: Le fichier existe
Make.top:74 : la recette pour la cible « startup_dir » a échouée
make[1]: [startup_dir] Erreur 1 (ignorée)
mkdir bin/linux
mkdir: impossible de créer le répertoire «bin/linux»: Le fichier existe
Make.top:74 : la recette pour la cible « startup_dir » a échouée
make[1]: [startup_dir] Erreur 1 (ignorée)
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top startup_src     arch=linux
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=src/auxil       arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd src/auxil ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd src/auxil/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=src/blas        arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd src/blas ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd src/blas/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=src/comm        arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd src/comm ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd src/comm/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=src/grid        arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd src/grid ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd src/grid/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=src/panel       arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd src/panel ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd src/panel/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=src/pauxil      arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd src/pauxil ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd src/pauxil/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=src/pfact       arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd src/pfact ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd src/pfact/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=src/pgesv       arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd src/pgesv ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd src/pgesv/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top startup_tst     arch=linux
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=testing/matgen  arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd testing/matgen ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd testing/matgen/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=testing/timer   arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd testing/timer ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd testing/timer/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=testing/pmatgen arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd testing/pmatgen ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd testing/pmatgen/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=testing/ptimer  arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd testing/ptimer ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd testing/ptimer/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top leaf le=testing/ptest   arch=linux
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd testing/ptest ; mkdir linux )
mkdir: impossible de créer le répertoire «linux»: Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
( cd testing/ptest/linux ; \
            ln -s /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/Make.linux Make.inc )
ln: impossible de créer le lien symbolique 'Make.inc': Le fichier existe
Make.top:188 : la recette pour la cible « leaf » a échouée
make[2]: [leaf] Erreur 1 (ignorée)
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top refresh_src     arch=linux
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
cp makes/Make.auxil    src/auxil/linux/Makefile
cp makes/Make.blas     src/blas/linux/Makefile
cp makes/Make.comm     src/comm/linux/Makefile
cp makes/Make.grid     src/grid/linux/Makefile
cp makes/Make.panel    src/panel/linux/Makefile
cp makes/Make.pauxil   src/pauxil/linux/Makefile
cp makes/Make.pfact    src/pfact/linux/Makefile
cp makes/Make.pgesv    src/pgesv/linux/Makefile
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top refresh_tst     arch=linux
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
cp makes/Make.matgen   testing/matgen/linux/Makefile
cp makes/Make.timer    testing/timer/linux/Makefile
cp makes/Make.pmatgen  testing/pmatgen/linux/Makefile
cp makes/Make.ptimer   testing/ptimer/linux/Makefile
cp makes/Make.ptest    testing/ptest/linux/Makefile
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top refresh_src     arch=linux
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
cp makes/Make.auxil    src/auxil/linux/Makefile
cp makes/Make.blas     src/blas/linux/Makefile
cp makes/Make.comm     src/comm/linux/Makefile
cp makes/Make.grid     src/grid/linux/Makefile
cp makes/Make.panel    src/panel/linux/Makefile
cp makes/Make.pauxil   src/pauxil/linux/Makefile
cp makes/Make.pfact    src/pfact/linux/Makefile
cp makes/Make.pgesv    src/pgesv/linux/Makefile
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top refresh_tst     arch=linux
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
cp makes/Make.matgen   testing/matgen/linux/Makefile
cp makes/Make.timer    testing/timer/linux/Makefile
cp makes/Make.pmatgen  testing/pmatgen/linux/Makefile
cp makes/Make.ptimer   testing/ptimer/linux/Makefile
cp makes/Make.ptest    testing/ptest/linux/Makefile
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top build_src       arch=linux
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd src/auxil/linux;         make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/auxil/linux »
/usr/bin/mpicc -o HPL_dlacpy.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlacpy.c
/usr/bin/mpicc -o HPL_dlatcpy.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlatcpy.c
/usr/bin/mpicc -o HPL_fprintf.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_fprintf.c
/usr/bin/mpicc -o HPL_warn.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_warn.c
/usr/bin/mpicc -o HPL_abort.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_abort.c
/usr/bin/mpicc -o HPL_dlaprnt.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaprnt.c
/usr/bin/mpicc -o HPL_dlange.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlange.c
/usr/bin/mpicc -o HPL_dlamch.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include    ../HPL_dlamch.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_dlacpy.o           HPL_dlatcpy.o          HPL_fprintf.o HPL_warn.o             HPL_abort.o            HPL_dlaprnt.o HPL_dlange.o HPL_dlamch.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/auxil/linux »
( cd src/blas/linux;          make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/blas/linux »
/usr/bin/mpicc -o HPL_dcopy.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dcopy.c
/usr/bin/mpicc -o HPL_daxpy.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_daxpy.c
/usr/bin/mpicc -o HPL_dscal.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dscal.c
/usr/bin/mpicc -o HPL_idamax.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_idamax.c
/usr/bin/mpicc -o HPL_dgemv.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dgemv.c
/usr/bin/mpicc -o HPL_dtrsv.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dtrsv.c
/usr/bin/mpicc -o HPL_dger.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dger.c
/usr/bin/mpicc -o HPL_dgemm.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dgemm.c
/usr/bin/mpicc -o HPL_dtrsm.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dtrsm.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_dcopy.o            HPL_daxpy.o            HPL_dscal.o HPL_idamax.o           HPL_dgemv.o            HPL_dtrsv.o HPL_dger.o             HPL_dgemm.o            HPL_dtrsm.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/blas/linux »
( cd src/comm/linux;          make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/comm/linux »
/usr/bin/mpicc -o HPL_1ring.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_1ring.c
/usr/bin/mpicc -o HPL_1rinM.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_1rinM.c
/usr/bin/mpicc -o HPL_2ring.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_2ring.c
/usr/bin/mpicc -o HPL_2rinM.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_2rinM.c
/usr/bin/mpicc -o HPL_blong.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_blong.c
/usr/bin/mpicc -o HPL_blonM.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_blonM.c
/usr/bin/mpicc -o HPL_packL.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_packL.c
/usr/bin/mpicc -o HPL_copyL.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_copyL.c
/usr/bin/mpicc -o HPL_binit.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_binit.c
/usr/bin/mpicc -o HPL_bcast.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_bcast.c
/usr/bin/mpicc -o HPL_bwait.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_bwait.c
/usr/bin/mpicc -o HPL_send.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_send.c
/usr/bin/mpicc -o HPL_recv.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_recv.c
/usr/bin/mpicc -o HPL_sdrv.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_sdrv.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_1ring.o            HPL_1rinM.o            HPL_2ring.o HPL_2rinM.o            HPL_blong.o            HPL_blonM.o HPL_packL.o            HPL_copyL.o            HPL_binit.o HPL_bcast.o            HPL_bwait.o            HPL_send.o HPL_recv.o             HPL_sdrv.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/comm/linux »
( cd src/grid/linux;          make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/grid/linux »
/usr/bin/mpicc -o HPL_grid_init.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_grid_init.c
/usr/bin/mpicc -o HPL_pnum.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pnum.c
/usr/bin/mpicc -o HPL_grid_info.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_grid_info.c
/usr/bin/mpicc -o HPL_grid_exit.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_grid_exit.c
/usr/bin/mpicc -o HPL_broadcast.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_broadcast.c
/usr/bin/mpicc -o HPL_reduce.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_reduce.c
/usr/bin/mpicc -o HPL_all_reduce.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_all_reduce.c
/usr/bin/mpicc -o HPL_barrier.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_barrier.c
/usr/bin/mpicc -o HPL_min.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_min.c
/usr/bin/mpicc -o HPL_max.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_max.c
/usr/bin/mpicc -o HPL_sum.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_sum.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_grid_init.o        HPL_pnum.o             HPL_grid_info.o HPL_grid_exit.o        HPL_broadcast.o        HPL_reduce.o HPL_all_reduce.o       HPL_barrier.o          HPL_min.o HPL_max.o              HPL_sum.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/grid/linux »
( cd src/panel/linux;         make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/panel/linux »
/usr/bin/mpicc -o HPL_pdpanel_new.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdpanel_new.c
/usr/bin/mpicc -o HPL_pdpanel_init.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdpanel_init.c
/usr/bin/mpicc -o HPL_pdpanel_disp.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdpanel_disp.c
/usr/bin/mpicc -o HPL_pdpanel_free.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdpanel_free.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_pdpanel_new.o      HPL_pdpanel_init.o     HPL_pdpanel_disp.o HPL_pdpanel_free.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/panel/linux »
( cd src/pauxil/linux;        make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/pauxil/linux »
/usr/bin/mpicc -o HPL_indxg2l.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_indxg2l.c
/usr/bin/mpicc -o HPL_indxg2lp.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_indxg2lp.c
/usr/bin/mpicc -o HPL_indxg2p.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_indxg2p.c
/usr/bin/mpicc -o HPL_indxl2g.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_indxl2g.c
/usr/bin/mpicc -o HPL_infog2l.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_infog2l.c
/usr/bin/mpicc -o HPL_numroc.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_numroc.c
/usr/bin/mpicc -o HPL_numrocI.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_numrocI.c
/usr/bin/mpicc -o HPL_dlaswp00N.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp00N.c
/usr/bin/mpicc -o HPL_dlaswp10N.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp10N.c
/usr/bin/mpicc -o HPL_dlaswp01N.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp01N.c
/usr/bin/mpicc -o HPL_dlaswp01T.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp01T.c
/usr/bin/mpicc -o HPL_dlaswp02N.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp02N.c
/usr/bin/mpicc -o HPL_dlaswp03N.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp03N.c
/usr/bin/mpicc -o HPL_dlaswp03T.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp03T.c
/usr/bin/mpicc -o HPL_dlaswp04N.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp04N.c
/usr/bin/mpicc -o HPL_dlaswp04T.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp04T.c
/usr/bin/mpicc -o HPL_dlaswp05N.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp05N.c
/usr/bin/mpicc -o HPL_dlaswp05T.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp05T.c
/usr/bin/mpicc -o HPL_dlaswp06N.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp06N.c
/usr/bin/mpicc -o HPL_dlaswp06T.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlaswp06T.c
/usr/bin/mpicc -o HPL_pwarn.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pwarn.c
/usr/bin/mpicc -o HPL_pabort.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pabort.c
/usr/bin/mpicc -o HPL_pdlaprnt.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdlaprnt.c
/usr/bin/mpicc -o HPL_pdlamch.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdlamch.c
/usr/bin/mpicc -o HPL_pdlange.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdlange.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_indxg2l.o          HPL_indxg2lp.o         HPL_indxg2p.o HPL_indxl2g.o          HPL_infog2l.o          HPL_numroc.o HPL_numrocI.o          HPL_dlaswp00N.o        HPL_dlaswp10N.o HPL_dlaswp01N.o        HPL_dlaswp01T.o        HPL_dlaswp02N.o HPL_dlaswp03N.o        HPL_dlaswp03T.o        HPL_dlaswp04N.o HPL_dlaswp04T.o        HPL_dlaswp05N.o        HPL_dlaswp05T.o HPL_dlaswp06N.o        HPL_dlaswp06T.o        HPL_pwarn.o HPL_pabort.o           HPL_pdlaprnt.o         HPL_pdlamch.o HPL_pdlange.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/pauxil/linux »
( cd src/pfact/linux;         make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/pfact/linux »
/usr/bin/mpicc -o HPL_dlocmax.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlocmax.c
/usr/bin/mpicc -o HPL_dlocswpN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlocswpN.c
/usr/bin/mpicc -o HPL_dlocswpT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dlocswpT.c
/usr/bin/mpicc -o HPL_pdmxswp.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdmxswp.c
/usr/bin/mpicc -o HPL_pdpancrN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdpancrN.c
/usr/bin/mpicc -o HPL_pdpancrT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdpancrT.c
/usr/bin/mpicc -o HPL_pdpanllN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdpanllN.c
/usr/bin/mpicc -o HPL_pdpanllT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdpanllT.c
/usr/bin/mpicc -o HPL_pdpanrlN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdpanrlN.c
/usr/bin/mpicc -o HPL_pdpanrlT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdpanrlT.c
/usr/bin/mpicc -o HPL_pdrpanllN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdrpanllN.c
/usr/bin/mpicc -o HPL_pdrpanllT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdrpanllT.c
/usr/bin/mpicc -o HPL_pdrpancrN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdrpancrN.c
/usr/bin/mpicc -o HPL_pdrpancrT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdrpancrT.c
/usr/bin/mpicc -o HPL_pdrpanrlN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdrpanrlN.c
/usr/bin/mpicc -o HPL_pdrpanrlT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdrpanrlT.c
/usr/bin/mpicc -o HPL_pdfact.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdfact.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_dlocmax.o          HPL_dlocswpN.o         HPL_dlocswpT.o HPL_pdmxswp.o          HPL_pdpancrN.o         HPL_pdpancrT.o HPL_pdpanllN.o         HPL_pdpanllT.o         HPL_pdpanrlN.o HPL_pdpanrlT.o         HPL_pdrpanllN.o        HPL_pdrpanllT.o HPL_pdrpancrN.o        HPL_pdrpancrT.o        HPL_pdrpanrlN.o HPL_pdrpanrlT.o        HPL_pdfact.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/pfact/linux »
( cd src/pgesv/linux;         make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/pgesv/linux »
/usr/bin/mpicc -o HPL_pipid.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pipid.c
/usr/bin/mpicc -o HPL_plindx0.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_plindx0.c
/usr/bin/mpicc -o HPL_pdlaswp00N.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdlaswp00N.c
/usr/bin/mpicc -o HPL_pdlaswp00T.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdlaswp00T.c
/usr/bin/mpicc -o HPL_perm.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_perm.c
/usr/bin/mpicc -o HPL_logsort.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_logsort.c
/usr/bin/mpicc -o HPL_plindx10.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_plindx10.c
/usr/bin/mpicc -o HPL_plindx1.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_plindx1.c
/usr/bin/mpicc -o HPL_spreadN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_spreadN.c
/usr/bin/mpicc -o HPL_spreadT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_spreadT.c
/usr/bin/mpicc -o HPL_rollN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_rollN.c
/usr/bin/mpicc -o HPL_rollT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_rollT.c
/usr/bin/mpicc -o HPL_equil.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_equil.c
/usr/bin/mpicc -o HPL_pdlaswp01N.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdlaswp01N.c
/usr/bin/mpicc -o HPL_pdlaswp01T.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdlaswp01T.c
/usr/bin/mpicc -o HPL_pdupdateNN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdupdateNN.c
/usr/bin/mpicc -o HPL_pdupdateNT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdupdateNT.c
/usr/bin/mpicc -o HPL_pdupdateTN.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdupdateTN.c
/usr/bin/mpicc -o HPL_pdupdateTT.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdupdateTT.c
/usr/bin/mpicc -o HPL_pdtrsv.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdtrsv.c
/usr/bin/mpicc -o HPL_pdgesv0.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdgesv0.c
/usr/bin/mpicc -o HPL_pdgesvK1.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdgesvK1.c
/usr/bin/mpicc -o HPL_pdgesvK2.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdgesvK2.c
/usr/bin/mpicc -o HPL_pdgesv.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdgesv.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_pipid.o            HPL_plindx0.o          HPL_pdlaswp00N.o HPL_pdlaswp00T.o       HPL_perm.o             HPL_logsort.o HPL_plindx10.o         HPL_plindx1.o          HPL_spreadN.o HPL_spreadT.o          HPL_rollN.o            HPL_rollT.o HPL_equil.o            HPL_pdlaswp01N.o       HPL_pdlaswp01T.o HPL_pdupdateNN.o       HPL_pdupdateNT.o       HPL_pdupdateTN.o HPL_pdupdateTT.o       HPL_pdtrsv.o           HPL_pdgesv0.o HPL_pdgesvK1.o         HPL_pdgesvK2.o         HPL_pdgesv.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/src/pgesv/linux »
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
make -f Make.top build_tst       arch=linux
make[1] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
( cd testing/matgen/linux;    make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/matgen/linux »
/usr/bin/mpicc -o HPL_dmatgen.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_dmatgen.c
/usr/bin/mpicc -o HPL_ladd.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_ladd.c
/usr/bin/mpicc -o HPL_lmul.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_lmul.c
/usr/bin/mpicc -o HPL_xjumpm.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_xjumpm.c
/usr/bin/mpicc -o HPL_jumpit.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_jumpit.c
/usr/bin/mpicc -o HPL_rand.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_rand.c
/usr/bin/mpicc -o HPL_setran.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_setran.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_dmatgen.o          HPL_ladd.o             HPL_lmul.o HPL_xjumpm.o           HPL_jumpit.o           HPL_rand.o HPL_setran.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/matgen/linux »
( cd testing/timer/linux;     make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/timer/linux »
/usr/bin/mpicc -o HPL_timer.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_timer.c
/usr/bin/mpicc -o HPL_timer_cputime.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_timer_cputime.c
/usr/bin/mpicc -o HPL_timer_walltime.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_timer_walltime.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_timer.o            HPL_timer_cputime.o    HPL_timer_walltime.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/timer/linux »
( cd testing/pmatgen/linux;   make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/pmatgen/linux »
/usr/bin/mpicc -o HPL_pdmatgen.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdmatgen.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_pdmatgen.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/pmatgen/linux »
( cd testing/ptimer/linux;    make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/ptimer/linux »
/usr/bin/mpicc -o HPL_ptimer.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_ptimer.c
/usr/bin/mpicc -o HPL_ptimer_cputime.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_ptimer_cputime.c
/usr/bin/mpicc -o HPL_ptimer_walltime.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_ptimer_walltime.c
ar r /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  HPL_ptimer.o           HPL_ptimer_cputime.o   HPL_ptimer_walltime.o
echo /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a 
/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a
touch lib.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/ptimer/linux »
( cd testing/ptest/linux;     make )
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/ptest/linux »
/usr/bin/mpicc -o HPL_pddriver.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pddriver.c
/usr/bin/mpicc -o HPL_pdinfo.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdinfo.c
../HPL_pdinfo.c: In function ‘HPL_pdinfo’:
../HPL_pdinfo.c:304:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:305:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( auth, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:309:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:311:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:327:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:337:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp ); lineptr = line;
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:351:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:361:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp ); lineptr = line;
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:375:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:379:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:389:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp ); lineptr = line;
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:400:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp ); lineptr = line;
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:426:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:431:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:440:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp ); lineptr = line;
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:453:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:462:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp ); lineptr = line;
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:476:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:485:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp ); lineptr = line;
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:499:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:508:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp ); lineptr = line;
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:521:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:530:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp ); lineptr = line;
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:546:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:555:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp ); lineptr = line;
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:570:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:579:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:585:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:591:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:597:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../HPL_pdinfo.c:603:7: warning: ignoring return value of ‘fgets’, declared with attribute warn_unused_result [-Wunused-result]
       (void) fgets( line, HPL_LINE_MAX - 2, infp );
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/usr/bin/mpicc -o HPL_pdtest.o -c -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops  ../HPL_pdtest.c
/usr/bin/mpicc -DAdd_ -DF77_INTEGER=int -DStringSunStyle -DHPL_CALL_CBLAS -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include -I/home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/include/linux  -I /usr/lib/mpich/include  -fomit-frame-pointer -O3 -funroll-loops -pthread -o /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/bin/linux/xhpl HPL_pddriver.o         HPL_pdinfo.o           HPL_pdtest.o /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/lib/linux/libhpl.a  /usr/lib/atlas-base/libf77blas.a /usr/lib/atlas-base/libatlas.a -lblas -L /usr/lib/mpich/lib -L /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib -ldumpi -lmpl
make /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/bin/linux/HPL.dat
make[3] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/ptest/linux »
( cp ../HPL.dat /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/bin/linux )
make[3] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/ptest/linux »
touch dexe.grd
make[2] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl/testing/ptest/linux »
make[1] : on quitte le répertoire « /home/chevamax/Documents/Stage_LIG_2017/logiciels/hpl »
#+end_example

On peut ensuite lancer une execution d'HPL dans =~/hpl/bin/linux=. Il
faut modifier le fichier HPL.dat pour les paramètres de l'execution,
et ensuite lancer *xhpl* via *mpirun* pour avoir des traces.

****** Exemple de fichier HPL.dat
#+BEGIN_EXAMPLE
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any) 
6            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
18048         Ns
1            # of NBs
192           NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
2            Ps
2            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
1            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
#+END_EXAMPLE

puis lancer avec  =mpirun -n 4 ./xhpl=

On voit ensuite les fichiers dumpi dans le répertoire.
On peut les convertirs en texte via cette commande
=/path/to/dumpi/bin/dumpi2ascii <nomfichier.bin>=

***** ROSS                                                         :ROSS:
#+begin_src sh :session foo :results output :exports both 
cd ~/Documents/Stage_LIG_2017/logiciels
git clone -b master --single-branch git@github.com:carothersc/ROSS.git
cd ROSS
mkdir build
cd build
ARCH=x86_64 CC=mpicc CXX=mpicxx cmake -DCMAKE_INSTALL_PREFIX=../install ../
make -j 3
make install
#+end_src

***** CODES                                                       :CODES:
#+begin_src sh :session foo
cd ~/Documents/Stage_LIG_2017/logiciels
wget ftp://ftp.mcs.anl.gov/pub/CODES/releases/codes-0.5.2.tar.gz
tar -xvf codes-0.5.2.tar.gz
mv codes-0.5.2 CODES
cd CODES
./prepare.sh
mkdir build
cd build
../configure --with-dumpi=/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install --prefix=/home/chevamax/Documents/Stage_LIG_2017/logiciels/CODES/install CC=mpicc PKG_CONFIG_PATH=../../ROSS/install/lib/pkgconfig
make && make install
make tests && make check
#+end_src

***** Tout est installé !


**** Réunion avec Florence
Il faut que je fasse une trace HPL sur 4 et 16 noeuds, puis que je la
rejoue sur CODES. Il faut que je trouve un moyen de visualiser les
traces. Ensuite je tenterais Grid5000 (cf [[https://www.grid5000.fr/mediawiki/index.php/Getting_Started][gettingStarted]]).

**** Traces

***** 4 noeuds
- Fichier HPL.dat
#+BEGIN_EXAMPLE
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any) 
6            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
20352         Ns
1            # of NBs
192           NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
2            Ps
2            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
1            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
#+END_EXAMPLE

#+begin_src sh :session foo :results output :exports both 
cd ~/Documents/Stage_LIG_2017/logiciels/hpl/bin/linux
mpirun -n 4 ./xhpl
#+end_src

#+RESULTS:
#+BEGIN_EXAMPLE
================================================================================
HPLinpack 2.2  --  High-Performance Linpack benchmark  --   February 24, 2016
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :   20352 
NB     :     192 
PMAP   : Row-major process mapping
P      :       2 
Q      :       2 
PFACT  :   Right 
NBMIN  :       4 
NDIV   :       2 
RFACT  :   Crout 
BCAST  :  1ringM 
DEPTH  :       1 
SWAP   : Mix (threshold = 64)
L1     : transposed form
U      : transposed form
EQUIL  : yes
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0

================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       20352   192     2     2             422.90              1.329e+01
HPL_pdgesv() start time Mon May 22 11:02:30 2017

HPL_pdgesv() end time   Mon May 22 11:09:33 2017

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=        0.0012039 ...... PASSED
================================================================================

Finished      1 tests with the following results:
              1 tests completed and passed residual checks,
              0 tests completed and failed residual checks,
              0 tests skipped because of illegal input values.
--------------------------------------------------------------------------------

End of Tests.
================================================================================
#+END_EXAMPLE

***** 16 noeuds
- Fichier HPL.dat
  
***** Visualisation
Il existe CommGram pour visualiser :
http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1101&context=computerscidiss
Mais je trouve pas l'outil sur le web.
Beaucoup d'outils utilisent pourtant DUMPI, il doit bien y avoir
quelque chose pour les analyser autre que l'outil
fourni... http://portal.nersc.gov/project/CAL/designforward.htm

https://xgitlab.cels.anl.gov/gonsie/codes/raw/592f7e82f9c2ad611471d43d59740990e73c4725/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-1728.conf

La doc CODES sur les traces n'est pas à jour, elle date d'avant le merge
entre codes-base et codes-net, ce qui n'aide pas...

SST possède des outils pour la visualisation mais rien à voir avec
DUMPI j'ai l'impression :
http://sst.sandia.gov/files/sstmacro_manual.pdf
Partie 3.9 et suite.

Ici quelques exemples de SST :
http://sst.sandia.gov/doxygen/sstmacro/dumpitracer.html
https://calccrypto.github.io/sst-macro/page_DumpiTutorial.html

pour le convertisseur otf de dumpi, il faut la libotf (et reinstaller
dumpi). On peut aussi générer des tests pour DUMPI (je sais pas si
c'est utile).

De base avec DUMPI on a accès à ca : 
#+begin_example
dumpistats --help
dumpistats:  Extract statistics from DUMPI trace files
Options:
   (-h|--help)                Print help screen and exit
   (-v|--verbose)             Verbose status output
   (-b|--bin)      timerange  Define a collection time range
   (-m|--mark)     /crt/end/  Create/end a bin at annotations
   (-g|--gather)   /crt/end/  Accumulate to an annotated bin
   (-c|--count)    funcname   Count entries into a function
   (-t|--time)     funcname   Accumulate time in a function
   (-s|--sent)     funcname   Count bytes sent by a function
   (-r|--recvd)    funcname   Count bytes recvd by function
   (-x|--exchange) funcname   Full send/recv exchange info
   (-l|--lump)     funcname   Lump (bin) messages by size
   (-p|--perfctr)  funcname   PAPI perfcounter info
   (-i|--in)       metafile   DUMPI metafile (required)
   (-o|--out)      fileroot   Output file root (required)

The timerange has the form:
  (all | mpi | BOUND to BOUND) [by TIME]
    where BOUND the form:
      (IDENTITY | TIME) [(+|-) TIME]
    and IDENTITY is:
      start | end | init | finalize | TIME
    and TIME is:
      (hh:)?(mm:)?[0-9]0-9+(.[0-9]*)?

The funcname has the form:
  all | mpi | non-mpi | sends | recvs | coll | wait | io | RE
    where RE is a case-insensitive regular expression
    (e.g. "MPI_Wait.*" or "mpio?_(wait.*|test.*))"
  All regular expressions are implicitly flanked by ^ and $,
  so "mpi_.?send" does not match MPI_Sendrecv

The annotated bins (-m and -g) use regular expressions
with a format similar to sed basic regular expressions, so:    -m '/begin bin/end bin/'
    -m '!begin bin!end bin!i'
    -g '| +start loop [0-9]+ *$|end loop [0-9]+$|'
are all valid annotations (note that backreferences are 
not allowed).  Escaped characters other than the delimiter
are passed directly to the regular expression parser.

Example 1:
  dumpistats --bin=all --time=mpi --time=other \
        -i dumpi.meta -o stats
     Writes a file called stats-0.dat with three columns:
       1:  The rank of each MPI node
       2:  Aggregate time spent in MPI calls
       3:  Aggregate time spent outside MPI calls

Example 2
  dumpistats --bin='init to finalize by 1:00' \ 
        --count=mpi_.*send -i dumpi.meta -o stats
     Writes files with send counts binned into 1 minute
     intervals.

Example 3
  dumpistats  --bin='begin+10 to end-10' -s all -r all \
         -i dumpi.meta -o stats \
      Writes a new file containing bytes sent and received
      starting 10 seconds after first and ending 10 seconds
      before last simulation timestamp
chevamax@Black-Pearl-2:~/Documents/Stag
#+end_example
*** 2017-05-23 mardi
**** Exemple de trace
***** ASCII
Il y a trois parties distinctes dans les fichier ASCII, qui se
répètent.
#+BEGIN_EXAMPLE
int source=1
int tag=1001
MPI_Comm comm=6 (user-defined-comm)
MPI_Request request=[2]
MPI_Irecv returning at walltime 59684.193756739, cputime 5.265173058 seconds in thread 0.
MPI_Send entering at walltime 59684.193758170, cputime 5.265174471 seconds in thread 0.
int count=388
MPI_Datatype datatype=14 (MPI_DOUBLE)
int dest=1
int tag=1001
MPI_Comm comm=6 (user-defined-comm)
MPI_Send returning at walltime 59684.193759116, cputime 5.265175437 seconds in thread 0.
MPI_Wait entering at walltime 59684.193760267, cputime 5.265176611 seconds in thread 0.
MPI_Request request=[2]
MPI_Status status=[{bytes=1568, cancelled=0, source=1, tag=1001, error=1}]
MPI_Wait returning at walltime 59684.193761705, cputime 5.265178044 seconds in thread 0.
MPI_Irecv entering at walltime 59684.193828567, cputime 5.265244802 seconds in thread 0.
int count=196
MPI_Datatype datatype=14 (MPI_DOUBLE)
int source=1
int tag=1001
MPI_Comm comm=6 (user-defined-comm)
MPI_Request request=[2]
MPI_Irecv returning at walltime 59684.193829258, cputime 5.265245560 seconds in thread 0.
MPI_Send entering at walltime 59684.193830658, cputime 5.265246970 seconds in thread 0.
int count=388
MPI_Datatype datatype=14 (MPI_DOUBLE)
int dest=1
int tag=1001
MPI_Comm comm=6 (user-defined-comm)
MPI_Send returning at walltime 59684.193831550, cputime 5.265247866 seconds in thread 0.
MPI_Wait entering at walltime 59684.193832698, cputime 5.265249003 seconds in thread 0.
MPI_Request request=[2]
MPI_Status status=[{bytes=1568, cancelled=0, source=1, tag=1001, error=1}]
MPI_Wait returning at walltime 59684.193833889, cputime 5.265250193 seconds in thread 0.
MPI_Irecv entering at walltime 59684.194138884, cputime 5.265555006 seconds in thread 0.
#+END_EXAMPLE

#+BEGIN_EXAMPLE
int address=-1014243776
MPI_Address returning at walltime 59684.354258600, cputime 5.425630894 seconds in thread 0.
MPI_Address entering at walltime 59684.354259540, cputime 5.425631843 seconds in thread 0.
int address=-1014162368
MPI_Address returning at walltime 59684.354259971, cputime 5.425632266 seconds in thread 0.
MPI_Address entering at walltime 59684.354260905, cputime 5.425633215 seconds in thread 0.
int address=-1014080960
MPI_Address returning at walltime 59684.354261328, cputime 5.425633619 seconds in thread 0.
MPI_Address entering at walltime 59684.354262258, cputime 5.425634561 seconds in thread 0.
int address=-1013999552
MPI_Address returning at walltime 59684.354262686, cputime 5.425634975 seconds in thread 0.
MPI_Address entering at walltime 59684.354263635, cputime 5.425635936 seconds in thread 0.
int address=-1013918144
MPI_Address returning at walltime 59684.354264054, cputime 5.425636343 seconds in thread 0.
MPI_Address entering at walltime 59684.354264893, cputime 5.425637232 seconds in thread 0.
int address=-1013836736
MPI_Address returning at walltime 59684.354265287, cputime 5.425637586 seconds in thread 0.
MPI_Address entering at walltime 59684.354266224, cputime 5.425638522 seconds in thread 0.
int address=-1013755328
MPI_Address returning at walltime 59684.354266644, cputime 5.425638946 seconds in thread 0.
MPI_Address entering at walltime 59684.354267566, cputime 5.425639865 seconds in thread 0.
int address=-1013673920
MPI_Address returning at walltime 59684.354267982, cputime 5.425640277 seconds in thread 0.
MPI_Address entering at walltime 59684.354268905, cputime 5.425641211 seconds in thread 0.
int address=-1013592512
MPI_Address returning at walltime 59684.354269316, cputime 5.425641609 seconds in thread 0.
MPI_Address entering at walltime 59684.354270244, cputime 5.425642549 seconds in thread 0.
int address=-1013511104
MPI_Address returning at walltime 59684.354270661, cputime 5.425642955 seconds in thread 0.
MPI_Address entering at walltime 59684.354271595, cputime 5.425643892 seconds in thread 0.
int address=-1013429696
MPI_Address returning at walltime 59684.354272026, cputime 5.425644307 seconds in thread 0.
MPI_Address entering at walltime 59684.354272967, cputime 5.425645269 seconds in thread 0.
#+END_EXAMPLE

#+BEGIN_EXAMPLE
int count=193
int lengths[193]=[9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 9984, 37057]
int indices[193]=[0, 0, 81408, 0, 162816, 0, 244224, 0, 325632, 0, 407040, 0, 488448, 0, 569856, 0, 651264, 0, 732672, 0, 814080, 0, 895488, 0, 976896, 0, 1058304, 0, 1139712, 0, 1221120, 0, 1302528, 0, 1383936, 0, 1465344, 0, 1546752, 0, 1628160, 0, 1709568, 0, 1790976, 0, 1872384, 0, 1953792, 0, 2035200, 0, 2116608, 0, 2198016, 0, 2279424, 0, 2360832, 0, 2442240, 0, 2523648, 0, 2605056, 0, 2686464, 0, 2767872, 0, 2849280, 0, 2930688, 0, 3012096, 0, 3093504, 0, 3174912, 0, 3256320, 0, 3337728, 0, 3419136, 0, 3500544, 0, 3581952, 0, 3663360, 0, 3744768, 0, 3826176, 0, 3907584, 0, 3988992, 0, 4070400, 0, 4151808, 0, 4233216, 0, 4314624, 0, 4396032, 0, 4477440, 0, 4558848, 0, 4640256, 0, 4721664, 0, 4803072, 0, 4884480, 0, 4965888, 0, 5047296, 0, 5128704, 0, 5210112, 0, 5291520, 0, 5372928, 0, 5454336, 0, 5535744, 0, 5617152, 0, 5698560, 0, 5779968, 0, 5861376, 0, 5942784, 0, 6024192, 0, 6105600, 0, 6187008, 0, 6268416, 0, 6349824, 0, 6431232, 0, 6512640, 0, 6594048, 0, 6675456, 0, 6756864, 0, 6838272, 0, 6919680, 0, 7001088, 0, 7082496, 0, 7163904, 0, 7245312, 0, 7326720, 0, 7408128, 0, 7489536, 0, 7570944, 0, 7652352, 0, 7733760, 0, 7815168]
MPI_Datatype oldtypes[193]=[14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]
MPI_Datatype newtype=28 (user-defined-datatype)
MPI_Type_struct returning at walltime 59684.354384131, cputime 5.425756419 seconds in thread 0.
MPI_Type_commit entering at walltime 59684.354408868, cputime 5.425781148 seconds in thread 0.
MPI_Datatype datatype=28 (user-defined-datatype)
MPI_Type_commit returning at walltime 59684.354423395, cputime 5.425795676 seconds in thread 0.
MPI_Send entering at walltime 59684.354425047, cputime 5.425797351 seconds in thread 0.
#+END_EXAMPLE

***** Binaire
On dispose du format binaire si on veut faire autre chose (doc)
#+BEGIN_EXAMPLE
/**
\page traceformat The DUMPI trace file format

The results of a DUMPI profiling run consists of 
- One ASCII metafile (briefly described in section METAFILE) for the
  entire run, and
- One binary trace file for each node (described in section TRACEFILE
  and subsections T0 through T8).


METAFILE

The metafile is a simple key/value ASCII file that is intended to be
human-readable and to facilitate grouping related trace files
together.


TRACEFILE

Each trace file consists of a 64-bit lead-in magic number and 8 data
records.  Only the lead-in magic and the index record (record T.8) are
positional.  The former occupies the first 8 bytes of the file and the
latter it occupies the last few bytes of the file (the exact number of bytes
is, unfortunately, dependent on the actual DUMPI version -- 
see section T8 for details).

T.0:  Lead-in magic:  The trace file starts with the 8-byte sequence
      {0xff, 0xaa, 0xdd, 'D', 'U', 'M', 'P', 'I'}

T.1:  The actual profiled calls
      - The stream starts with two 32-bit values representing the time
        bias (in seconds) for CPU and wall time, respectively.	  
      - Immediately following the time biases, is the time-ordered
        stream of profiled calls.
        Each profiled call has the following structure:
          1) A 16-bit index identifying the profiled call (defined in
             common/funclabels.h).  The value DUMPI_END_OF_STREAM
	     terminates the stream.
	  2) An 8-bit mask defining what optional fields are output on
             the stream (masks defined in common/settings.h).
	     Currently, the following masks are used:
	       - DUMPI_ENABLE_STATUS_MASK (for historical reasons, this
      	         mask occupies the two lowest order bits).  Specifies
                 that MPI_Status information is stored for this call.
	       - DUMPI_CPUTIME_MASK (value (1<<2)).  Specifies that
                 cpu time information (high resolution timers) is
                 stored for both function entry and exit.  See section
                 TIMEINFO for more information.
	       - DUMPI_WALLTIME_MASK (value (1<<3)).  Specifies that
	         wall time information is stored for function entry
                 and exit.  See subsection TIMEINFO.
	       - DUMPI_THREADID_MASK (value (1<<6)).  Specifies that
                 a 16-bit integer thread identifier is stored for the
      		 call.  See subsection THREADINFO.
	       - DUMPI_PERFINFO_MASK (value (1<<7)).  Specifies that
  	         performance counter information is stored.  See section
    		 PERFINFO.
	  3) Timer information.  Note that all time information is
      	     biased by subtracting the number of seconds at the start
      	     of simulation from the stored values.  Time values are
  	     stored as uint16_t for seconds and uint32_t for
             nanoseconds.  Time biases are stored at the beginning of
	     the stream.
	     a) If DUMPI_CPUTIME_MASK was enabled, CPU time is here.
 	     b) if DUMPI_WALLTIME_MASK was enabled, wall time is here.
	  4) Performance counter information
	     If DUMPI_PERFINFO_MASK was enabled, PAPI performance
	     counters are stored for the call.  The format for the
	     perfcounter information is:
	       - An 8-bit value indicating the number of active
	         perfcounters (let's call this 'perfcount').
	       - 'perfcount' pairs of two 64-bit integer values
	       	 indicating the state of each perfcounter when the
	       	 function was entered and exited.
	  5) Call arguments as defined in common/argtypes.h.  Note that
	       - Opaque MPI types (MPI_Comm, MPI_Op, etc.) are
	         hashed to zero-based integer values.  Pre-defined
		 objects (MPI_COMM_WORLD, MPI_ANY_SOURCE, etc.) are
		 mapped onto the values of the corresponding DUMPI types
		 (DUMPI_COMM_WORLD, DUMPI_ANY_SOURCE) as defined in
		 common/constants.h.
	       - Array (vector) arguments are stored as a 32-bit integer
	         representing array length followed by all elements of
		 the array.  Empty arrays and NULL array pointers are
		 both stored as the length argument 0 followed by no
		 values.
	       - Status arrays are stored as four separate arrays:
	         1) Array of int32_t for bytes sent/received
		 2) Array of int32_t for sources
		 3) Array of int8_t for 'cancelled' state flag
		 4) Array of int8_t for 'error' state flag
	       - Status arrays may be of length 0 even if
	         DUMPI_ENABLE_STATUS_MASK was active.  This is to
		 permit more compact profiles (in libdumpi, this is
		 true when the dumpi.conf file defines 'statuses' as
		 'success').

T.2:  A header record containing 
      - Version information (stored as three 8-bit values)
      - Time at the start of simulation (64-bits)
      - Hostname as reported by gethostname(2)
      - Username as reported by the LOGNAME environment variable
        or "<none>" if username is not defined.
      - Information about the network mesh dimensions (if available
        on the platform).  These include:
	   o Mesh dimension (set to zero if no mesh info is
	     available).
	   o Position of current node in mesh if available
	     (array of size mesh dimension).
	   o Size of mesh if available.  Initialized to zero
	     if mesh dimension is not zero but mesh size is not
	     available.

T.3:  A footer record containing:
      - A 64-bit magic number indicating the start of the record
        (0xf007fee7).
      - A list of 32-bit integers indicating of how many times each
        MPI call was encountered (DUMPI_ALL_FUNCTIONS entries --
        covers all MPI-2 calls).
      - A list of 32-bit integers indicating how often each call was
      	entered but not profiled (similar as above).

T.4:  A keyval record containing:
      - A 32-bit integer listing the number of key/value pairs stored.
      - A list of string pairs for each key/value entry.

T.6:  A listing of perfcounter labels.  This section may be empty, in
which case the index record points to offset 0.  Otherwise, this
section contains:
      - A 32-bit integer indicating the number of perfcounter labels.
      - A list of string names for each perfcounter.

T.7:  A listing of function names encountered in call tracing.  This
section may be empty, in which case the index record points to offset
0.  Otherwise, this section contains:
	- A 32-bit integer indicating the number of function
	  address/name pairs stored.
	- A list of paired 64-bit integers / string names for each
	function encountered.  If a name is not available for one or
	more function names, the name gets stored as "<none>".

T.8: An index record that lists the offsets into records T.1-7.  This
record is always the last record in the file.  This index record is a
not ideal (should probably be replaced with a set of named
sections), but currently consists of:
      - (last 64 bits): offset of keyval section
      - (preceding that): offset of footer section
      - (preceding that): offset of body (stream) section
      - (preceding that): offset of header section
      - (preceding that): offset of percounter labels OR header magic
        ({0xff, 0xaa, 0xdd, 'D', 'U', 'M', 'P', 'I'}) if this trace
	file precedes the addition of perfcounter labels.
      - (preceding that if perfcounter labels were included): offset
        of function names OR header magic if this trace file precedes
        the addition of perfcounter labels.
      - (preceding that if function names were included):  header
      	magic (eventually new sections might be prepended here).

*/
#+END_EXAMPLE

***** TODO OTF2

**** Tentative replay exemple sur CODES                            :CODES:

CODES a l'air de pouvoir utiliser des trace *ScalaTrace* pour les
rejouer (cf =./model-net-dumpi-traces-dump --help= ). Je sais pas si
c'est mieux que DUMPI.

Via le fichier =README_traces.txt=, dans le dossier
CODES/src/network-worloads, ce [[http://portal.nersc.gov/project/CAL/designforward.ht][lien]] ainsi que ce [[https://xgitlab.cels.anl.gov/codes/codes/blob/c4cba873cf9d5621dd1115a01e1982c8d0e8b31e/src/models/mpi-trace-replay/conf/modelnet-mpi-test-dragonfly.conf][fichier]] j'ai réussi à
faire tourner l'exemple ci dessous (trace dumpi sur CODES).

#+BEGIN_EXAMPLE
./model-net-mpi-replay --sync=1 --num_net_traces=216 --workload_file=/home/chevamax/Documents/Stage_LIG_2017/logiciels/test/df_AMG_n216_dumpi/dumpi-2014.03.03.14.55.23- --workload_type="dumpi" --lp-io-dir=amg-216-trace --lp-io-use-suffix=1 -- ../../src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf
#+END_EXAMPLE

La commande ci-dessus me donne la sortie suivante :

#+BEGIN_EXAMPLE
Tue May 23 11:12:28 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 478 receives 478 collectives 0 delays 1218 wait alls 56 waits 0 send time 612826.428503 wait 14669328.061635
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 832 receives 832 collectives 0 delays 1930 wait alls 60 waits 0 send time 1018385.000731 wait 64370657.327652
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 566 receives 566 collectives 0 delays 1398 wait alls 60 waits 0 send time 750754.503860 wait 72563845.789648
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 840 receives 840 collectives 0 delays 1946 wait alls 60 waits 0 send time 1047815.519626 wait 157051216.933560
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 464 receives 464 collectives 0 delays 1184 wait alls 50 waits 0 send time 647397.328881 wait 66705324.503492
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 386 receives 386 collectives 0 delays 1034 wait alls 56 waits 0 send time 503242.249048 wait 66246441.900921
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 654 receives 654 collectives 0 delays 1574 wait alls 60 waits 0 send time 846491.070120 wait 117358929.038023
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 790 receives 790 collectives 0 delays 1846 wait alls 60 waits 0 send time 1041571.325534 wait 70288613.070390
 LP 18 unmatched irecvs 0 unmatched sends 0 Total sends 1194 receives 1194 collectives 0 delays 2654 wait alls 60 waits 0 send time 1470155.471660 wait 68216556.238328
 LP 19 unmatched irecvs 0 unmatched sends 0 Total sends 1010 receives 1010 collectives 0 delays 2286 wait alls 60 waits 0 send time 1264102.027885 wait 66450400.852029
 LP 20 unmatched irecvs 0 unmatched sends 0 Total sends 956 receives 956 collectives 0 delays 2178 wait alls 60 waits 0 send time 1205111.164923 wait 59633424.426049
 LP 21 unmatched irecvs 0 unmatched sends 0 Total sends 528 receives 528 collectives 0 delays 1318 wait alls 56 waits 0 send time 723458.275000 wait 117875229.796452
 LP 27 unmatched irecvs 0 unmatched sends 0 Total sends 1026 receives 1026 collectives 0 delays 2324 wait alls 66 waits 0 send time 1235379.351462 wait 74247344.022784
 LP 28 unmatched irecvs 0 unmatched sends 0 Total sends 1004 receives 1004 collectives 0 delays 2274 wait alls 60 waits 0 send time 1255522.120947 wait 118504878.250013
 LP 29 unmatched irecvs 0 unmatched sends 0 Total sends 1066 receives 1066 collectives 0 delays 2398 wait alls 60 waits 0 send time 1336751.450998 wait 63630925.617630
 LP 30 unmatched irecvs 0 unmatched sends 0 Total sends 1042 receives 1042 collectives 0 delays 2356 wait alls 66 waits 0 send time 1303271.469921 wait 116979946.393221
 LP 36 unmatched irecvs 0 unmatched sends 0 Total sends 978 receives 978 collectives 0 delays 2222 wait alls 60 waits 0 send time 1264630.386888 wait 60750187.906107
 LP 37 unmatched irecvs 0 unmatched sends 0 Total sends 628 receives 628 collectives 0 delays 1522 wait alls 60 waits 0 send time 816591.158841 wait 159049402.123190
 LP 38 unmatched irecvs 0 unmatched sends 0 Total sends 756 receives 756 collectives 0 delays 1778 wait alls 60 waits 0 send time 962886.542565 wait 62828381.096530
 LP 39 unmatched irecvs 0 unmatched sends 0 Total sends 1224 receives 1224 collectives 0 delays 2714 wait alls 60 waits 0 send time 1516243.201509 wait 117957528.505235
 LP 45 unmatched irecvs 0 unmatched sends 0 Total sends 1058 receives 1058 collectives 0 delays 2382 wait alls 60 waits 0 send time 1320910.147259 wait 60146490.080305
 LP 46 unmatched irecvs 0 unmatched sends 0 Total sends 708 receives 708 collectives 0 delays 1672 wait alls 50 waits 0 send time 967492.464439 wait 158523458.934212
 LP 47 unmatched irecvs 0 unmatched sends 0 Total sends 1132 receives 1132 collectives 0 delays 2530 wait alls 60 waits 0 send time 1397302.027431 wait 74069915.010258
 LP 48 unmatched irecvs 0 unmatched sends 0 Total sends 740 receives 740 collectives 0 delays 1746 wait alls 60 waits 0 send time 928590.332869 wait 65232701.431432
 LP 54 unmatched irecvs 0 unmatched sends 0 Total sends 664 receives 664 collectives 0 delays 1594 wait alls 60 waits 0 send time 848973.949047 wait 36345958.118133
 LP 55 unmatched irecvs 0 unmatched sends 0 Total sends 674 receives 674 collectives 0 delays 1604 wait alls 50 waits 0 send time 938229.478640 wait 125516971.862382
 LP 56 unmatched irecvs 0 unmatched sends 0 Total sends 1028 receives 1028 collectives 0 delays 2328 wait alls 66 waits 0 send time 1267561.482412 wait 117911981.356183
 LP 57 unmatched irecvs 0 unmatched sends 0 Total sends 1236 receives 1236 collectives 0 delays 2744 wait alls 66 waits 0 send time 1504637.069302 wait 50347396.693650
 LP 63 unmatched irecvs 0 unmatched sends 0 Total sends 878 receives 878 collectives 0 delays 2022 wait alls 60 waits 0 send time 1146823.708011 wait 123363217.866624
 LP 64 unmatched irecvs 0 unmatched sends 0 Total sends 690 receives 690 collectives 0 delays 1646 wait alls 60 waits 0 send time 879885.796478 wait 227372017.567977
 LP 65 unmatched irecvs 0 unmatched sends 0 Total sends 646 receives 646 collectives 0 delays 1558 wait alls 60 waits 0 send time 774507.757742 wait 71956122.051773
 LP 66 unmatched irecvs 0 unmatched sends 0 Total sends 768 receives 768 collectives 0 delays 1798 wait alls 56 waits 0 send time 950385.223462 wait 158583089.307583
 LP 72 unmatched irecvs 0 unmatched sends 0 Total sends 670 receives 670 collectives 0 delays 1606 wait alls 60 waits 0 send time 849275.876803 wait 121573968.425720
 LP 73 unmatched irecvs 0 unmatched sends 0 Total sends 794 receives 794 collectives 0 delays 1854 wait alls 60 waits 0 send time 967002.342169 wait 226746422.904084
 LP 74 unmatched irecvs 0 unmatched sends 0 Total sends 676 receives 676 collectives 0 delays 1618 wait alls 60 waits 0 send time 853497.663715 wait 73897714.895026
 LP 75 unmatched irecvs 0 unmatched sends 0 Total sends 516 receives 516 collectives 0 delays 1294 wait alls 56 waits 0 send time 641329.482212 wait 184735895.207342
 LP 81 unmatched irecvs 0 unmatched sends 0 Total sends 650 receives 650 collectives 0 delays 1562 wait alls 56 waits 0 send time 858544.351669 wait 223511975.760137
 LP 82 unmatched irecvs 0 unmatched sends 0 Total sends 1008 receives 1008 collectives 0 delays 2282 wait alls 60 waits 0 send time 1287474.151241 wait 74911528.900648
 LP 83 unmatched irecvs 0 unmatched sends 0 Total sends 886 receives 886 collectives 0 delays 2038 wait alls 60 waits 0 send time 1173980.655245 wait 186291322.476267
 LP 84 unmatched irecvs 0 unmatched sends 0 Total sends 726 receives 726 collectives 0 delays 1708 wait alls 50 waits 0 send time 1006582.941256 wait 180308323.805768
 LP 90 unmatched irecvs 0 unmatched sends 0 Total sends 1080 receives 1080 collectives 0 delays 2432 wait alls 66 waits 0 send time 1358841.625623 wait 114358120.575814
 LP 91 unmatched irecvs 0 unmatched sends 0 Total sends 804 receives 804 collectives 0 delays 1874 wait alls 60 waits 0 send time 1009827.313515 wait 225479612.621467
 LP 92 unmatched irecvs 0 unmatched sends 0 Total sends 996 receives 996 collectives 0 delays 2258 wait alls 60 waits 0 send time 1292993.186195 wait 159080981.001880
 LP 93 unmatched irecvs 0 unmatched sends 0 Total sends 1044 receives 1044 collectives 0 delays 2344 wait alls 50 waits 0 send time 1409568.727912 wait 123803484.427334
 LP 99 unmatched irecvs 0 unmatched sends 0 Total sends 1010 receives 1010 collectives 0 delays 2276 wait alls 50 waits 0 send time 1392107.978241 wait 72580169.316250
 LP 100 unmatched irecvs 0 unmatched sends 0 Total sends 1952 receives 1952 collectives 0 delays 4182 wait alls 72 waits 0 send time 2346398.054515 wait 223514663.875979
 LP 101 unmatched irecvs 0 unmatched sends 0 Total sends 998 receives 998 collectives 0 delays 2252 wait alls 50 waits 0 send time 1362987.743583 wait 178904814.966390
 LP 102 unmatched irecvs 0 unmatched sends 0 Total sends 1016 receives 1016 collectives 0 delays 2298 wait alls 60 waits 0 send time 1296163.307307 wait 159821204.649910
 LP 108 unmatched irecvs 0 unmatched sends 0 Total sends 858 receives 858 collectives 0 delays 1982 wait alls 60 waits 0 send time 1098585.436320 wait 61204099.545769
 LP 109 unmatched irecvs 0 unmatched sends 0 Total sends 1852 receives 1852 collectives 0 delays 3980 wait alls 70 waits 0 send time 2231130.888345 wait 239282149.488297
 LP 110 unmatched irecvs 0 unmatched sends 0 Total sends 1036 receives 1036 collectives 0 delays 2328 wait alls 50 waits 0 send time 1417221.694278 wait 231406965.247936
 LP 111 unmatched irecvs 0 unmatched sends 0 Total sends 1096 receives 1096 collectives 0 delays 2448 wait alls 50 waits 0 send time 1469905.820924 wait 242108030.842040
 LP 117 unmatched irecvs 0 unmatched sends 0 Total sends 1508 receives 1508 collectives 0 delays 3282 wait alls 60 waits 0 send time 1893400.232530 wait 227244561.724871
 LP 118 unmatched irecvs 0 unmatched sends 0 Total sends 704 receives 704 collectives 0 delays 1664 wait alls 50 waits 0 send time 987026.752725 wait 182374233.768544
 LP 119 unmatched irecvs 0 unmatched sends 0 Total sends 708 receives 708 collectives 0 delays 1672 wait alls 50 waits 0 send time 980978.812301 wait 13967711.361794
 LP 120 unmatched irecvs 0 unmatched sends 0 Total sends 1344 receives 1344 collectives 0 delays 2954 wait alls 60 waits 0 send time 1720179.012817 wait 35015531.040038
 LP 126 unmatched irecvs 0 unmatched sends 0 Total sends 1614 receives 1614 collectives 0 delays 3494 wait alls 60 waits 0 send time 2023971.208713 wait 182946647.373766
 LP 127 unmatched irecvs 0 unmatched sends 0 Total sends 1642 receives 1642 collectives 0 delays 3550 wait alls 60 waits 0 send time 2015590.811251 wait 184289237.361690
 LP 128 unmatched irecvs 0 unmatched sends 0 Total sends 1346 receives 1346 collectives 0 delays 2958 wait alls 60 waits 0 send time 1700357.502209 wait 243228804.840325
 LP 129 unmatched irecvs 0 unmatched sends 0 Total sends 1214 receives 1214 collectives 0 delays 2694 wait alls 60 waits 0 send time 1518689.918331 wait 179953424.617609
 LP 135 unmatched irecvs 0 unmatched sends 0 Total sends 926 receives 926 collectives 0 delays 2118 wait alls 60 waits 0 send time 1246349.675652 wait 237034450.909029
 LP 136 unmatched irecvs 0 unmatched sends 0 Total sends 954 receives 954 collectives 0 delays 2164 wait alls 50 waits 0 send time 1352503.447149 wait 235505287.971080
 LP 137 unmatched irecvs 0 unmatched sends 0 Total sends 1460 receives 1460 collectives 0 delays 3196 wait alls 70 waits 0 send time 1852326.853503 wait 236632839.250267
 LP 138 unmatched irecvs 0 unmatched sends 0 Total sends 1066 receives 1066 collectives 0 delays 2388 wait alls 50 waits 0 send time 1409283.182707 wait 235555608.684319
 LP 144 unmatched irecvs 0 unmatched sends 0 Total sends 1752 receives 1752 collectives 0 delays 3776 wait alls 66 waits 0 send time 2130807.570266 wait 225930646.267494
 LP 145 unmatched irecvs 0 unmatched sends 0 Total sends 1130 receives 1130 collectives 0 delays 2532 wait alls 66 waits 0 send time 1388729.942234 wait 242848402.532029
 LP 146 unmatched irecvs 0 unmatched sends 0 Total sends 718 receives 718 collectives 0 delays 1698 wait alls 56 waits 0 send time 930406.156660 wait 229788144.347419
 LP 147 unmatched irecvs 0 unmatched sends 0 Total sends 670 receives 670 collectives 0 delays 1596 wait alls 50 waits 0 send time 945853.904861 wait 234202698.028066
 LP 153 unmatched irecvs 0 unmatched sends 0 Total sends 1562 receives 1562 collectives 0 delays 3396 wait alls 66 waits 0 send time 1893068.247829 wait 44977957.049517
 LP 154 unmatched irecvs 0 unmatched sends 0 Total sends 780 receives 780 collectives 0 delays 1826 wait alls 60 waits 0 send time 1038846.624604 wait 228435021.601487
 LP 155 unmatched irecvs 0 unmatched sends 0 Total sends 884 receives 884 collectives 0 delays 2034 wait alls 60 waits 0 send time 1153261.695121 wait 242093904.982951
 LP 156 unmatched irecvs 0 unmatched sends 0 Total sends 704 receives 704 collectives 0 delays 1674 wait alls 60 waits 0 send time 883647.081994 wait 226714793.456252
 LP 162 unmatched irecvs 0 unmatched sends 0 Total sends 848 receives 848 collectives 0 delays 1968 wait alls 66 waits 0 send time 1056655.404335 wait 157175170.273683
 LP 163 unmatched irecvs 0 unmatched sends 0 Total sends 1212 receives 1212 collectives 0 delays 2696 wait alls 66 waits 0 send time 1473940.846421 wait 28336559.684429
 LP 164 unmatched irecvs 0 unmatched sends 0 Total sends 1194 receives 1194 collectives 0 delays 2660 wait alls 66 waits 0 send time 1440356.469666 wait 151749253.578270
 LP 165 unmatched irecvs 0 unmatched sends 0 Total sends 682 receives 682 collectives 0 delays 1620 wait alls 50 waits 0 send time 955091.139227 wait 161779088.065750
 LP 171 unmatched irecvs 0 unmatched sends 0 Total sends 944 receives 944 collectives 0 delays 2154 wait alls 60 waits 0 send time 1221491.539740 wait 43063209.056568
 LP 172 unmatched irecvs 0 unmatched sends 0 Total sends 602 receives 602 collectives 0 delays 1470 wait alls 60 waits 0 send time 784025.269904 wait 233804447.033664
 LP 173 unmatched irecvs 0 unmatched sends 0 Total sends 942 receives 942 collectives 0 delays 2150 wait alls 60 waits 0 send time 1229456.029328 wait 50172193.310056
 LP 174 unmatched irecvs 0 unmatched sends 0 Total sends 968 receives 968 collectives 0 delays 2192 wait alls 50 waits 0 send time 1319939.372680 wait 227463054.419840
 LP 180 unmatched irecvs 0 unmatched sends 0 Total sends 1024 receives 1024 collectives 0 delays 2304 wait alls 50 waits 0 send time 1400669.257240 wait 55674515.977710
 LP 181 unmatched irecvs 0 unmatched sends 0 Total sends 1454 receives 1454 collectives 0 delays 3174 wait alls 60 waits 0 send time 1816148.078225 wait 181507834.768630
 LP 182 unmatched irecvs 0 unmatched sends 0 Total sends 1544 receives 1544 collectives 0 delays 3354 wait alls 60 waits 0 send time 1911634.255754 wait 224604363.142339
 LP 183 unmatched irecvs 0 unmatched sends 0 Total sends 1078 receives 1078 collectives 0 delays 2422 wait alls 60 waits 0 send time 1309799.522605 wait 230384066.827760
 LP 189 unmatched irecvs 0 unmatched sends 0 Total sends 1054 receives 1054 collectives 0 delays 2374 wait alls 60 waits 0 send time 1362222.775894 wait 189425317.992194
 LP 190 unmatched irecvs 0 unmatched sends 0 Total sends 1702 receives 1702 collectives 0 delays 3670 wait alls 60 waits 0 send time 2070140.487704 wait 117181727.419880
 LP 191 unmatched irecvs 0 unmatched sends 0 Total sends 1096 receives 1096 collectives 0 delays 2448 wait alls 50 waits 0 send time 1452532.883894 wait 224114053.684710
 LP 192 unmatched irecvs 0 unmatched sends 0 Total sends 1548 receives 1548 collectives 0 delays 3362 wait alls 60 waits 0 send time 1914415.247092 wait 227210177.590556
 LP 198 unmatched irecvs 0 unmatched sends 0 Total sends 1014 receives 1014 collectives 0 delays 2284 wait alls 50 waits 0 send time 1393252.678999 wait 220299486.476730
 LP 199 unmatched irecvs 0 unmatched sends 0 Total sends 1390 receives 1390 collectives 0 delays 3056 wait alls 70 waits 0 send time 1669083.321960 wait 241003019.512698
 LP 200 unmatched irecvs 0 unmatched sends 0 Total sends 1628 receives 1628 collectives 0 delays 3534 wait alls 72 waits 0 send time 1969004.464915 wait 185460021.241456
 LP 201 unmatched irecvs 0 unmatched sends 0 Total sends 1666 receives 1666 collectives 0 delays 3598 wait alls 60 waits 0 send time 2080485.562014 wait 227932490.141174
 LP 207 unmatched irecvs 0 unmatched sends 0 Total sends 1722 receives 1722 collectives 0 delays 3710 wait alls 60 waits 0 send time 2139705.407085 wait 176512745.363789
 LP 208 unmatched irecvs 0 unmatched sends 0 Total sends 1458 receives 1458 collectives 0 delays 3182 wait alls 60 waits 0 send time 1843616.685628 wait 189486438.128715
 LP 209 unmatched irecvs 0 unmatched sends 0 Total sends 1538 receives 1538 collectives 0 delays 3342 wait alls 60 waits 0 send time 1918725.054495 wait 239318145.981069
 LP 210 unmatched irecvs 0 unmatched sends 0 Total sends 740 receives 740 collectives 0 delays 1736 wait alls 50 waits 0 send time 1016436.312454 wait 231850048.656625
 LP 216 unmatched irecvs 0 unmatched sends 0 Total sends 992 receives 992 collectives 0 delays 2250 wait alls 60 waits 0 send time 1267154.230789 wait 177495309.095000
 LP 217 unmatched irecvs 0 unmatched sends 0 Total sends 1456 receives 1456 collectives 0 delays 3178 wait alls 60 waits 0 send time 1821569.537089 wait 116792071.752879
 LP 218 unmatched irecvs 0 unmatched sends 0 Total sends 1050 receives 1050 collectives 0 delays 2356 wait alls 50 waits 0 send time 1436108.782684 wait 37291272.684969
 LP 219 unmatched irecvs 0 unmatched sends 0 Total sends 1584 receives 1584 collectives 0 delays 3434 wait alls 60 waits 0 send time 1973944.999668 wait 49904082.837897
 LP 225 unmatched irecvs 0 unmatched sends 0 Total sends 1012 receives 1012 collectives 0 delays 2280 wait alls 50 waits 0 send time 1388698.228261 wait 126694818.180600
 LP 226 unmatched irecvs 0 unmatched sends 0 Total sends 1028 receives 1028 collectives 0 delays 2322 wait alls 60 waits 0 send time 1320794.752760 wait 118546753.671267
 LP 227 unmatched irecvs 0 unmatched sends 0 Total sends 822 receives 822 collectives 0 delays 1910 wait alls 60 waits 0 send time 1043424.735314 wait 163767703.897466
 LP 228 unmatched irecvs 0 unmatched sends 0 Total sends 724 receives 724 collectives 0 delays 1704 wait alls 50 waits 0 send time 1019108.346654 wait 120293854.572289
 LP 234 unmatched irecvs 0 unmatched sends 0 Total sends 1170 receives 1170 collectives 0 delays 2606 wait alls 60 waits 0 send time 1477524.275769 wait 163838276.502195
 LP 235 unmatched irecvs 0 unmatched sends 0 Total sends 1080 receives 1080 collectives 0 delays 2426 wait alls 60 waits 0 send time 1362518.229356 wait 72469903.266244
 LP 236 unmatched irecvs 0 unmatched sends 0 Total sends 1034 receives 1034 collectives 0 delays 2340 wait alls 66 waits 0 send time 1305745.383152 wait 181265556.555207
 LP 237 unmatched irecvs 0 unmatched sends 0 Total sends 736 receives 736 collectives 0 delays 1738 wait alls 60 waits 0 send time 930078.205976 wait 124599280.836448
 LP 243 unmatched irecvs 0 unmatched sends 0 Total sends 748 receives 748 collectives 0 delays 1762 wait alls 60 waits 0 send time 944942.837639 wait 226684255.961120
 LP 244 unmatched irecvs 0 unmatched sends 0 Total sends 1266 receives 1266 collectives 0 delays 2798 wait alls 60 waits 0 send time 1559772.421147 wait 224068310.261899
 LP 245 unmatched irecvs 0 unmatched sends 0 Total sends 882 receives 882 collectives 0 delays 2030 wait alls 60 waits 0 send time 1171132.628482 wait 59646838.696376
 LP 246 unmatched irecvs 0 unmatched sends 0 Total sends 884 receives 884 collectives 0 delays 2034 wait alls 60 waits 0 send time 1169541.450742 wait 230349262.080623
 LP 252 unmatched irecvs 0 unmatched sends 0 Total sends 1132 receives 1132 collectives 0 delays 2530 wait alls 60 waits 0 send time 1419571.729277 wait 117445327.827391
 LP 253 unmatched irecvs 0 unmatched sends 0 Total sends 496 receives 496 collectives 0 delays 1248 wait alls 50 waits 0 send time 697422.312509 wait 51882411.210824
 LP 254 unmatched irecvs 0 unmatched sends 0 Total sends 668 receives 668 collectives 0 delays 1592 wait alls 50 waits 0 send time 897448.488961 wait 220915271.187779
 LP 255 unmatched irecvs 0 unmatched sends 0 Total sends 1482 receives 1482 collectives 0 delays 3230 wait alls 60 waits 0 send time 1843559.665634 wait 125682220.057065
 LP 261 unmatched irecvs 0 unmatched sends 0 Total sends 1732 receives 1732 collectives 0 delays 3730 wait alls 60 waits 0 send time 2055488.948681 wait 171337339.221986
 LP 262 unmatched irecvs 0 unmatched sends 0 Total sends 2020 receives 2020 collectives 0 delays 4316 wait alls 70 waits 0 send time 2390286.865081 wait 179397186.992181
 LP 263 unmatched irecvs 0 unmatched sends 0 Total sends 1038 receives 1038 collectives 0 delays 2332 wait alls 50 waits 0 send time 1400244.520539 wait 78380897.412120
 LP 264 unmatched irecvs 0 unmatched sends 0 Total sends 854 receives 854 collectives 0 delays 1974 wait alls 60 waits 0 send time 1130986.389140 wait 185598053.656403
 LP 270 unmatched irecvs 0 unmatched sends 0 Total sends 1318 receives 1318 collectives 0 delays 2912 wait alls 70 waits 0 send time 1605993.188225 wait 183441365.822489
 LP 271 unmatched irecvs 0 unmatched sends 0 Total sends 1494 receives 1494 collectives 0 delays 3254 wait alls 60 waits 0 send time 1853253.940284 wait 237731108.339606
 LP 272 unmatched irecvs 0 unmatched sends 0 Total sends 1114 receives 1114 collectives 0 delays 2484 wait alls 50 waits 0 send time 1506073.121325 wait 187989091.856423
 LP 273 unmatched irecvs 0 unmatched sends 0 Total sends 1504 receives 1504 collectives 0 delays 3274 wait alls 60 waits 0 send time 1925987.933597 wait 186961890.194399
 LP 279 unmatched irecvs 0 unmatched sends 0 Total sends 1284 receives 1284 collectives 0 delays 2834 wait alls 60 waits 0 send time 1699702.868855 wait 41587294.618191
 LP 280 unmatched irecvs 0 unmatched sends 0 Total sends 962 receives 962 collectives 0 delays 2190 wait alls 60 waits 0 send time 1268313.802547 wait 56179800.029834
 LP 281 unmatched irecvs 0 unmatched sends 0 Total sends 772 receives 772 collectives 0 delays 1800 wait alls 50 waits 0 send time 1045713.702199 wait 242366453.174915
 LP 282 unmatched irecvs 0 unmatched sends 0 Total sends 1444 receives 1444 collectives 0 delays 3154 wait alls 60 waits 0 send time 1827816.130192 wait 239838693.289420
 LP 288 unmatched irecvs 0 unmatched sends 0 Total sends 1086 receives 1086 collectives 0 delays 2428 wait alls 50 waits 0 send time 1495648.511026 wait 242109938.601502
 LP 289 unmatched irecvs 0 unmatched sends 0 Total sends 1474 receives 1474 collectives 0 delays 3214 wait alls 60 waits 0 send time 1866654.582048 wait 239148605.136481
 LP 290 unmatched irecvs 0 unmatched sends 0 Total sends 1666 receives 1666 collectives 0 delays 3598 wait alls 60 waits 0 send time 2074126.473026 wait 231683272.233514
 LP 291 unmatched irecvs 0 unmatched sends 0 Total sends 1270 receives 1270 collectives 0 delays 2806 wait alls 60 waits 0 send time 1570096.533302 wait 229041214.090651
 LP 297 unmatched irecvs 0 unmatched sends 0 Total sends 1168 receives 1168 collectives 0 delays 2602 wait alls 60 waits 0 send time 1478174.260760 wait 228006413.133874
 LP 298 unmatched irecvs 0 unmatched sends 0 Total sends 1854 receives 1854 collectives 0 delays 3984 wait alls 70 waits 0 send time 2235385.680993 wait 54407837.495879
 LP 299 unmatched irecvs 0 unmatched sends 0 Total sends 1530 receives 1530 collectives 0 delays 3326 wait alls 60 waits 0 send time 1918623.175208 wait 43435109.217533
 LP 300 unmatched irecvs 0 unmatched sends 0 Total sends 1688 receives 1688 collectives 0 delays 3642 wait alls 60 waits 0 send time 2073585.683253 wait 180443386.817762
 LP 306 unmatched irecvs 0 unmatched sends 0 Total sends 1006 receives 1006 collectives 0 delays 2268 wait alls 50 waits 0 send time 1369678.066230 wait 234827300.894513
 LP 307 unmatched irecvs 0 unmatched sends 0 Total sends 1182 receives 1182 collectives 0 delays 2630 wait alls 60 waits 0 send time 1453872.587879 wait 225128096.688900
 LP 308 unmatched irecvs 0 unmatched sends 0 Total sends 774 receives 774 collectives 0 delays 1814 wait alls 60 waits 0 send time 986412.539371 wait 238588346.901442
 LP 309 unmatched irecvs 0 unmatched sends 0 Total sends 1064 receives 1064 collectives 0 delays 2394 wait alls 60 waits 0 send time 1368119.135462 wait 230947991.556343
 LP 315 unmatched irecvs 0 unmatched sends 0 Total sends 676 receives 676 collectives 0 delays 1608 wait alls 50 waits 0 send time 925767.213918 wait 229314849.631709
 LP 316 unmatched irecvs 0 unmatched sends 0 Total sends 1082 receives 1082 collectives 0 delays 2430 wait alls 60 waits 0 send time 1359575.569396 wait 241533502.538067
 LP 317 unmatched irecvs 0 unmatched sends 0 Total sends 958 receives 958 collectives 0 delays 2182 wait alls 60 waits 0 send time 1216238.557433 wait 235232023.804240
 LP 318 unmatched irecvs 0 unmatched sends 0 Total sends 544 receives 544 collectives 0 delays 1350 wait alls 56 waits 0 send time 728295.820487 wait 241818960.780068
 LP 324 unmatched irecvs 0 unmatched sends 0 Total sends 942 receives 942 collectives 0 delays 2156 wait alls 66 waits 0 send time 1140182.089447 wait 47864710.306853
 LP 325 unmatched irecvs 0 unmatched sends 0 Total sends 754 receives 754 collectives 0 delays 1774 wait alls 60 waits 0 send time 1023713.128732 wait 239851624.351898
 LP 326 unmatched irecvs 0 unmatched sends 0 Total sends 1178 receives 1178 collectives 0 delays 2622 wait alls 60 waits 0 send time 1446818.706926 wait 227632766.612120
 LP 327 unmatched irecvs 0 unmatched sends 0 Total sends 682 receives 682 collectives 0 delays 1620 wait alls 50 waits 0 send time 948953.171935 wait 238425191.492895
 LP 333 unmatched irecvs 0 unmatched sends 0 Total sends 930 receives 930 collectives 0 delays 2126 wait alls 60 waits 0 send time 1197751.448529 wait 235173176.706286
 LP 334 unmatched irecvs 0 unmatched sends 0 Total sends 748 receives 748 collectives 0 delays 1762 wait alls 60 waits 0 send time 921654.263274 wait 227369786.169390
 LP 335 unmatched irecvs 0 unmatched sends 0 Total sends 904 receives 904 collectives 0 delays 2074 wait alls 60 waits 0 send time 1175455.369354 wait 239444069.426787
 LP 336 unmatched irecvs 0 unmatched sends 0 Total sends 1574 receives 1574 collectives 0 delays 3420 wait alls 66 waits 0 send time 1909375.763095 wait 240876821.702868
 LP 342 unmatched irecvs 0 unmatched sends 0 Total sends 1064 receives 1064 collectives 0 delays 2384 wait alls 50 waits 0 send time 1431078.085745 wait 239582472.113191
 LP 343 unmatched irecvs 0 unmatched sends 0 Total sends 1022 receives 1022 collectives 0 delays 2300 wait alls 50 waits 0 send time 1392639.599693 wait 54306997.881879
 LP 344 unmatched irecvs 0 unmatched sends 0 Total sends 1392 receives 1392 collectives 0 delays 3050 wait alls 60 waits 0 send time 1762868.547964 wait 237377876.573738
 LP 345 unmatched irecvs 0 unmatched sends 0 Total sends 732 receives 732 collectives 0 delays 1720 wait alls 50 waits 0 send time 1009808.307314 wait 229509575.923357
 LP 351 unmatched irecvs 0 unmatched sends 0 Total sends 1096 receives 1096 collectives 0 delays 2458 wait alls 60 waits 0 send time 1395358.000737 wait 239002588.009873
 LP 352 unmatched irecvs 0 unmatched sends 0 Total sends 1082 receives 1082 collectives 0 delays 2420 wait alls 50 waits 0 send time 1468959.213742 wait 238612525.266457
 LP 353 unmatched irecvs 0 unmatched sends 0 Total sends 1540 receives 1540 collectives 0 delays 3346 wait alls 60 waits 0 send time 1910908.953035 wait 236151256.616066
 LP 354 unmatched irecvs 0 unmatched sends 0 Total sends 1070 receives 1070 collectives 0 delays 2396 wait alls 50 waits 0 send time 1460741.955959 wait 244232512.426837
 LP 360 unmatched irecvs 0 unmatched sends 0 Total sends 1070 receives 1070 collectives 0 delays 2396 wait alls 50 waits 0 send time 1439992.987388 wait 232491468.639140
 LP 361 unmatched irecvs 0 unmatched sends 0 Total sends 1002 receives 1002 collectives 0 delays 2270 wait alls 60 waits 0 send time 1268899.691438 wait 234563596.485585
 LP 362 unmatched irecvs 0 unmatched sends 0 Total sends 736 receives 736 collectives 0 delays 1728 wait alls 50 waits 0 send time 1047246.540606 wait 226312200.080464
 LP 363 unmatched irecvs 0 unmatched sends 0 Total sends 972 receives 972 collectives 0 delays 2200 wait alls 50 waits 0 send time 1346636.251382 wait 234003752.808560
 LP 369 unmatched irecvs 0 unmatched sends 0 Total sends 1762 receives 1762 collectives 0 delays 3790 wait alls 60 waits 0 send time 2158949.638434 wait 240479668.726626
 LP 370 unmatched irecvs 0 unmatched sends 0 Total sends 1684 receives 1684 collectives 0 delays 3634 wait alls 60 waits 0 send time 2045615.450660 wait 233456691.814360
 LP 371 unmatched irecvs 0 unmatched sends 0 Total sends 1410 receives 1410 collectives 0 delays 3086 wait alls 60 waits 0 send time 1793352.499286 wait 239029658.233294
 LP 372 unmatched irecvs 0 unmatched sends 0 Total sends 948 receives 948 collectives 0 delays 2162 wait alls 60 waits 0 send time 1232901.018381 wait 236571751.867198
 LP 378 unmatched irecvs 0 unmatched sends 0 Total sends 954 receives 954 collectives 0 delays 2174 wait alls 60 waits 0 send time 1252010.787490 wait 28872020.499920
 LP 379 unmatched irecvs 0 unmatched sends 0 Total sends 1270 receives 1270 collectives 0 delays 2806 wait alls 60 waits 0 send time 1627886.750878 wait 223550083.236020
 LP 380 unmatched irecvs 0 unmatched sends 0 Total sends 1506 receives 1506 collectives 0 delays 3278 wait alls 60 waits 0 send time 1877065.936016 wait 44484845.249219
 LP 381 unmatched irecvs 0 unmatched sends 0 Total sends 980 receives 980 collectives 0 delays 2216 wait alls 50 waits 0 send time 1347203.572137 wait 37974722.570828
 LP 387 unmatched irecvs 0 unmatched sends 0 Total sends 1702 receives 1702 collectives 0 delays 3676 wait alls 66 waits 0 send time 2071769.522280 wait 120930600.863587
 LP 388 unmatched irecvs 0 unmatched sends 0 Total sends 846 receives 846 collectives 0 delays 1958 wait alls 60 waits 0 send time 1121676.269522 wait 236390395.372274
 LP 389 unmatched irecvs 0 unmatched sends 0 Total sends 644 receives 644 collectives 0 delays 1554 wait alls 60 waits 0 send time 849074.614689 wait 239583967.466475
 LP 390 unmatched irecvs 0 unmatched sends 0 Total sends 898 receives 898 collectives 0 delays 2062 wait alls 60 waits 0 send time 1194248.256042 wait 77551255.308577
 LP 396 unmatched irecvs 0 unmatched sends 0 Total sends 1394 receives 1394 collectives 0 delays 3066 wait alls 72 waits 0 send time 1680447.482774 wait 181954755.003045
 LP 397 unmatched irecvs 0 unmatched sends 0 Total sends 1448 receives 1448 collectives 0 delays 3168 wait alls 66 waits 0 send time 1730700.253292 wait 179452043.558964
 LP 398 unmatched irecvs 0 unmatched sends 0 Total sends 1090 receives 1090 collectives 0 delays 2446 wait alls 60 waits 0 send time 1362520.825888 wait 184339756.060921
 LP 399 unmatched irecvs 0 unmatched sends 0 Total sends 964 receives 964 collectives 0 delays 2194 wait alls 60 waits 0 send time 1163915.966380 wait 226976312.408118
 LP 405 unmatched irecvs 0 unmatched sends 0 Total sends 654 receives 654 collectives 0 delays 1574 wait alls 60 waits 0 send time 785996.187805 wait 235819906.841401
 LP 406 unmatched irecvs 0 unmatched sends 0 Total sends 654 receives 654 collectives 0 delays 1570 wait alls 56 waits 0 send time 842502.145936 wait 182925092.226000
 LP 407 unmatched irecvs 0 unmatched sends 0 Total sends 734 receives 734 collectives 0 delays 1730 wait alls 56 waits 0 send time 924714.183380 wait 188442299.926165
 LP 408 unmatched irecvs 0 unmatched sends 0 Total sends 1032 receives 1032 collectives 0 delays 2340 wait alls 70 waits 0 send time 1235125.968845 wait 161082373.801387
 LP 414 unmatched irecvs 0 unmatched sends 0 Total sends 778 receives 778 collectives 0 delays 1828 wait alls 66 waits 0 send time 972096.350793 wait 231361833.270194
 LP 415 unmatched irecvs 0 unmatched sends 0 Total sends 726 receives 726 collectives 0 delays 1724 wait alls 66 waits 0 send time 840037.942789 wait 161424164.007532
 LP 416 unmatched irecvs 0 unmatched sends 0 Total sends 784 receives 784 collectives 0 delays 1836 wait alls 62 waits 0 send time 991389.414077 wait 182640364.070923
 LP 417 unmatched irecvs 0 unmatched sends 0 Total sends 1380 receives 1380 collectives 0 delays 3036 wait alls 70 waits 0 send time 1648899.118545 wait 233117396.568241
 LP 423 unmatched irecvs 0 unmatched sends 0 Total sends 690 receives 690 collectives 0 delays 1636 wait alls 50 waits 0 send time 973248.559611 wait 162699136.436757
 LP 424 unmatched irecvs 0 unmatched sends 0 Total sends 682 receives 682 collectives 0 delays 1620 wait alls 50 waits 0 send time 955682.437545 wait 230925787.404967
 LP 425 unmatched irecvs 0 unmatched sends 0 Total sends 1206 receives 1206 collectives 0 delays 2678 wait alls 60 waits 0 send time 1487774.469349 wait 228363345.677702
 LP 426 unmatched irecvs 0 unmatched sends 0 Total sends 728 receives 728 collectives 0 delays 1722 wait alls 60 waits 0 send time 912367.151058 wait 229756244.240830
 LP 432 unmatched irecvs 0 unmatched sends 0 Total sends 722 receives 722 collectives 0 delays 1710 wait alls 60 waits 0 send time 911450.421544 wait 40130943.189459
 LP 433 unmatched irecvs 0 unmatched sends 0 Total sends 1064 receives 1064 collectives 0 delays 2394 wait alls 60 waits 0 send time 1361151.609776 wait 74152714.250620
 LP 434 unmatched irecvs 0 unmatched sends 0 Total sends 972 receives 972 collectives 0 delays 2210 wait alls 60 waits 0 send time 1252078.728616 wait 227835027.875529
 LP 435 unmatched irecvs 0 unmatched sends 0 Total sends 1156 receives 1156 collectives 0 delays 2578 wait alls 60 waits 0 send time 1447239.062585 wait 228657241.661385
 LP 441 unmatched irecvs 0 unmatched sends 0 Total sends 788 receives 788 collectives 0 delays 1842 wait alls 60 waits 0 send time 1062313.289631 wait 237410160.582239
 LP 442 unmatched irecvs 0 unmatched sends 0 Total sends 884 receives 884 collectives 0 delays 2034 wait alls 60 waits 0 send time 1089451.411746 wait 121658383.355642
 LP 443 unmatched irecvs 0 unmatched sends 0 Total sends 950 receives 950 collectives 0 delays 2166 wait alls 60 waits 0 send time 1155609.993471 wait 164773488.395418
 LP 444 unmatched irecvs 0 unmatched sends 0 Total sends 1086 receives 1086 collectives 0 delays 2438 wait alls 60 waits 0 send time 1362374.449758 wait 161352193.949322
 LP 450 unmatched irecvs 0 unmatched sends 0 Total sends 1188 receives 1188 collectives 0 delays 2652 wait alls 70 waits 0 send time 1477398.179498 wait 120814719.110398
 LP 451 unmatched irecvs 0 unmatched sends 0 Total sends 1102 receives 1102 collectives 0 delays 2470 wait alls 60 waits 0 send time 1351933.540047 wait 120456427.367757
 LP 452 unmatched irecvs 0 unmatched sends 0 Total sends 1612 receives 1612 collectives 0 delays 3502 wait alls 72 waits 0 send time 1880752.859434 wait 226689385.494537
 LP 453 unmatched irecvs 0 unmatched sends 0 Total sends 694 receives 694 collectives 0 delays 1654 wait alls 60 waits 0 send time 878190.189927 wait 119678148.347902
 LP 459 unmatched irecvs 0 unmatched sends 0 Total sends 934 receives 934 collectives 0 delays 2140 wait alls 66 waits 0 send time 1122283.617665 wait 124098407.652423
 LP 460 unmatched irecvs 0 unmatched sends 0 Total sends 896 receives 896 collectives 0 delays 2058 wait alls 60 waits 0 send time 1150567.877600 wait 187384767.985001
 LP 461 unmatched irecvs 0 unmatched sends 0 Total sends 1174 receives 1174 collectives 0 delays 2614 wait alls 60 waits 0 send time 1423410.518199 wait 183216359.475160
 LP 462 unmatched irecvs 0 unmatched sends 0 Total sends 804 receives 804 collectives 0 delays 1874 wait alls 60 waits 0 send time 1074091.749002 wait 77431811.628409
 LP 468 unmatched irecvs 0 unmatched sends 0 Total sends 970 receives 970 collectives 0 delays 2206 wait alls 60 waits 0 send time 1211009.781027 wait 117800903.942812
 LP 469 unmatched irecvs 0 unmatched sends 0 Total sends 606 receives 606 collectives 0 delays 1478 wait alls 60 waits 0 send time 809266.469347 wait 231986628.903106
 LP 470 unmatched irecvs 0 unmatched sends 0 Total sends 464 receives 464 collectives 0 delays 1190 wait alls 56 waits 0 send time 584690.327968 wait 162310616.991503
 LP 471 unmatched irecvs 0 unmatched sends 0 Total sends 736 receives 736 collectives 0 delays 1744 wait alls 66 waits 0 send time 940218.685630 wait 186896575.091691
 LP 477 unmatched irecvs 0 unmatched sends 0 Total sends 866 receives 866 collectives 0 delays 1998 wait alls 60 waits 0 send time 1009355.984707 wait 48347050.325136
 LP 478 unmatched irecvs 0 unmatched sends 0 Total sends 776 receives 776 collectives 0 delays 1818 wait alls 60 waits 0 send time 934262.453814 wait 237456164.552619
 LP 479 unmatched irecvs 0 unmatched sends 0 Total sends 576 receives 576 collectives 0 delays 1418 wait alls 60 waits 0 send time 751647.854004 wait 235302036.020718
 LP 480 unmatched irecvs 0 unmatched sends 0 Total sends 468 receives 468 collectives 0 delays 1198 wait alls 56 waits 0 send time 572385.313953 wait 226989333.384371
	: Running Time = 10.3426 seconds

TW Library Statistics:
	Total Events Processed                                11540841
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                  11540841
	Event Rate (events/sec)                              1115858.1
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        469247
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)     24.7646

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 136925280 recvd 136925280 
 max runtime 832428174.181527 ns avg runtime 831592158.607247 
 max comm time 244256041.446027 avg comm time 165598826.375765 
 max send time 2390286.865081 avg send time 1321096.933746 
 max recv time 1242807411.964691 avg recv time 491966904.633888 
 max wait time 244232512.426837 avg wait time 165575909.276246 
LP-IO: writing output to amg-216-trace-11281-1495530749/
LP-IO: data files:
   amg-216-trace-11281-1495530749/dragonfly-router-traffic
   amg-216-trace-11281-1495530749/dragonfly-router-stats
   amg-216-trace-11281-1495530749/dragonfly-msg-stats
   amg-216-trace-11281-1495530749/model-net-category-all
   amg-216-trace-11281-1495530749/model-net-category-test
   amg-216-trace-11281-1495530749/mpi-replay-stats
 Average number of hops traversed 3.033524 average chunk latency 1.896739 us maximum chunk latency 17.016149 us avg message size 610.249207 bytes finished messages 224376 finished chunks 702636 

 ADAPTIVE ROUTING STATS: 574306 chunks routed minimally 128330 chunks routed non-minimally completed packets 702636 

 Total packets generated 449894 finished 449894
 
 #+END_EXAMPLE

Je sais pas trop si ca a fonctionné, mais plusieurs fichiers ont été
créé.
#+BEGIN_EXAMPLE
-rw-r--r-- 1 chevamax chevamax        0 mai   23 11:12 dragonfly-cn-sampling-0.bin
-rw-r--r-- 1 chevamax chevamax      245 mai   23 11:12 dragonfly-cn-sampling.meta
-rw-r--r-- 1 chevamax chevamax        0 mai   23 11:12 dragonfly-router-sampling-0.bin
-rw-r--r-- 1 chevamax chevamax      356 mai   23 11:12 dragonfly-router-sampling.meta
-rw-r--r-- 1 chevamax chevamax        0 mai   23 11:12 mpi-aggregate-logs-0.bin
-rw-r--r-- 1 chevamax chevamax 15605566 mai   23 11:12 mpi-op-logs
-rw-r--r-- 1 chevamax chevamax        0 mai   23 11:12 mpi-workload-meta-log
-rw-r--r-- 1 chevamax chevamax      991 mai   23 11:12 ross.csv
#+END_EXAMPLE

**** Tentative replay HPL sur CODES                            :CODES:HPL:
Il faut maintenant que je trouve comment faire le fichier .conf pour
qu'il corresponde à ma machine.

D'après les informations obtenues avec la commande dumpistats ci
dessous, mes noeuds ont communiqué 2 à 2 (pour la configuration à
4). A 16, c'est différent.
#+BEGIN_EXAMPLE
dumpistats --bin'begin+10 to end-10' -x all -i dumpi-2017.05.22.11.02.24.meta -o statsT
#+END_EXAMPLE

Utilisation d'une configuration de base :
#+BEGIN_EXAMPLE
LPGROUPS
{
   MODELNET_GRP
   {
      repetitions="1";
      modelnet_simplenet="1";
      nw-lp="4";
   }
}
PARAMS
{
   packet_size="512";
   message_size="784";
   modelnet_order=( "simplenet" );
   # scheduler options
   modelnet_scheduler="fcfs";
   net_startup_ns="1.5";
   net_bw_mbps="20000";
}
#+END_EXAMPLE

en tentant avec ca :

#+begin_src sh :results output :exports both
./model-net-mpi-replay --sync=1 --num_net_traces=1 --workload_file=/home/chevamax/Documents/Stage_LIG_2017/simulation/hpl/4/dumpi-2017.05.22.11.02.24- --workload_type="dumpi" --lp-io-dir=hpl-trace --lp-io-use-suffix=1 -- /home/chevamax/Documents/Stage_LIG_2017/logiciels/CODES/src/network-workloads/conf/modelnet-mpi-test-hpl.conf
#+end_src

#+RESULTS:
#+BEGIN_EXAMPLE
Tue May 23 14:50:03 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                    5
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              1281
	Network events                                           50000
	Total events                                             51280


 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type 
 Undefined data type *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 1 unmatched sends 0 Total sends 0 receives 1 collectives 0 delays 7 wait alls 0 waits 0 send time 0.000000 wait 0.000000
	: Running Time = 0.0000 seconds

TW Library Statistics:
	Total Events Processed                                       8
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                         8
	Event Rate (events/sec)                               307692.3
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         51281
	Memory Allocated                                         51168
	Memory Wasted                                              720

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.0001

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 0 recvd 4 
 max runtime 0.000000 ns avg runtime 0.000000 
 max comm time 0.000000 avg comm time -66232.000000 
 max send time 0.000000 avg send time 0.000000 
 max recv time 0.000000 avg recv time 0.000000 
 max wait time 0.000000 avg wait time 0.000000 
LP-IO: writing output to hpl-trace-25282-1495543803/
LP-IO: data files:
   hpl-trace-25282-1495543803/mpi-replay-stats
   hpl-trace-25282-1495543803/model-net-category-all
#+END_EXAMPLE

Ca à pas l'air de fonctionner...

**** Meeting Florence

Topologie de ma machine pour les experiences HPL (4/16 noeuds)
.conf Réalisation d'un .conf correct (via documentation)
dumpi: Y a t'il un soucis avec ma configuration DUMPI, qui a des
soucis avec les versions récentes de MPICH. (Utilisation de OpenMPI ?)

code source AMG : peut permettre d'aider en le compilant et
l'executant sur ma machine pour faire des traces.


**** WAITING Mail CODES
Pour avoir plus d'information 

*** 2017-05-24 mercredi

**** Meeting Florence
- Continuer sur les fichiers de configuration
- Si pas de solution, utiliser la mailing list de CODES https://lists.mcs.anl.gov/mailman/listinfo/codes-ross-users
- Tenter la conversion OTF2
- Hpl semble être une topologie en anneau virtuel (demander
  confirmation à Christian/Tom)
- Lire Chapitre 3 (début sur les topologies) et 4 du livre d'Arnaud.

**** Recherche utilisation DUMPI                             :CODES:DUMPI:
Le site https://xgitlab.cels.anl.gov/codes/codes/tree/master est plus
à jour que les version téléchargeables. Il y a en effet tous les
fichiers link par la documentation.

***** Wiki                                                        :CODES:
https://xgitlab.cels.anl.gov/codes/codes/wikis/home

***** AMG 2x2x2
#+BEGIN_EXAMPLE
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2014.03.03.14.12.46- --workload_type=dumpi --lp-io-dir=amg-8-trace --lp-io-use-suffix=1 -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf 

Wed May 24 11:27:03 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 306 receives 306 collectives 0 delays 814 wait alls 52 waits 0 send time 454987.043399 wait 5480162.099075
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 302 receives 302 collectives 0 delays 806 wait alls 52 waits 0 send time 437452.653750 wait 5509041.914856
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 286 receives 286 collectives 0 delays 774 wait alls 52 waits 0 send time 417481.542902 wait 5617532.521296
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 302 receives 302 collectives 0 delays 806 wait alls 52 waits 0 send time 454267.648339 wait 211396.551050
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 300 receives 300 collectives 0 delays 802 wait alls 52 waits 0 send time 432114.409959 wait 5616206.186374
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 288 receives 288 collectives 0 delays 778 wait alls 52 waits 0 send time 412936.417664 wait 4086486.198338
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 314 receives 314 collectives 0 delays 830 wait alls 52 waits 0 send time 443263.126450 wait 5487746.164523
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 306 receives 306 collectives 0 delays 814 wait alls 52 waits 0 send time 451719.941103 wait 3100534.023560
	: Running Time = 0.0817 seconds

TW Library Statistics:
	Total Events Processed                                  133778
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    133778
	Event Rate (events/sec)                              1637048.9
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        469247
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.1957

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 2995472 recvd 2995472 
 max runtime 204724201.764873 ns avg runtime 204720205.039492 
 max comm time 5624775.444477 avg comm time 4396023.664492 
 max send time 454987.043399 avg send time 438027.847946 
 max recv time 27624654.204193 avg recv time 12025780.130547 
 max wait time 5617532.521296 avg wait time 4388638.207384 
LP-IO: writing output to amg-8-trace-3127-1495618023/
LP-IO: data files:
   amg-8-trace-3127-1495618023/dragonfly-router-traffic
   amg-8-trace-3127-1495618023/dragonfly-router-stats
   amg-8-trace-3127-1495618023/dragonfly-msg-stats
   amg-8-trace-3127-1495618023/model-net-category-all
   amg-8-trace-3127-1495618023/model-net-category-test
   amg-8-trace-3127-1495618023/mpi-replay-stats
 Average number of hops traversed 1.370578 average chunk latency 2.238281 us maximum chunk latency 17.937647 us avg message size 1246.036621 bytes finished messages 2404 finished chunks 13174 

 ADAPTIVE ROUTING STATS: 13174 chunks routed minimally 0 chunks routed non-minimally completed packets 13174 

 Total packets generated 7564 finished 7564 

#+END_EXAMPLE

Maintenant je vais générer les traces dumpi.
D'apres les traces dumpi, le logiciel a été lancer avec les parametres
suivants : =../../test/amg2013.dumpi", "-laplace", "-n", "40",
"40","40", "-P", "2", "2", "2"= 

****** Compilation AMG
Changement du makefile.include pour ajouter le lien
vers la librairie dumpi + quelques autres options :
#+BEGIN_EXAMPLE
INCLUDE_CFLAGS = -O2 -DTIMER_USE_MPI -HYPRE_LONG_LONG 
INCLUDE_LFLAGS = -lm
#+END_EXAMPLE

et modifier le makefile du sous dossier test avec les liens dans
LFLAGS (ou les mettre dans le CFLAGS ci-dessus)
#+BEGIN_EXAMPLE
-L<path to dumpi lib> -ldumpi
#+END_EXAMPLE

****** Lancement d'AMG
#+begin_src sh :results output :exports both
mpirun -np 8 ./amg2013 -laplace -n 40 40 40 -P 2 2 2
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
  3D 7-point Laplace problem on a cube
  (nx_global, ny_global, nz_global) = (80, 80, 80)
  (Px, Py, Pz) = (2, 2, 2)
  (cx, cy, cz) = (1.000000, 1.000000, 1.000000)

=============================================
Setup matrix and rhs:
=============================================
Setup matrix and rhs:
Setup matrix and rhs  wall clock time = 0.060024 seconds
Setup matrix and rhs  cpu clock time  = 0.015629 seconds
=============================================
Setup phase times:
=============================================
PCG Setup:
PCG Setup  wall clock time = 5.858475 seconds
PCG Setup  cpu clock time  = 2.916854 seconds

System Size / Setup Phase Time: 8.739475e+04

=============================================
Solve phase times:
=============================================
PCG Solve:
PCG Solve  wall clock time = 7.226493 seconds
PCG Solve  cpu clock time  = 3.576355 seconds

AMG2013 Benchmark version 1.0
Iterations = 12
Final Relative Residual Norm = 3.236117e-07

System Size * Iterations / Solve Phase Time: 8.502050e+05
#+END_EXAMPLE

****** Rejouer les traces 
#+begin_src sh
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.05.24.13.06.01- --workload_type=dumpi --lp-io-dir=amg-8-trace --lp-io-use-suffix=1 -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed May 24 13:09:07 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 7627778.443787 wait 667432446.994742
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 7059958.455029 wait 678810251.070519
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 7500539.710242 wait 663184169.518037
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 7407161.176370 wait 735409722.753113
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 8326637.819773 wait 772007202.073255
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 7644843.716817 wait 674714897.711984
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 8007634.995992 wait 631998280.891059
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 7420675.017480 wait 621048688.444790
	: Running Time = 1.2770 seconds

TW Library Statistics:
	Total Events Processed                                 2202661
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                   2202661
	Event Rate (events/sec)                              1724837.8
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        469247
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      3.0578

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 56024028 recvd 56024028 
 max runtime 1776257220.685390 ns avg runtime 1771414870.410956 
 max comm time 772087959.012618 avg comm time 680658436.910956 
 max send time 8326637.819773 avg send time 7624403.666936 
 max recv time 1612631904.903633 avg recv time 1503182135.896835 
 max wait time 772007202.073255 avg wait time 680575707.432187 
LP-IO: writing output to amg-8-trace-17411-1495624147/
LP-IO: data files:
   amg-8-trace-17411-1495624147/dragonfly-router-traffic
   amg-8-trace-17411-1495624147/dragonfly-router-stats
   amg-8-trace-17411-1495624147/dragonfly-msg-stats
   amg-8-trace-17411-1495624147/model-net-category-all
   amg-8-trace-17411-1495624147/model-net-category-test
   amg-8-trace-17411-1495624147/mpi-replay-stats
 Average number of hops traversed 1.364874 average chunk latency 4.296207 us maximum chunk latency 94.070031 us avg message size 2141.427490 bytes finished messages 26162 finished chunks 234149 

 ADAPTIVE ROUTING STATS: 234149 chunks routed minimally 0 chunks routed non-minimally completed packets 234149 

 Total packets generated 127075 finished 127075 

#+END_EXAMPLE
Ca fonctionne donc normalement. J'ai rejoué les traces sur le fichier
de configuration dfly216 et ca à donc bien fonctionné.

**** DUMPI + OTF2
https://github.com/sstsimulator/sst-dumpi/blob/master/docs/tools.dox
Lien libotf utilisé :
http://download.savannah.gnu.org/releases/m17n/libotf-0.9.13.tar.gz
Dependance installée :
http://download.savannah.gnu.org/releases/freetype/freetype-2.8.tar.gz


installation de libotf-dev

Cependant le configure ne trouve pas la librairie...
C'est dans la partie suivante du Makefile que ca se joue :
#+BEGIN_EXAMPLE
# If we are building binary utlities, test wheter we can build the
# dumpi-to-otf converter.
if test "$disable_bin" != "yes"; then
   oldlibs="$LIBS"
   LIBS="$LIBS -lotf -lz"
   { $as_echo "$as_me:${as_lineno-$LINENO}: checking wheter OTF library and header file can be found" >&5
$as_echo_n "checking wheter OTF library and header file can be found... " >&6; }
   cat confdefs.h - <<_ACEOF >conftest.$ac_ext
/* end confdefs.h.  */

	  #include <otf.h>
	  #include <stdlib.h>

int
main ()
{

	  OTF_FileManager *mgr = NULL;
	  mgr = OTF_FileManager_open(10);
	  if(!mgr) {
	    return EXIT_FAILURE;
	  }
	  OTF_FileManager_close(mgr);
	  return EXIT_SUCCESS;


  ;
  return 0;
}
_ACEOF
if ac_fn_c_try_link "$LINENO"; then :

        { $as_echo "$as_me:${as_lineno-$LINENO}: result: yes" >&5
$as_echo "yes" >&6; }
	enable_otf="yes"

else

        { $as_echo "$as_me:${as_lineno-$LINENO}: result: no" >&5
$as_echo "no" >&6; }
	LIBS="$oldlibs"


fi
rm -f core conftest.err conftest.$ac_objext \
    conftest$ac_exeext conftest.$ac_ext
fi

 if test "$disable_bin" != "yes"; then
  WITH_BIN_TRUE=
  WITH_BIN_FALSE='#'
else
  WITH_BIN_TRUE='#'
  WITH_BIN_FALSE=
fi

 if test "$enable_test" = "yes"; then
  WITH_TEST_TRUE=
  WITH_TEST_FALSE='#'
else
  WITH_TEST_TRUE='#'
  WITH_TEST_FALSE=
fi

 if test "$enable_libdumpi" = "yes"; then
  WITH_LIBDUMPI_TRUE=
  WITH_LIBDUMPI_FALSE='#'
else
  WITH_LIBDUMPI_TRUE='#'
  WITH_LIBDUMPI_FALSE=
fi

 if test "$disable_libundumpi" != "yes"; then
  WITH_LIBUNDUMPI_TRUE=
  WITH_LIBUNDUMPI_FALSE='#'
else
  WITH_LIBUNDUMPI_TRUE='#'
  WITH_LIBUNDUMPI_FALSE=
fi

 if test "$enable_otf" = "yes"; then
  WITH_OTF_TRUE=
  WITH_OTF_FALSE='#'
else
  WITH_OTF_TRUE='#'
  WITH_OTF_FALSE=
fi
#+END_EXAMPLE
Pourtant j'ai bien la librairie dans =/usr/lib/x84_64-linux-gnu/= pour
le package libotf-dev et dans =/usr/local/lib= pour celui installé à la
main.
J'ai également trois fichiers otf.h :
- =./home/chevamax/Documents/Stage_LIG_2017/logiciels/libotf-0.9.13/src/otf.h=
- =./usr/local/include/otf.h=
- =./usr/include/otf.h=

je lance le fichier de configuration avec 
#+BEGIN_EXAMPLE
../configure --enable-libdumpi --enable-test --prefix=/home/chevamax/Documents/Stage_LIG_2017/logiciels/dumpiOTF/sst-dumpi/install
CC=mpicc CXX=mpiCC CFLAGS="-DMPICH_SUPPRESS_PROTOTYPES=1
-DHAVE_PRAGMA_HP_SEC_DEF=1 -pthread"
#+END_EXAMPLE

*** 2017-05-26 vendredi

**** Mail Arnaud 1
Bonjour Maxime,

Le 24/05/2017 08:44, Maxime Chevalier a écrit :
> pour rejouer des traces sur CODES il me faut une topologie de réseau. Du
> coup je voulais savoir quelle topologie utiliser pour rejouer les traces
> faites sur mon ordinateurs (2x2 et 4x4).

Désolé pour le temps de réponse. J'étais en réunion toute la
journée. À ce stade, la topologie n'a pas d'importance. Commence par
la plus simple possible pour voir ce que ça donne. J'ai regardé la
sortie donnée dans "**** Tentative replay exemple sur CODES". Ça fait
beaucoup d'informations sur l'exécution mais je n'ai pas trop réussi à
trouver combien de temps ça prenait en pratique. C'est ce genre de
question qu'il fadra se poser. Pour moi, le fait que la simulation
parallèle avec CODES même avec un modèle simple soit plus rapide qu'un
rejeu séquentiel avec SimGrid reste à démontrer. Et une fois ceci
infirmé ou confirmé, ce qui va nous intéresser, c'est comment
paralléliser en temps une simulation SimGrid de HPL mais il faut
d'abord bien étudier l'existant et comprendre ses limitations...

Pour scalatrace, si je ne m'abuse, c'est un autre format de trace
particulièrement compressé et qui exploite la régularité des codes
parallèles. Mais tu peux poser la question à Lucas et vérifier.

A+
    Arnaud

**** Objectifs
- [X] Faire tourner les simulations en parrallèle sur CODES (sync=3)
- [X] Trouver le temps d'execution d'une simulation (de ROSS)
- [X] Analyser le contenu des fichiers produits
- [ ] Faire un .conf de Grille 2D
- [ ] Dumpi + OTF

**** Analyser le contenu des fichiers produits                     :CODES:

***** Simulation séquentielle

****** Sortie Console
#+BEGIN_EXAMPLE
Tue May 23 11:12:28 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 478 receives 478 collectives 0 delays 1218 wait alls 56 waits 0 send time 612826.428503 wait 14669328.061635
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 832 receives 832 collectives 0 delays 1930 wait alls 60 waits 0 send time 1018385.000731 wait 64370657.327652
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 566 receives 566 collectives 0 delays 1398 wait alls 60 waits 0 send time 750754.503860 wait 72563845.789648
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 840 receives 840 collectives 0 delays 1946 wait alls 60 waits 0 send time 1047815.519626 wait 157051216.933560
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 464 receives 464 collectives 0 delays 1184 wait alls 50 waits 0 send time 647397.328881 wait 66705324.503492
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 386 receives 386 collectives 0 delays 1034 wait alls 56 waits 0 send time 503242.249048 wait 66246441.900921
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 654 receives 654 collectives 0 delays 1574 wait alls 60 waits 0 send time 846491.070120 wait 117358929.038023
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 790 receives 790 collectives 0 delays 1846 wait alls 60 waits 0 send time 1041571.325534 wait 70288613.070390
 LP 18 unmatched irecvs 0 unmatched sends 0 Total sends 1194 receives 1194 collectives 0 delays 2654 wait alls 60 waits 0 send time 1470155.471660 wait 68216556.238328
 LP 19 unmatched irecvs 0 unmatched sends 0 Total sends 1010 receives 1010 collectives 0 delays 2286 wait alls 60 waits 0 send time 1264102.027885 wait 66450400.852029
 LP 20 unmatched irecvs 0 unmatched sends 0 Total sends 956 receives 956 collectives 0 delays 2178 wait alls 60 waits 0 send time 1205111.164923 wait 59633424.426049
 LP 21 unmatched irecvs 0 unmatched sends 0 Total sends 528 receives 528 collectives 0 delays 1318 wait alls 56 waits 0 send time 723458.275000 wait 117875229.796452
 LP 27 unmatched irecvs 0 unmatched sends 0 Total sends 1026 receives 1026 collectives 0 delays 2324 wait alls 66 waits 0 send time 1235379.351462 wait 74247344.022784
 LP 28 unmatched irecvs 0 unmatched sends 0 Total sends 1004 receives 1004 collectives 0 delays 2274 wait alls 60 waits 0 send time 1255522.120947 wait 118504878.250013
 LP 29 unmatched irecvs 0 unmatched sends 0 Total sends 1066 receives 1066 collectives 0 delays 2398 wait alls 60 waits 0 send time 1336751.450998 wait 63630925.617630
 LP 30 unmatched irecvs 0 unmatched sends 0 Total sends 1042 receives 1042 collectives 0 delays 2356 wait alls 66 waits 0 send time 1303271.469921 wait 116979946.393221
 LP 36 unmatched irecvs 0 unmatched sends 0 Total sends 978 receives 978 collectives 0 delays 2222 wait alls 60 waits 0 send time 1264630.386888 wait 60750187.906107
 LP 37 unmatched irecvs 0 unmatched sends 0 Total sends 628 receives 628 collectives 0 delays 1522 wait alls 60 waits 0 send time 816591.158841 wait 159049402.123190
 LP 38 unmatched irecvs 0 unmatched sends 0 Total sends 756 receives 756 collectives 0 delays 1778 wait alls 60 waits 0 send time 962886.542565 wait 62828381.096530
 LP 39 unmatched irecvs 0 unmatched sends 0 Total sends 1224 receives 1224 collectives 0 delays 2714 wait alls 60 waits 0 send time 1516243.201509 wait 117957528.505235
 LP 45 unmatched irecvs 0 unmatched sends 0 Total sends 1058 receives 1058 collectives 0 delays 2382 wait alls 60 waits 0 send time 1320910.147259 wait 60146490.080305
 LP 46 unmatched irecvs 0 unmatched sends 0 Total sends 708 receives 708 collectives 0 delays 1672 wait alls 50 waits 0 send time 967492.464439 wait 158523458.934212
 LP 47 unmatched irecvs 0 unmatched sends 0 Total sends 1132 receives 1132 collectives 0 delays 2530 wait alls 60 waits 0 send time 1397302.027431 wait 74069915.010258
 LP 48 unmatched irecvs 0 unmatched sends 0 Total sends 740 receives 740 collectives 0 delays 1746 wait alls 60 waits 0 send time 928590.332869 wait 65232701.431432
 LP 54 unmatched irecvs 0 unmatched sends 0 Total sends 664 receives 664 collectives 0 delays 1594 wait alls 60 waits 0 send time 848973.949047 wait 36345958.118133
 LP 55 unmatched irecvs 0 unmatched sends 0 Total sends 674 receives 674 collectives 0 delays 1604 wait alls 50 waits 0 send time 938229.478640 wait 125516971.862382
 LP 56 unmatched irecvs 0 unmatched sends 0 Total sends 1028 receives 1028 collectives 0 delays 2328 wait alls 66 waits 0 send time 1267561.482412 wait 117911981.356183
 LP 57 unmatched irecvs 0 unmatched sends 0 Total sends 1236 receives 1236 collectives 0 delays 2744 wait alls 66 waits 0 send time 1504637.069302 wait 50347396.693650
 LP 63 unmatched irecvs 0 unmatched sends 0 Total sends 878 receives 878 collectives 0 delays 2022 wait alls 60 waits 0 send time 1146823.708011 wait 123363217.866624
 LP 64 unmatched irecvs 0 unmatched sends 0 Total sends 690 receives 690 collectives 0 delays 1646 wait alls 60 waits 0 send time 879885.796478 wait 227372017.567977
 LP 65 unmatched irecvs 0 unmatched sends 0 Total sends 646 receives 646 collectives 0 delays 1558 wait alls 60 waits 0 send time 774507.757742 wait 71956122.051773
 LP 66 unmatched irecvs 0 unmatched sends 0 Total sends 768 receives 768 collectives 0 delays 1798 wait alls 56 waits 0 send time 950385.223462 wait 158583089.307583
 LP 72 unmatched irecvs 0 unmatched sends 0 Total sends 670 receives 670 collectives 0 delays 1606 wait alls 60 waits 0 send time 849275.876803 wait 121573968.425720
 LP 73 unmatched irecvs 0 unmatched sends 0 Total sends 794 receives 794 collectives 0 delays 1854 wait alls 60 waits 0 send time 967002.342169 wait 226746422.904084
 LP 74 unmatched irecvs 0 unmatched sends 0 Total sends 676 receives 676 collectives 0 delays 1618 wait alls 60 waits 0 send time 853497.663715 wait 73897714.895026
 LP 75 unmatched irecvs 0 unmatched sends 0 Total sends 516 receives 516 collectives 0 delays 1294 wait alls 56 waits 0 send time 641329.482212 wait 184735895.207342
 LP 81 unmatched irecvs 0 unmatched sends 0 Total sends 650 receives 650 collectives 0 delays 1562 wait alls 56 waits 0 send time 858544.351669 wait 223511975.760137
 LP 82 unmatched irecvs 0 unmatched sends 0 Total sends 1008 receives 1008 collectives 0 delays 2282 wait alls 60 waits 0 send time 1287474.151241 wait 74911528.900648
 LP 83 unmatched irecvs 0 unmatched sends 0 Total sends 886 receives 886 collectives 0 delays 2038 wait alls 60 waits 0 send time 1173980.655245 wait 186291322.476267
 LP 84 unmatched irecvs 0 unmatched sends 0 Total sends 726 receives 726 collectives 0 delays 1708 wait alls 50 waits 0 send time 1006582.941256 wait 180308323.805768
 LP 90 unmatched irecvs 0 unmatched sends 0 Total sends 1080 receives 1080 collectives 0 delays 2432 wait alls 66 waits 0 send time 1358841.625623 wait 114358120.575814
 LP 91 unmatched irecvs 0 unmatched sends 0 Total sends 804 receives 804 collectives 0 delays 1874 wait alls 60 waits 0 send time 1009827.313515 wait 225479612.621467
 LP 92 unmatched irecvs 0 unmatched sends 0 Total sends 996 receives 996 collectives 0 delays 2258 wait alls 60 waits 0 send time 1292993.186195 wait 159080981.001880
 LP 93 unmatched irecvs 0 unmatched sends 0 Total sends 1044 receives 1044 collectives 0 delays 2344 wait alls 50 waits 0 send time 1409568.727912 wait 123803484.427334
 LP 99 unmatched irecvs 0 unmatched sends 0 Total sends 1010 receives 1010 collectives 0 delays 2276 wait alls 50 waits 0 send time 1392107.978241 wait 72580169.316250
 LP 100 unmatched irecvs 0 unmatched sends 0 Total sends 1952 receives 1952 collectives 0 delays 4182 wait alls 72 waits 0 send time 2346398.054515 wait 223514663.875979
 LP 101 unmatched irecvs 0 unmatched sends 0 Total sends 998 receives 998 collectives 0 delays 2252 wait alls 50 waits 0 send time 1362987.743583 wait 178904814.966390
 LP 102 unmatched irecvs 0 unmatched sends 0 Total sends 1016 receives 1016 collectives 0 delays 2298 wait alls 60 waits 0 send time 1296163.307307 wait 159821204.649910
 LP 108 unmatched irecvs 0 unmatched sends 0 Total sends 858 receives 858 collectives 0 delays 1982 wait alls 60 waits 0 send time 1098585.436320 wait 61204099.545769
 LP 109 unmatched irecvs 0 unmatched sends 0 Total sends 1852 receives 1852 collectives 0 delays 3980 wait alls 70 waits 0 send time 2231130.888345 wait 239282149.488297
 LP 110 unmatched irecvs 0 unmatched sends 0 Total sends 1036 receives 1036 collectives 0 delays 2328 wait alls 50 waits 0 send time 1417221.694278 wait 231406965.247936
 LP 111 unmatched irecvs 0 unmatched sends 0 Total sends 1096 receives 1096 collectives 0 delays 2448 wait alls 50 waits 0 send time 1469905.820924 wait 242108030.842040
 LP 117 unmatched irecvs 0 unmatched sends 0 Total sends 1508 receives 1508 collectives 0 delays 3282 wait alls 60 waits 0 send time 1893400.232530 wait 227244561.724871
 LP 118 unmatched irecvs 0 unmatched sends 0 Total sends 704 receives 704 collectives 0 delays 1664 wait alls 50 waits 0 send time 987026.752725 wait 182374233.768544
 LP 119 unmatched irecvs 0 unmatched sends 0 Total sends 708 receives 708 collectives 0 delays 1672 wait alls 50 waits 0 send time 980978.812301 wait 13967711.361794
 LP 120 unmatched irecvs 0 unmatched sends 0 Total sends 1344 receives 1344 collectives 0 delays 2954 wait alls 60 waits 0 send time 1720179.012817 wait 35015531.040038
 LP 126 unmatched irecvs 0 unmatched sends 0 Total sends 1614 receives 1614 collectives 0 delays 3494 wait alls 60 waits 0 send time 2023971.208713 wait 182946647.373766
 LP 127 unmatched irecvs 0 unmatched sends 0 Total sends 1642 receives 1642 collectives 0 delays 3550 wait alls 60 waits 0 send time 2015590.811251 wait 184289237.361690
 LP 128 unmatched irecvs 0 unmatched sends 0 Total sends 1346 receives 1346 collectives 0 delays 2958 wait alls 60 waits 0 send time 1700357.502209 wait 243228804.840325
 LP 129 unmatched irecvs 0 unmatched sends 0 Total sends 1214 receives 1214 collectives 0 delays 2694 wait alls 60 waits 0 send time 1518689.918331 wait 179953424.617609
 LP 135 unmatched irecvs 0 unmatched sends 0 Total sends 926 receives 926 collectives 0 delays 2118 wait alls 60 waits 0 send time 1246349.675652 wait 237034450.909029
 LP 136 unmatched irecvs 0 unmatched sends 0 Total sends 954 receives 954 collectives 0 delays 2164 wait alls 50 waits 0 send time 1352503.447149 wait 235505287.971080
 LP 137 unmatched irecvs 0 unmatched sends 0 Total sends 1460 receives 1460 collectives 0 delays 3196 wait alls 70 waits 0 send time 1852326.853503 wait 236632839.250267
 LP 138 unmatched irecvs 0 unmatched sends 0 Total sends 1066 receives 1066 collectives 0 delays 2388 wait alls 50 waits 0 send time 1409283.182707 wait 235555608.684319
 LP 144 unmatched irecvs 0 unmatched sends 0 Total sends 1752 receives 1752 collectives 0 delays 3776 wait alls 66 waits 0 send time 2130807.570266 wait 225930646.267494
 LP 145 unmatched irecvs 0 unmatched sends 0 Total sends 1130 receives 1130 collectives 0 delays 2532 wait alls 66 waits 0 send time 1388729.942234 wait 242848402.532029
 LP 146 unmatched irecvs 0 unmatched sends 0 Total sends 718 receives 718 collectives 0 delays 1698 wait alls 56 waits 0 send time 930406.156660 wait 229788144.347419
 LP 147 unmatched irecvs 0 unmatched sends 0 Total sends 670 receives 670 collectives 0 delays 1596 wait alls 50 waits 0 send time 945853.904861 wait 234202698.028066
 LP 153 unmatched irecvs 0 unmatched sends 0 Total sends 1562 receives 1562 collectives 0 delays 3396 wait alls 66 waits 0 send time 1893068.247829 wait 44977957.049517
 LP 154 unmatched irecvs 0 unmatched sends 0 Total sends 780 receives 780 collectives 0 delays 1826 wait alls 60 waits 0 send time 1038846.624604 wait 228435021.601487
 LP 155 unmatched irecvs 0 unmatched sends 0 Total sends 884 receives 884 collectives 0 delays 2034 wait alls 60 waits 0 send time 1153261.695121 wait 242093904.982951
 LP 156 unmatched irecvs 0 unmatched sends 0 Total sends 704 receives 704 collectives 0 delays 1674 wait alls 60 waits 0 send time 883647.081994 wait 226714793.456252
 LP 162 unmatched irecvs 0 unmatched sends 0 Total sends 848 receives 848 collectives 0 delays 1968 wait alls 66 waits 0 send time 1056655.404335 wait 157175170.273683
 LP 163 unmatched irecvs 0 unmatched sends 0 Total sends 1212 receives 1212 collectives 0 delays 2696 wait alls 66 waits 0 send time 1473940.846421 wait 28336559.684429
 LP 164 unmatched irecvs 0 unmatched sends 0 Total sends 1194 receives 1194 collectives 0 delays 2660 wait alls 66 waits 0 send time 1440356.469666 wait 151749253.578270
 LP 165 unmatched irecvs 0 unmatched sends 0 Total sends 682 receives 682 collectives 0 delays 1620 wait alls 50 waits 0 send time 955091.139227 wait 161779088.065750
 LP 171 unmatched irecvs 0 unmatched sends 0 Total sends 944 receives 944 collectives 0 delays 2154 wait alls 60 waits 0 send time 1221491.539740 wait 43063209.056568
 LP 172 unmatched irecvs 0 unmatched sends 0 Total sends 602 receives 602 collectives 0 delays 1470 wait alls 60 waits 0 send time 784025.269904 wait 233804447.033664
 LP 173 unmatched irecvs 0 unmatched sends 0 Total sends 942 receives 942 collectives 0 delays 2150 wait alls 60 waits 0 send time 1229456.029328 wait 50172193.310056
 LP 174 unmatched irecvs 0 unmatched sends 0 Total sends 968 receives 968 collectives 0 delays 2192 wait alls 50 waits 0 send time 1319939.372680 wait 227463054.419840
 LP 180 unmatched irecvs 0 unmatched sends 0 Total sends 1024 receives 1024 collectives 0 delays 2304 wait alls 50 waits 0 send time 1400669.257240 wait 55674515.977710
 LP 181 unmatched irecvs 0 unmatched sends 0 Total sends 1454 receives 1454 collectives 0 delays 3174 wait alls 60 waits 0 send time 1816148.078225 wait 181507834.768630
 LP 182 unmatched irecvs 0 unmatched sends 0 Total sends 1544 receives 1544 collectives 0 delays 3354 wait alls 60 waits 0 send time 1911634.255754 wait 224604363.142339
 LP 183 unmatched irecvs 0 unmatched sends 0 Total sends 1078 receives 1078 collectives 0 delays 2422 wait alls 60 waits 0 send time 1309799.522605 wait 230384066.827760
 LP 189 unmatched irecvs 0 unmatched sends 0 Total sends 1054 receives 1054 collectives 0 delays 2374 wait alls 60 waits 0 send time 1362222.775894 wait 189425317.992194
 LP 190 unmatched irecvs 0 unmatched sends 0 Total sends 1702 receives 1702 collectives 0 delays 3670 wait alls 60 waits 0 send time 2070140.487704 wait 117181727.419880
 LP 191 unmatched irecvs 0 unmatched sends 0 Total sends 1096 receives 1096 collectives 0 delays 2448 wait alls 50 waits 0 send time 1452532.883894 wait 224114053.684710
 LP 192 unmatched irecvs 0 unmatched sends 0 Total sends 1548 receives 1548 collectives 0 delays 3362 wait alls 60 waits 0 send time 1914415.247092 wait 227210177.590556
 LP 198 unmatched irecvs 0 unmatched sends 0 Total sends 1014 receives 1014 collectives 0 delays 2284 wait alls 50 waits 0 send time 1393252.678999 wait 220299486.476730
 LP 199 unmatched irecvs 0 unmatched sends 0 Total sends 1390 receives 1390 collectives 0 delays 3056 wait alls 70 waits 0 send time 1669083.321960 wait 241003019.512698
 LP 200 unmatched irecvs 0 unmatched sends 0 Total sends 1628 receives 1628 collectives 0 delays 3534 wait alls 72 waits 0 send time 1969004.464915 wait 185460021.241456
 LP 201 unmatched irecvs 0 unmatched sends 0 Total sends 1666 receives 1666 collectives 0 delays 3598 wait alls 60 waits 0 send time 2080485.562014 wait 227932490.141174
 LP 207 unmatched irecvs 0 unmatched sends 0 Total sends 1722 receives 1722 collectives 0 delays 3710 wait alls 60 waits 0 send time 2139705.407085 wait 176512745.363789
 LP 208 unmatched irecvs 0 unmatched sends 0 Total sends 1458 receives 1458 collectives 0 delays 3182 wait alls 60 waits 0 send time 1843616.685628 wait 189486438.128715
 LP 209 unmatched irecvs 0 unmatched sends 0 Total sends 1538 receives 1538 collectives 0 delays 3342 wait alls 60 waits 0 send time 1918725.054495 wait 239318145.981069
 LP 210 unmatched irecvs 0 unmatched sends 0 Total sends 740 receives 740 collectives 0 delays 1736 wait alls 50 waits 0 send time 1016436.312454 wait 231850048.656625
 LP 216 unmatched irecvs 0 unmatched sends 0 Total sends 992 receives 992 collectives 0 delays 2250 wait alls 60 waits 0 send time 1267154.230789 wait 177495309.095000
 LP 217 unmatched irecvs 0 unmatched sends 0 Total sends 1456 receives 1456 collectives 0 delays 3178 wait alls 60 waits 0 send time 1821569.537089 wait 116792071.752879
 LP 218 unmatched irecvs 0 unmatched sends 0 Total sends 1050 receives 1050 collectives 0 delays 2356 wait alls 50 waits 0 send time 1436108.782684 wait 37291272.684969
 LP 219 unmatched irecvs 0 unmatched sends 0 Total sends 1584 receives 1584 collectives 0 delays 3434 wait alls 60 waits 0 send time 1973944.999668 wait 49904082.837897
 LP 225 unmatched irecvs 0 unmatched sends 0 Total sends 1012 receives 1012 collectives 0 delays 2280 wait alls 50 waits 0 send time 1388698.228261 wait 126694818.180600
 LP 226 unmatched irecvs 0 unmatched sends 0 Total sends 1028 receives 1028 collectives 0 delays 2322 wait alls 60 waits 0 send time 1320794.752760 wait 118546753.671267
 LP 227 unmatched irecvs 0 unmatched sends 0 Total sends 822 receives 822 collectives 0 delays 1910 wait alls 60 waits 0 send time 1043424.735314 wait 163767703.897466
 LP 228 unmatched irecvs 0 unmatched sends 0 Total sends 724 receives 724 collectives 0 delays 1704 wait alls 50 waits 0 send time 1019108.346654 wait 120293854.572289
 LP 234 unmatched irecvs 0 unmatched sends 0 Total sends 1170 receives 1170 collectives 0 delays 2606 wait alls 60 waits 0 send time 1477524.275769 wait 163838276.502195
 LP 235 unmatched irecvs 0 unmatched sends 0 Total sends 1080 receives 1080 collectives 0 delays 2426 wait alls 60 waits 0 send time 1362518.229356 wait 72469903.266244
 LP 236 unmatched irecvs 0 unmatched sends 0 Total sends 1034 receives 1034 collectives 0 delays 2340 wait alls 66 waits 0 send time 1305745.383152 wait 181265556.555207
 LP 237 unmatched irecvs 0 unmatched sends 0 Total sends 736 receives 736 collectives 0 delays 1738 wait alls 60 waits 0 send time 930078.205976 wait 124599280.836448
 LP 243 unmatched irecvs 0 unmatched sends 0 Total sends 748 receives 748 collectives 0 delays 1762 wait alls 60 waits 0 send time 944942.837639 wait 226684255.961120
 LP 244 unmatched irecvs 0 unmatched sends 0 Total sends 1266 receives 1266 collectives 0 delays 2798 wait alls 60 waits 0 send time 1559772.421147 wait 224068310.261899
 LP 245 unmatched irecvs 0 unmatched sends 0 Total sends 882 receives 882 collectives 0 delays 2030 wait alls 60 waits 0 send time 1171132.628482 wait 59646838.696376
 LP 246 unmatched irecvs 0 unmatched sends 0 Total sends 884 receives 884 collectives 0 delays 2034 wait alls 60 waits 0 send time 1169541.450742 wait 230349262.080623
 LP 252 unmatched irecvs 0 unmatched sends 0 Total sends 1132 receives 1132 collectives 0 delays 2530 wait alls 60 waits 0 send time 1419571.729277 wait 117445327.827391
 LP 253 unmatched irecvs 0 unmatched sends 0 Total sends 496 receives 496 collectives 0 delays 1248 wait alls 50 waits 0 send time 697422.312509 wait 51882411.210824
 LP 254 unmatched irecvs 0 unmatched sends 0 Total sends 668 receives 668 collectives 0 delays 1592 wait alls 50 waits 0 send time 897448.488961 wait 220915271.187779
 LP 255 unmatched irecvs 0 unmatched sends 0 Total sends 1482 receives 1482 collectives 0 delays 3230 wait alls 60 waits 0 send time 1843559.665634 wait 125682220.057065
 LP 261 unmatched irecvs 0 unmatched sends 0 Total sends 1732 receives 1732 collectives 0 delays 3730 wait alls 60 waits 0 send time 2055488.948681 wait 171337339.221986
 LP 262 unmatched irecvs 0 unmatched sends 0 Total sends 2020 receives 2020 collectives 0 delays 4316 wait alls 70 waits 0 send time 2390286.865081 wait 179397186.992181
 LP 263 unmatched irecvs 0 unmatched sends 0 Total sends 1038 receives 1038 collectives 0 delays 2332 wait alls 50 waits 0 send time 1400244.520539 wait 78380897.412120
 LP 264 unmatched irecvs 0 unmatched sends 0 Total sends 854 receives 854 collectives 0 delays 1974 wait alls 60 waits 0 send time 1130986.389140 wait 185598053.656403
 LP 270 unmatched irecvs 0 unmatched sends 0 Total sends 1318 receives 1318 collectives 0 delays 2912 wait alls 70 waits 0 send time 1605993.188225 wait 183441365.822489
 LP 271 unmatched irecvs 0 unmatched sends 0 Total sends 1494 receives 1494 collectives 0 delays 3254 wait alls 60 waits 0 send time 1853253.940284 wait 237731108.339606
 LP 272 unmatched irecvs 0 unmatched sends 0 Total sends 1114 receives 1114 collectives 0 delays 2484 wait alls 50 waits 0 send time 1506073.121325 wait 187989091.856423
 LP 273 unmatched irecvs 0 unmatched sends 0 Total sends 1504 receives 1504 collectives 0 delays 3274 wait alls 60 waits 0 send time 1925987.933597 wait 186961890.194399
 LP 279 unmatched irecvs 0 unmatched sends 0 Total sends 1284 receives 1284 collectives 0 delays 2834 wait alls 60 waits 0 send time 1699702.868855 wait 41587294.618191
 LP 280 unmatched irecvs 0 unmatched sends 0 Total sends 962 receives 962 collectives 0 delays 2190 wait alls 60 waits 0 send time 1268313.802547 wait 56179800.029834
 LP 281 unmatched irecvs 0 unmatched sends 0 Total sends 772 receives 772 collectives 0 delays 1800 wait alls 50 waits 0 send time 1045713.702199 wait 242366453.174915
 LP 282 unmatched irecvs 0 unmatched sends 0 Total sends 1444 receives 1444 collectives 0 delays 3154 wait alls 60 waits 0 send time 1827816.130192 wait 239838693.289420
 LP 288 unmatched irecvs 0 unmatched sends 0 Total sends 1086 receives 1086 collectives 0 delays 2428 wait alls 50 waits 0 send time 1495648.511026 wait 242109938.601502
 LP 289 unmatched irecvs 0 unmatched sends 0 Total sends 1474 receives 1474 collectives 0 delays 3214 wait alls 60 waits 0 send time 1866654.582048 wait 239148605.136481
 LP 290 unmatched irecvs 0 unmatched sends 0 Total sends 1666 receives 1666 collectives 0 delays 3598 wait alls 60 waits 0 send time 2074126.473026 wait 231683272.233514
 LP 291 unmatched irecvs 0 unmatched sends 0 Total sends 1270 receives 1270 collectives 0 delays 2806 wait alls 60 waits 0 send time 1570096.533302 wait 229041214.090651
 LP 297 unmatched irecvs 0 unmatched sends 0 Total sends 1168 receives 1168 collectives 0 delays 2602 wait alls 60 waits 0 send time 1478174.260760 wait 228006413.133874
 LP 298 unmatched irecvs 0 unmatched sends 0 Total sends 1854 receives 1854 collectives 0 delays 3984 wait alls 70 waits 0 send time 2235385.680993 wait 54407837.495879
 LP 299 unmatched irecvs 0 unmatched sends 0 Total sends 1530 receives 1530 collectives 0 delays 3326 wait alls 60 waits 0 send time 1918623.175208 wait 43435109.217533
 LP 300 unmatched irecvs 0 unmatched sends 0 Total sends 1688 receives 1688 collectives 0 delays 3642 wait alls 60 waits 0 send time 2073585.683253 wait 180443386.817762
 LP 306 unmatched irecvs 0 unmatched sends 0 Total sends 1006 receives 1006 collectives 0 delays 2268 wait alls 50 waits 0 send time 1369678.066230 wait 234827300.894513
 LP 307 unmatched irecvs 0 unmatched sends 0 Total sends 1182 receives 1182 collectives 0 delays 2630 wait alls 60 waits 0 send time 1453872.587879 wait 225128096.688900
 LP 308 unmatched irecvs 0 unmatched sends 0 Total sends 774 receives 774 collectives 0 delays 1814 wait alls 60 waits 0 send time 986412.539371 wait 238588346.901442
 LP 309 unmatched irecvs 0 unmatched sends 0 Total sends 1064 receives 1064 collectives 0 delays 2394 wait alls 60 waits 0 send time 1368119.135462 wait 230947991.556343
 LP 315 unmatched irecvs 0 unmatched sends 0 Total sends 676 receives 676 collectives 0 delays 1608 wait alls 50 waits 0 send time 925767.213918 wait 229314849.631709
 LP 316 unmatched irecvs 0 unmatched sends 0 Total sends 1082 receives 1082 collectives 0 delays 2430 wait alls 60 waits 0 send time 1359575.569396 wait 241533502.538067
 LP 317 unmatched irecvs 0 unmatched sends 0 Total sends 958 receives 958 collectives 0 delays 2182 wait alls 60 waits 0 send time 1216238.557433 wait 235232023.804240
 LP 318 unmatched irecvs 0 unmatched sends 0 Total sends 544 receives 544 collectives 0 delays 1350 wait alls 56 waits 0 send time 728295.820487 wait 241818960.780068
 LP 324 unmatched irecvs 0 unmatched sends 0 Total sends 942 receives 942 collectives 0 delays 2156 wait alls 66 waits 0 send time 1140182.089447 wait 47864710.306853
 LP 325 unmatched irecvs 0 unmatched sends 0 Total sends 754 receives 754 collectives 0 delays 1774 wait alls 60 waits 0 send time 1023713.128732 wait 239851624.351898
 LP 326 unmatched irecvs 0 unmatched sends 0 Total sends 1178 receives 1178 collectives 0 delays 2622 wait alls 60 waits 0 send time 1446818.706926 wait 227632766.612120
 LP 327 unmatched irecvs 0 unmatched sends 0 Total sends 682 receives 682 collectives 0 delays 1620 wait alls 50 waits 0 send time 948953.171935 wait 238425191.492895
 LP 333 unmatched irecvs 0 unmatched sends 0 Total sends 930 receives 930 collectives 0 delays 2126 wait alls 60 waits 0 send time 1197751.448529 wait 235173176.706286
 LP 334 unmatched irecvs 0 unmatched sends 0 Total sends 748 receives 748 collectives 0 delays 1762 wait alls 60 waits 0 send time 921654.263274 wait 227369786.169390
 LP 335 unmatched irecvs 0 unmatched sends 0 Total sends 904 receives 904 collectives 0 delays 2074 wait alls 60 waits 0 send time 1175455.369354 wait 239444069.426787
 LP 336 unmatched irecvs 0 unmatched sends 0 Total sends 1574 receives 1574 collectives 0 delays 3420 wait alls 66 waits 0 send time 1909375.763095 wait 240876821.702868
 LP 342 unmatched irecvs 0 unmatched sends 0 Total sends 1064 receives 1064 collectives 0 delays 2384 wait alls 50 waits 0 send time 1431078.085745 wait 239582472.113191
 LP 343 unmatched irecvs 0 unmatched sends 0 Total sends 1022 receives 1022 collectives 0 delays 2300 wait alls 50 waits 0 send time 1392639.599693 wait 54306997.881879
 LP 344 unmatched irecvs 0 unmatched sends 0 Total sends 1392 receives 1392 collectives 0 delays 3050 wait alls 60 waits 0 send time 1762868.547964 wait 237377876.573738
 LP 345 unmatched irecvs 0 unmatched sends 0 Total sends 732 receives 732 collectives 0 delays 1720 wait alls 50 waits 0 send time 1009808.307314 wait 229509575.923357
 LP 351 unmatched irecvs 0 unmatched sends 0 Total sends 1096 receives 1096 collectives 0 delays 2458 wait alls 60 waits 0 send time 1395358.000737 wait 239002588.009873
 LP 352 unmatched irecvs 0 unmatched sends 0 Total sends 1082 receives 1082 collectives 0 delays 2420 wait alls 50 waits 0 send time 1468959.213742 wait 238612525.266457
 LP 353 unmatched irecvs 0 unmatched sends 0 Total sends 1540 receives 1540 collectives 0 delays 3346 wait alls 60 waits 0 send time 1910908.953035 wait 236151256.616066
 LP 354 unmatched irecvs 0 unmatched sends 0 Total sends 1070 receives 1070 collectives 0 delays 2396 wait alls 50 waits 0 send time 1460741.955959 wait 244232512.426837
 LP 360 unmatched irecvs 0 unmatched sends 0 Total sends 1070 receives 1070 collectives 0 delays 2396 wait alls 50 waits 0 send time 1439992.987388 wait 232491468.639140
 LP 361 unmatched irecvs 0 unmatched sends 0 Total sends 1002 receives 1002 collectives 0 delays 2270 wait alls 60 waits 0 send time 1268899.691438 wait 234563596.485585
 LP 362 unmatched irecvs 0 unmatched sends 0 Total sends 736 receives 736 collectives 0 delays 1728 wait alls 50 waits 0 send time 1047246.540606 wait 226312200.080464
 LP 363 unmatched irecvs 0 unmatched sends 0 Total sends 972 receives 972 collectives 0 delays 2200 wait alls 50 waits 0 send time 1346636.251382 wait 234003752.808560
 LP 369 unmatched irecvs 0 unmatched sends 0 Total sends 1762 receives 1762 collectives 0 delays 3790 wait alls 60 waits 0 send time 2158949.638434 wait 240479668.726626
 LP 370 unmatched irecvs 0 unmatched sends 0 Total sends 1684 receives 1684 collectives 0 delays 3634 wait alls 60 waits 0 send time 2045615.450660 wait 233456691.814360
 LP 371 unmatched irecvs 0 unmatched sends 0 Total sends 1410 receives 1410 collectives 0 delays 3086 wait alls 60 waits 0 send time 1793352.499286 wait 239029658.233294
 LP 372 unmatched irecvs 0 unmatched sends 0 Total sends 948 receives 948 collectives 0 delays 2162 wait alls 60 waits 0 send time 1232901.018381 wait 236571751.867198
 LP 378 unmatched irecvs 0 unmatched sends 0 Total sends 954 receives 954 collectives 0 delays 2174 wait alls 60 waits 0 send time 1252010.787490 wait 28872020.499920
 LP 379 unmatched irecvs 0 unmatched sends 0 Total sends 1270 receives 1270 collectives 0 delays 2806 wait alls 60 waits 0 send time 1627886.750878 wait 223550083.236020
 LP 380 unmatched irecvs 0 unmatched sends 0 Total sends 1506 receives 1506 collectives 0 delays 3278 wait alls 60 waits 0 send time 1877065.936016 wait 44484845.249219
 LP 381 unmatched irecvs 0 unmatched sends 0 Total sends 980 receives 980 collectives 0 delays 2216 wait alls 50 waits 0 send time 1347203.572137 wait 37974722.570828
 LP 387 unmatched irecvs 0 unmatched sends 0 Total sends 1702 receives 1702 collectives 0 delays 3676 wait alls 66 waits 0 send time 2071769.522280 wait 120930600.863587
 LP 388 unmatched irecvs 0 unmatched sends 0 Total sends 846 receives 846 collectives 0 delays 1958 wait alls 60 waits 0 send time 1121676.269522 wait 236390395.372274
 LP 389 unmatched irecvs 0 unmatched sends 0 Total sends 644 receives 644 collectives 0 delays 1554 wait alls 60 waits 0 send time 849074.614689 wait 239583967.466475
 LP 390 unmatched irecvs 0 unmatched sends 0 Total sends 898 receives 898 collectives 0 delays 2062 wait alls 60 waits 0 send time 1194248.256042 wait 77551255.308577
 LP 396 unmatched irecvs 0 unmatched sends 0 Total sends 1394 receives 1394 collectives 0 delays 3066 wait alls 72 waits 0 send time 1680447.482774 wait 181954755.003045
 LP 397 unmatched irecvs 0 unmatched sends 0 Total sends 1448 receives 1448 collectives 0 delays 3168 wait alls 66 waits 0 send time 1730700.253292 wait 179452043.558964
 LP 398 unmatched irecvs 0 unmatched sends 0 Total sends 1090 receives 1090 collectives 0 delays 2446 wait alls 60 waits 0 send time 1362520.825888 wait 184339756.060921
 LP 399 unmatched irecvs 0 unmatched sends 0 Total sends 964 receives 964 collectives 0 delays 2194 wait alls 60 waits 0 send time 1163915.966380 wait 226976312.408118
 LP 405 unmatched irecvs 0 unmatched sends 0 Total sends 654 receives 654 collectives 0 delays 1574 wait alls 60 waits 0 send time 785996.187805 wait 235819906.841401
 LP 406 unmatched irecvs 0 unmatched sends 0 Total sends 654 receives 654 collectives 0 delays 1570 wait alls 56 waits 0 send time 842502.145936 wait 182925092.226000
 LP 407 unmatched irecvs 0 unmatched sends 0 Total sends 734 receives 734 collectives 0 delays 1730 wait alls 56 waits 0 send time 924714.183380 wait 188442299.926165
 LP 408 unmatched irecvs 0 unmatched sends 0 Total sends 1032 receives 1032 collectives 0 delays 2340 wait alls 70 waits 0 send time 1235125.968845 wait 161082373.801387
 LP 414 unmatched irecvs 0 unmatched sends 0 Total sends 778 receives 778 collectives 0 delays 1828 wait alls 66 waits 0 send time 972096.350793 wait 231361833.270194
 LP 415 unmatched irecvs 0 unmatched sends 0 Total sends 726 receives 726 collectives 0 delays 1724 wait alls 66 waits 0 send time 840037.942789 wait 161424164.007532
 LP 416 unmatched irecvs 0 unmatched sends 0 Total sends 784 receives 784 collectives 0 delays 1836 wait alls 62 waits 0 send time 991389.414077 wait 182640364.070923
 LP 417 unmatched irecvs 0 unmatched sends 0 Total sends 1380 receives 1380 collectives 0 delays 3036 wait alls 70 waits 0 send time 1648899.118545 wait 233117396.568241
 LP 423 unmatched irecvs 0 unmatched sends 0 Total sends 690 receives 690 collectives 0 delays 1636 wait alls 50 waits 0 send time 973248.559611 wait 162699136.436757
 LP 424 unmatched irecvs 0 unmatched sends 0 Total sends 682 receives 682 collectives 0 delays 1620 wait alls 50 waits 0 send time 955682.437545 wait 230925787.404967
 LP 425 unmatched irecvs 0 unmatched sends 0 Total sends 1206 receives 1206 collectives 0 delays 2678 wait alls 60 waits 0 send time 1487774.469349 wait 228363345.677702
 LP 426 unmatched irecvs 0 unmatched sends 0 Total sends 728 receives 728 collectives 0 delays 1722 wait alls 60 waits 0 send time 912367.151058 wait 229756244.240830
 LP 432 unmatched irecvs 0 unmatched sends 0 Total sends 722 receives 722 collectives 0 delays 1710 wait alls 60 waits 0 send time 911450.421544 wait 40130943.189459
 LP 433 unmatched irecvs 0 unmatched sends 0 Total sends 1064 receives 1064 collectives 0 delays 2394 wait alls 60 waits 0 send time 1361151.609776 wait 74152714.250620
 LP 434 unmatched irecvs 0 unmatched sends 0 Total sends 972 receives 972 collectives 0 delays 2210 wait alls 60 waits 0 send time 1252078.728616 wait 227835027.875529
 LP 435 unmatched irecvs 0 unmatched sends 0 Total sends 1156 receives 1156 collectives 0 delays 2578 wait alls 60 waits 0 send time 1447239.062585 wait 228657241.661385
 LP 441 unmatched irecvs 0 unmatched sends 0 Total sends 788 receives 788 collectives 0 delays 1842 wait alls 60 waits 0 send time 1062313.289631 wait 237410160.582239
 LP 442 unmatched irecvs 0 unmatched sends 0 Total sends 884 receives 884 collectives 0 delays 2034 wait alls 60 waits 0 send time 1089451.411746 wait 121658383.355642
 LP 443 unmatched irecvs 0 unmatched sends 0 Total sends 950 receives 950 collectives 0 delays 2166 wait alls 60 waits 0 send time 1155609.993471 wait 164773488.395418
 LP 444 unmatched irecvs 0 unmatched sends 0 Total sends 1086 receives 1086 collectives 0 delays 2438 wait alls 60 waits 0 send time 1362374.449758 wait 161352193.949322
 LP 450 unmatched irecvs 0 unmatched sends 0 Total sends 1188 receives 1188 collectives 0 delays 2652 wait alls 70 waits 0 send time 1477398.179498 wait 120814719.110398
 LP 451 unmatched irecvs 0 unmatched sends 0 Total sends 1102 receives 1102 collectives 0 delays 2470 wait alls 60 waits 0 send time 1351933.540047 wait 120456427.367757
 LP 452 unmatched irecvs 0 unmatched sends 0 Total sends 1612 receives 1612 collectives 0 delays 3502 wait alls 72 waits 0 send time 1880752.859434 wait 226689385.494537
 LP 453 unmatched irecvs 0 unmatched sends 0 Total sends 694 receives 694 collectives 0 delays 1654 wait alls 60 waits 0 send time 878190.189927 wait 119678148.347902
 LP 459 unmatched irecvs 0 unmatched sends 0 Total sends 934 receives 934 collectives 0 delays 2140 wait alls 66 waits 0 send time 1122283.617665 wait 124098407.652423
 LP 460 unmatched irecvs 0 unmatched sends 0 Total sends 896 receives 896 collectives 0 delays 2058 wait alls 60 waits 0 send time 1150567.877600 wait 187384767.985001
 LP 461 unmatched irecvs 0 unmatched sends 0 Total sends 1174 receives 1174 collectives 0 delays 2614 wait alls 60 waits 0 send time 1423410.518199 wait 183216359.475160
 LP 462 unmatched irecvs 0 unmatched sends 0 Total sends 804 receives 804 collectives 0 delays 1874 wait alls 60 waits 0 send time 1074091.749002 wait 77431811.628409
 LP 468 unmatched irecvs 0 unmatched sends 0 Total sends 970 receives 970 collectives 0 delays 2206 wait alls 60 waits 0 send time 1211009.781027 wait 117800903.942812
 LP 469 unmatched irecvs 0 unmatched sends 0 Total sends 606 receives 606 collectives 0 delays 1478 wait alls 60 waits 0 send time 809266.469347 wait 231986628.903106
 LP 470 unmatched irecvs 0 unmatched sends 0 Total sends 464 receives 464 collectives 0 delays 1190 wait alls 56 waits 0 send time 584690.327968 wait 162310616.991503
 LP 471 unmatched irecvs 0 unmatched sends 0 Total sends 736 receives 736 collectives 0 delays 1744 wait alls 66 waits 0 send time 940218.685630 wait 186896575.091691
 LP 477 unmatched irecvs 0 unmatched sends 0 Total sends 866 receives 866 collectives 0 delays 1998 wait alls 60 waits 0 send time 1009355.984707 wait 48347050.325136
 LP 478 unmatched irecvs 0 unmatched sends 0 Total sends 776 receives 776 collectives 0 delays 1818 wait alls 60 waits 0 send time 934262.453814 wait 237456164.552619
 LP 479 unmatched irecvs 0 unmatched sends 0 Total sends 576 receives 576 collectives 0 delays 1418 wait alls 60 waits 0 send time 751647.854004 wait 235302036.020718
 LP 480 unmatched irecvs 0 unmatched sends 0 Total sends 468 receives 468 collectives 0 delays 1198 wait alls 56 waits 0 send time 572385.313953 wait 226989333.384371
	: Running Time = 10.3426 seconds

TW Library Statistics:
	Total Events Processed                                11540841
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                  11540841
	Event Rate (events/sec)                              1115858.1
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        469247
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)     24.7646

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 136925280 recvd 136925280 
 max runtime 832428174.181527 ns avg runtime 831592158.607247 
 max comm time 244256041.446027 avg comm time 165598826.375765 
 max send time 2390286.865081 avg send time 1321096.933746 
 max recv time 1242807411.964691 avg recv time 491966904.633888 
 max wait time 244232512.426837 avg wait time 165575909.276246 
LP-IO: writing output to amg-216-trace-11281-1495530749/
LP-IO: data files:
   amg-216-trace-11281-1495530749/dragonfly-router-traffic
   amg-216-trace-11281-1495530749/dragonfly-router-stats
   amg-216-trace-11281-1495530749/dragonfly-msg-stats
   amg-216-trace-11281-1495530749/model-net-category-all
   amg-216-trace-11281-1495530749/model-net-category-test
   amg-216-trace-11281-1495530749/mpi-replay-stats
 Average number of hops traversed 3.033524 average chunk latency 1.896739 us maximum chunk latency 17.016149 us avg message size 610.249207 bytes finished messages 224376 finished chunks 702636 

 ADAPTIVE ROUTING STATS: 574306 chunks routed minimally 128330 chunks routed non-minimally completed packets 702636 

 Total packets generated 449894 finished 449894
 
 #+END_EXAMPLE
On peut voir ici les informations suivante :
- La version de CODES
- La configuration réseau
- La configuration des LPs
- Le nombre d'event par catégorie
- Par LP
  - unmatched irecvs
  - unmatched sends
  - Total :
    - sends
    - receives
    - collectives
    - delays
    - wait alls
    - waits
    - send time
    - wait time
- Le running time
- Le nombre d'evenement rejoué
- Des statistiques sur ces évennements
  - Abort
  - Roll Back
  - Tie in Queue
  - Efficacité
  - Remote Events
  - Net event
- La taille en mémoire des différents évènements
- La tailles des différentes structures
- Des informations sur les clock cycle
- Sur MPI
- Le total des bytes send et receive
- Le temps max d'execution et le temps moyens
- Le temps max de communication et le temps moyens
- Le temps max d'envoie et le temps moyens
- Le temps max de réception et le temps moyens
- Le temps max d'attente et le temps moyens
- Les fichiers de sortie
- Le nombre moyens de noeuds traversé
- La latence moyenne sur les liens
- La latence maximum
- La taille moyenne des messages
- Finished messages
- Finished chunks
- Statistiques de routage
  - nombre de paquet routé sur le plus petit chemin
  - nombre de paquet routé sur un autre chemin
  - nombre de paquet routé
- Nombre total de paquet généré et transmit
****** Fichier router-traffic
Une matrice au format suivant :
=Format <LP ID> <Group ID> <Router ID> <Link traffic per router port(s)># Router ports in the order: 8 local channels, 4 global channels=
****** Fichier router-stats
Une matrice au format suivant :
=Format <LP ID> <Group ID> <Router ID> <Busy time per router port(s)># Router ports in the order: 8 local channels, 4 global channels=
****** Fichier msg-stats
Une matrice au format suivant :
=Format <LP id> <Terminal ID> <Total Data Size> <Avg packet latency> <#
Flits/Packets finished> <Avg hops> <Busy Time>=
****** Fichier model-net-category-all
Une matrice au format suivant :
=<lp:_> <send_count:_> <send_bytes:_> <send_time:_> <recv_count:_> <recv_bytes:_> <recv_time:_>  <max_event_size:_>=
****** Fichier model-net-category-test
Une matrice avec le même format que le précédent, mais avec seulement
les LPs qui ont des valeurs
****** Fichier mpi-replay-stats
Une matrice au format suivant (sans les LPs à 0):
=Format <LP ID> <Terminal ID> <Total sends> <Total Recvs> <Bytes sent> <Bytes recvd> <Send time> <Comm. time> <Compute time>=
****** Autres fichiers
Il y a également d'autres fichiers construits, des fichiers binaires,
un fichier ross.csv et
- un fichier mpi-op-logs : 
#+BEGIN_EXAMPLE
 (565112103.590082) APP 0 MPI ISEND SOURCE 169 DEST 163 BYTES 12800 
 (565112103.590082) APP 0 MPI ISEND SOURCE 169 DEST 163 BYTES 12800 
 (565112103.590082) APP 0 MPI ISEND SOURCE 169 DEST 163 BYTES 12800 
 (565112103.590082) APP 0 MPI ISEND SOURCE 169 DEST 163 BYTES 12800 
 (565113384.250048) APP 0 MPI ISEND SOURCE 169 DEST 168 BYTES 12800 
 (565114608.130126) APP 0 MPI ISEND SOURCE 169 DEST 170 BYTES 12800 
 (565115641.097869) APP 0 MPI ISEND SOURCE 169 DEST 175 BYTES 12800 
 (565117495.340234) APP 0 MPI ISEND SOURCE 169 DEST 205 BYTES 12800 
 MPI WAITALL POSTED AT 169 
 (565121250.633979) APP 0 MPI ISEND SOURCE 88 DEST 52 BYTES 12800 
 (565124976.751223) APP 0 MPI ISEND SOURCE 88 DEST 82 BYTES 12800 
 (565124976.751223) APP 0 MPI ISEND SOURCE 88 DEST 82 BYTES 12800 
 (565125782.389739) APP 0 MPI ISEND SOURCE 88 DEST 87 BYTES 12800 
 (565126597.931201) APP 0 MPI ISEND SOURCE 88 DEST 89 BYTES 12800 
 (565127489.796056) APP 0 MPI ISEND SOURCE 88 DEST 94 BYTES 12800 
 (565128529.564635) APP 0 MPI ISEND SOURCE 88 DEST 124 BYTES 12800 
 MPI WAITALL POSTED AT 88 
 (565768994.209241) APP 0 MPI ISEND SOURCE 109 DEST 73 BYTES 12800 
 (565768994.209241) APP 0 MPI ISEND SOURCE 109 DEST 73 BYTES 12800 
 (565768994.209241) APP 0 MPI ISEND SOURCE 109 DEST 73 BYTES 12800 
 (565768994.209241) APP 0 MPI ISEND SOURCE 109 DEST 73 BYTES 12800 
 (565775857.400695) APP 0 MPI ISEND SOURCE 109 DEST 108 BYTES 12800 
 (565775857.400695) APP 0 MPI ISEND SOURCE 109 DEST 108 BYTES 12800 
 (565775857.400695) APP 0 MPI ISEND SOURCE 109 DEST 108 BYTES 12800 
 (565775857.400695) APP 0 MPI ISEND SOURCE 109 DEST 108 BYTES 12800 
 (565775857.400695) APP 0 MPI ISEND SOURCE 109 DEST 108 BYTES 12800 
 (565775857.400695) APP 0 MPI ISEND SOURCE 109 DEST 108 BYTES 12800 
 (565775857.400695) APP 0 MPI ISEND SOURCE 109 DEST 108 BYTES 12800 
 (565777437.072278) APP 0 MPI ISEND SOURCE 109 DEST 110 BYTES 12800 
 (565778350.760466) APP 0 MPI ISEND SOURCE 109 DEST 115 BYTES 12800 
 (565779914.347081) APP 0 MPI ISEND SOURCE 109 DEST 145 BYTES 12800 
 MPI WAITALL POSTED AT 109 
 (566189481.432541) APP 0 MPI ISEND SOURCE 190 DEST 154 BYTES 12800 
 (566189481.432541) APP 0 MPI ISEND SOURCE 190 DEST 154 BYTES 12800 
 (566189481.432541) APP 0 MPI ISEND SOURCE 190 DEST 154 BYTES 12800 
 (566189481.432541) APP 0 MPI ISEND SOURCE 190 DEST 154 BYTES 12800 
 (566190662.578188) APP 0 MPI ISEND SOURCE 190 DEST 184 BYTES 12800 
 (566191727.689164) APP 0 MPI ISEND SOURCE 190 DEST 189 BYTES 12800 
 (566191727.689164) APP 0 MPI ISEND SOURCE 190 DEST 189 BYTES 12800 
 (566191727.689164) APP 0 MPI ISEND SOURCE 190 DEST 189 BYTES 12800 
 (566191727.689164) APP 0 MPI ISEND SOURCE 190 DEST 189 BYTES 12800 
 (566192897.681644) APP 0 MPI ISEND SOURCE 190 DEST 191 BYTES 12800 
 (566194087.707728) APP 0 MPI ISEND SOURCE 190 DEST 196 BYTES 12800 
 MPI WAITALL POSTED AT 190 
 (570252614.987613) APP 0 MPI ISEND SOURCE 215 DEST 179 BYTES 12800 
 MPI WAITALL POSTED AT 190 
 (570252614.987613) APP 0 MPI ISEND SOURCE 215 DEST 179 BYTES 12800 
 MPI WAITALL POSTED AT 190 
 MPI WAITALL POSTED AT 190 
 (570252614.987613) APP 0 MPI ISEND SOURCE 215 DEST 179 BYTES 12800 
 (570252614.987613) APP 0 MPI ISEND SOURCE 215 DEST 179 BYTES 12800 
 (570252614.987613) APP 0 MPI ISEND SOURCE 215 DEST 179 BYTES 12800 
 MPI WAITALL POSTED AT 190 
 (570255059.175846) APP 0 MPI ISEND SOURCE 215 DEST 209 BYTES 12800 
 (570255059.175846) APP 0 MPI ISEND SOURCE 215 DEST 209 BYTES 12800 
 (570255059.175846) APP 0 MPI ISEND SOURCE 215 DEST 209 BYTES 12800 
 (570255059.175846) APP 0 MPI ISEND SOURCE 215 DEST 209 BYTES 12800 
 (570255059.175846) APP 0 MPI ISEND SOURCE 215 DEST 209 BYTES 12800 
 (570255059.175846) APP 0 MPI ISEND SOURCE 215 DEST 209 BYTES 12800 
 (570255059.175846) APP 0 MPI ISEND SOURCE 215 DEST 209 BYTES 12800 
 (570255059.175846) APP 0 MPI ISEND SOURCE 215 DEST 209 BYTES 12800 
 (570256325.174722) APP 0 MPI ISEND SOURCE 215 DEST 214 BYTES 12800 
 (570256325.174722) APP 0 MPI ISEND SOURCE 215 DEST 214 BYTES 12800 
 MPI WAITALL POSTED AT 215 
 (602993813.779419) APP 0 MPI ISEND SOURCE 182 DEST 146 BYTES 12800 
 (602997576.107047) APP 0 MPI ISEND SOURCE 182 DEST 181 BYTES 12800 
 (602997576.107047) APP 0 MPI ISEND SOURCE 182 DEST 181 BYTES 12800 
 (602998576.154434) APP 0 MPI ISEND SOURCE 182 DEST 183 BYTES 12800 
 (602999444.898646) APP 0 MPI ISEND SOURCE 182 DEST 188 BYTES 12800 
 (602999444.898646) APP 0 MPI ISEND SOURCE 182 DEST 188 BYTES 12800 
 MPI WAITALL POSTED AT 182 
 (603032941.477410) APP 0 MPI ISEND SOURCE 93 DEST 57 BYTES 12800 
 (603036666.366648) APP 0 MPI ISEND SOURCE 93 DEST 87 BYTES 12800 
 (603036666.366648) APP 0 MPI ISEND SOURCE 93 DEST 87 BYTES 12800 
 (603036666.366648) APP 0 MPI ISEND SOURCE 93 DEST 87 BYTES 12800 
 (603036666.366648) APP 0 MPI ISEND SOURCE 93 DEST 87 BYTES 12800 
 (603036666.366648) APP 0 MPI ISEND SOURCE 93 DEST 87 BYTES 12800 
 (603037544.002996) APP 0 MPI ISEND SOURCE 93 DEST 92 BYTES 12800 
 (603038249.630337) APP 0 MPI ISEND SOURCE 93 DEST 94 BYTES 12800 
 (603039132.711902) APP 0 MPI ISEND SOURCE 93 DEST 99 BYTES 12800 
 (603040565.615053) APP 0 MPI ISEND SOURCE 93 DEST 129 BYTES 12800 
 MPI WAITALL POSTED AT 93 
 (603052101.583046) APP 0 MPI ISEND SOURCE 205 DEST 169 BYTES 12800 
 (603052101.583046) APP 0 MPI ISEND SOURCE 205 DEST 169 BYTES 12800 
 (603052101.583046) APP 0 MPI ISEND SOURCE 205 DEST 169 BYTES 12800 
 (603052101.583046) APP 0 MPI ISEND SOURCE 205 DEST 169 BYTES 12800 
 (603055373.396152) APP 0 MPI ISEND SOURCE 205 DEST 199 BYTES 12800 
 (603055373.396152) APP 0 MPI ISEND SOURCE 205 DEST 199 BYTES 12800 
 (603055373.396152) APP 0 MPI ISEND SOURCE 205 DEST 199 BYTES 12800 
 (603055373.396152) APP 0 MPI ISEND SOURCE 205 DEST 199 BYTES 12800 
 (603056338.719917) APP 0 MPI ISEND SOURCE 205 DEST 204 BYTES 12800 
 (603057108.105422) APP 0 MPI ISEND SOURCE 205 DEST 206 BYTES 12800 
 (603057651.239669) APP 0 MPI ISEND SOURCE 205 DEST 211 BYTES 12800 
 MPI WAITALL POSTED AT 205 
 (603075434.020147) APP 0 MPI ISEND SOURCE 123 DEST 87 BYTES 12800 
 (603079172.339792) APP 0 MPI ISEND SOURCE 123 DEST 117 BYTES 12800 
 (603079172.339792) APP 0 MPI ISEND SOURCE 123 DEST 117 BYTES 12800 
 (603079172.339792) APP 0 MPI ISEND SOURCE 123 DEST 117 BYTES 12800 
 (603079172.339792) APP 0 MPI ISEND SOURCE 123 DEST 117 BYTES 12800 
 (603080251.715119) APP 0 MPI ISEND SOURCE 123 DEST 122 BYTES 12800 
 (603081504.733552) APP 0 MPI ISEND SOURCE 123 DEST 124 BYTES 12800 
 (603082645.815794) APP 0 MPI ISEND SOURCE 123 DEST 129 BYTES 12800 
 (603083478.424889) APP 0 MPI ISEND SOURCE 123 DEST 159 BYTES 12800 
 MPI WAITALL POSTED AT 123 
 (603092421.016605) APP 0 MPI ISEND SOURCE 119 DEST 83 BYTES 12800 
 (603092421.016605) APP 0 MPI ISEND SOURCE 119 DEST 83 BYTES 12800 
 (603092421.016605) APP 0 MPI ISEND SOURCE 119 DEST 83 BYTES 12800 
 (603092421.016605) APP 0 MPI ISEND SOURCE 119 DEST 83 BYTES 12800 
 (603092421.016605) APP 0 MPI ISEND SOURCE 119 DEST 83 BYTES 12800 
 (603096903.612525) APP 0 MPI ISEND SOURCE 119 DEST 113 BYTES 12800 
 (603096903.612525) APP 0 MPI ISEND SOURCE 119 DEST 113 BYTES 12800 
 (603098329.830321) APP 0 MPI ISEND SOURCE 119 DEST 118 BYTES 12800 
 (603096903.612525) APP 0 MPI ISEND SOURCE 119 DEST 113 BYTES 12800 
 (603098329.830321) APP 0 MPI ISEND SOURCE 119 DEST 118 BYTES 12800 
 (603099340.338005) APP 0 MPI ISEND SOURCE 119 DEST 125 BYTES 12800 
 (603100594.162374) APP 0 MPI ISEND SOURCE 119 DEST 155 BYTES 12800 
 MPI WAITALL POSTED AT 119 
 (603121523.222171) APP 0 MPI ISEND SOURCE 57 DEST 21 BYTES 12800 
(603109599.990422) APP ID 0 MPI WAITALL COMPLETED AT 155 
 (603121523.222171) APP 0 MPI ISEND SOURCE 57 DEST 21 BYTES 12800 
#+END_EXAMPLE
- un fichier dragonfly-router-sampling.meta
#+BEGIN_EXAMPLE
Router sample struct format: 
router_id (tw_lpid) 
busy time for each of the 16 links (double) 
link traffic for each of the 16 links (int64_t) 
sample end time (double) forward events per sample 
reverse events per sample 

Ordering of links 
8 local (router-router same group) channels 
4 global (router-router remote group) channels 
4 terminal channels
#+END_EXAMPLE

***** Simulation parallèle

****** Lancement de la simulation
#+begin_src sh :results output :exports both
mpirun -np 4 model-net-mpi-replay --sync=3 --num_net_traces=216 --workload_file=dumpi-2014.03.03.14.55.23- --workload_type=dumpi 
--lp-io-dir=amg-216-trace --lp-io-use-suffix=1 
-- /home/chevamax/Documents/Stage_LIG_2017/logiciels/CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf 
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Fri May 26 11:40:21 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            152065
	Network events                                           50000
	Total events                                            202064

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #59058: simulation 100% complete, max event queue size 1055 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***


 Incomplete wait operation Rank 0 
 Waiting reqs:  37  36  32  31  33  29  30  38  39  40  41  42  35  34 
 LP 0 unmatched irecvs 2 unmatched sends 0 Total sends 301 receives 299 collectives 0 delays 773 wait alls 39 waits 0 send time 415382.902444 wait 12386455.732546
 Incomplete wait operation Rank 1 
 Waiting reqs:  35  77  76  75  74  78  71  72  73  82  103  83  85  86  79  80  81  66  68  70  67  84 
 LP 1 unmatched irecvs 3 unmatched sends 0 Total sends 512 receives 513 collectives 0 delays 1200 wait alls 41 waits 0 send time 669003.187043 wait 62644270.504859
 Incomplete wait operation Rank 2 
 Waiting reqs:  19  42  35  34  43  21  32  8  31  7  33  12  9  11  20  41  40  39  38  37  13  36 
 LP 2 unmatched irecvs 2 unmatched sends 0 Total sends 381 receives 380 collectives 0 delays 936 wait alls 41 waits 0 send time 534302.917440 wait 70464938.150157
 Incomplete wait operation Rank 3 
 Waiting reqs:  72  71  68  65  64  60  57  56  61  88  75  76  77  78  67  79  62  74  73  58  70  69 
 LP 3 unmatched irecvs 1 unmatched sends 5 Total sends 520 receives 521 collectives 0 delays 1216 wait alls 41 waits 0 send time 693646.773594 wait 155606202.792946
 Incomplete wait operation Rank 4 
 Waiting reqs:  26  24  27  28  29  19  20  21  22  25  4  5  35  37 
 LP 9 unmatched irecvs 2 unmatched sends 5 Total sends 337 receives 334 collectives 0 delays 844 wait alls 37 waits 0 send time 485671.265945 wait 64598516.918089
 Incomplete wait operation Rank 5 
 Waiting reqs:  25  7  8  24  26  27  10  12  13  4  6  9  11  5  3  14  2  15  16  17  18  23  22  21  20  19 
 LP 10 unmatched irecvs 8 unmatched sends 0 Total sends 279 receives 272 collectives 0 delays 732 wait alls 41 waits 0 send time 375450.126926 wait 64064137.111587
 Incomplete wait operation Rank 6 
 Waiting reqs:  32  31  30  33  39  22  35  27  28  23  37  21  17  16  2  34  26  24  36  25  29  38 
 LP 11 unmatched irecvs 4 unmatched sends 0 Total sends 425 receives 424 collectives 0 delays 1024 wait alls 41 waits 0 send time 582328.159706 wait 115487347.173781
 Incomplete wait operation Rank 7 
 Waiting reqs:  22  48  49  3  51  12  9  8  16  17  53  41  15  19  14  26  21  7  2  47  46  45  44  43  42  40  50  23  25  27  24  52  13  11 
 LP 12 unmatched irecvs 6 unmatched sends 0 Total sends 542 receives 539 collectives 0 delays 1256 wait alls 41 waits 0 send time 759006.237219 wait 68579280.841909
 Incomplete wait operation Rank 8 
 Waiting reqs:  57  75  66  65  64  63  61  50  104  45  46  77  78  79  34  80  81  44  71  48  72  73  74  67  32  56  31  47  49  55  68  69  70  62 
 LP 18 unmatched irecvs 4 unmatched sends 0 Total sends 746 receives 745 collectives 0 delays 1666 wait alls 41 waits 0 send time 974743.004925 wait 66811610.103506
 Incomplete wait operation Rank 9 
 Waiting reqs:  19  55  14  32  20  26  13  27  4  44  25  11  24  29  37  36  30  35  40  38  39  41  8  43  45  10  21  22  9  5  3  2  28  23 
 LP 19 unmatched irecvs 2 unmatched sends 5 Total sends 647 receives 649 collectives 0 delays 1471 wait alls 41 waits 0 send time 859779.413906 wait 64953702.309415
 Incomplete wait operation Rank 10 
 Waiting reqs:  64  66  53  71  51  52  63  61  60  59  56  55  72  80  50  79  69  77  75  73  58 
 LP 20 unmatched irecvs 2 unmatched sends 6 Total sends 634 receives 629 collectives 0 delays 1441 wait alls 42 waits 0 send time 835784.586705 wait 58102177.763404
 Incomplete wait operation Rank 11 
 Waiting reqs:  45  21  33  42  41  46  44  47  32  40  31  30  29  35  34  43  15  11  39  38  37  36  28  27  26  25  24  23  22  20  2  8  17  16  14  13  10  18  5  3  4  6  19  12  9  7 
 LP 21 unmatched irecvs 17 unmatched sends 0 Total sends 392 receives 393 collectives 0 delays 966 wait alls 41 waits 0 send time 553408.600339 wait 115947781.505652
 Incomplete wait operation Rank 12 
 Waiting reqs:  64  63  60  85  84  59  58  82  55  83  81  86  87  57  80  65  56  72  91  79  61  62 
 LP 27 unmatched irecvs 6 unmatched sends 0 Total sends 610 receives 613 collectives 0 delays 1401 wait alls 44 waits 0 send time 776765.388509 wait 71804845.177689
 Incomplete wait operation Rank 13 
 Waiting reqs:  10  21  20  8  12  11  30  5  13  52  36  51  61  3  18  9  2  24  34  35  22  4  27  16  17  6  23  29  58  19  14  15  28  53 
 LP 28 unmatched irecvs 9 unmatched sends 0 Total sends 646 receives 648 collectives 0 delays 1469 wait alls 41 waits 0 send time 859766.150066 wait 117032449.612402
 Incomplete wait operation Rank 14 
 Waiting reqs:  46  44  43  42  5  49  40  52  4  68  31  33  32  24  14  47  48  55  15  58  59  60  61  62  53  57  28  34  38  39  54  56  41  16 
 LP 29 unmatched irecvs 6 unmatched sends 0 Total sends 674 receives 670 collectives 0 delays 1519 wait alls 41 waits 0 send time 895164.039717 wait 62125353.080480
 Incomplete wait operation Rank 15 
 Waiting reqs:  71  72  73  74  75  25  21  69  76  11  67  66  77  79  80  34  36  78  29  18  81  64  7  19  28  27  2  17  70  65  12  68  37  35 
 LP 30 unmatched irecvs 3 unmatched sends 8 Total sends 666 receives 662 collectives 0 delays 1506 wait alls 44 waits 0 send time 886355.675252 wait 115700421.294609
 Incomplete wait operation Rank 16 
 Waiting reqs:  52  33  5  10  38  21  34  31  12  41  37  7  22  27  13  53  4  28  23  39  54  8  14  40 
 LP 36 unmatched irecvs 4 unmatched sends 7 Total sends 647 receives 646 collectives 0 delays 1471 wait alls 42 waits 0 send time 887565.417378 wait 59486143.112935
 Incomplete wait operation Rank 17 
 Waiting reqs:  33  13  14  20  43  6  21  18  19  22  15  4  32  9  27  17  12  3  16  11  8  7  23  24  25  26  28  29  30  31  34  35  42  41  40  39  2  38  37  36  5  10 
 LP 37 unmatched irecvs 15 unmatched sends 0 Total sends 441 receives 441 collectives 0 delays 1065 wait alls 43 waits 0 send time 593064.067962 wait 157226339.247885
 Incomplete wait operation Rank 18 
 Waiting reqs:  44  43  19  45  40  42  47  48  46  50  53  65  49  51  39  36  25  35  38  37  52  41 
 LP 38 unmatched irecvs 6 unmatched sends 0 Total sends 471 receives 472 collectives 0 delays 1118 wait alls 41 waits 0 send time 643643.291487 wait 60931704.733800
 Incomplete wait operation Rank 19 
 Waiting reqs:  38  76  70  54  26  39  5  69  68  6  41  62  72  56  65  25  16  96  63  73  77  27  71  74  55  75  64  22  29  78  79  66  67  97 
 LP 39 unmatched irecvs 9 unmatched sends 0 Total sends 761 receives 758 collectives 0 delays 1694 wait alls 41 waits 0 send time 992110.774375 wait 116695804.483203
 Incomplete wait operation Rank 20 
 Waiting reqs:  49  48  37  36  35  47  46  45  44  60  50  42  38  58  5  24  74  51  13  75  32  52  73  34  61  62  18  63  64  65  4  33  59  43 
 LP 45 unmatched irecvs 6 unmatched sends 0 Total sends 670 receives 669 collectives 0 delays 1514 wait alls 41 waits 0 send time 893340.094200 wait 58760389.581386
 Incomplete wait operation Rank 21 
 Waiting reqs:  53  54  55  56  44  36  46  47  48  50  57  21  43  40  37  58  38  34  60  5  39  35  42  41  61  33  32  25  3  52  45  59  51  49 
 LP 46 unmatched irecvs 3 unmatched sends 3 Total sends 499 receives 499 collectives 0 delays 1168 wait alls 36 waits 0 send time 715597.483388 wait 156637382.439133
 Incomplete wait operation Rank 22 
 Waiting reqs:  100  97  96  78  110  86  79  109  91  108  87  94  107  106  93  83  82  105  102  80  103  81 
 LP 47 unmatched irecvs 4 unmatched sends 5 Total sends 720 receives 721 collectives 0 delays 1619 wait alls 42 waits 0 send time 940313.504340 wait 72892677.439708
 Incomplete wait operation Rank 23 
 Waiting reqs:  42  61  27  32  23  35  39  4  38  43  28  34  44  37  20  10  41  19  36  21  40  31  45  46  47  48  50  51  52  53  54  55  56  57  58  59  60  62  63  25  33  69  30  68  67  29  66  65  8  24  64  26 
 LP 48 unmatched irecvs 21 unmatched sends 0 Total sends 503 receives 499 collectives 0 delays 1185 wait alls 43 waits 0 send time 659201.318235 wait 63484386.958145
 Incomplete wait operation Rank 24 
 Waiting reqs:  41  21  34  49  44  35  46  32  53  43  45  47  48  42  50  40  39  33  52  37  36  38 
 LP 54 unmatched irecvs 6 unmatched sends 0 Total sends 426 receives 425 collectives 0 delays 1026 wait alls 41 waits 0 send time 575737.027317 wait 34128691.427088
 Incomplete wait operation Rank 25 
 Waiting reqs:  47  48  41  42  10  39  45  23  53  37  29  22  46  38  8  51  50  40  36  35  34  20  25  44  33  32  49  11  12  13  43  24  2  52 
 LP 55 unmatched irecvs 8 unmatched sends 0 Total sends 479 receives 480 collectives 0 delays 1129 wait alls 36 waits 0 send time 700441.949505 wait 123204683.627420
 Incomplete wait operation Rank 26 
 Waiting reqs:  21  6  37  16  47  38  30  22  62  19  20  26  29  35  63  25  10  41  17  33  18  27  34  9  2  3  36  40  32  12  28  39  13  8 
 LP 56 unmatched irecvs 5 unmatched sends 0 Total sends 659 receives 659 collectives 0 delays 1496 wait alls 44 waits 0 send time 864755.152622 wait 116247011.435699
 Incomplete wait operation Rank 27 
 Waiting reqs:  61  88  87  8  60  49  56  71  59  42  31  44  62  64  65  86  70  34  26  72  30  45  43  46  63  67  66  57  24  68  69  58  10  16 
 LP 57 unmatched irecvs 2 unmatched sends 6 Total sends 760 receives 763 collectives 0 delays 1701 wait alls 44 waits 0 send time 983122.295314 wait 48826799.803765
 Incomplete wait operation Rank 28 
 Waiting reqs:  21  11  41  26  36  7  9  40  34  39  27  43  24  22  37  2  44  13  38  8  6  31  15  42 
 LP 63 unmatched irecvs 5 unmatched sends 3 Total sends 597 receives 594 collectives 0 delays 1369 wait alls 42 waits 0 send time 821898.447180 wait 121472923.753388
 Incomplete wait operation Rank 29 
 Waiting reqs:  45  39  35  40  36  65  50  42  46  38  34  33  53  54  49  41  52  43  51  37  44  47  31  48  55  56  69  68  67  66  64  63  62  61  60  59  58  57 
 LP 64 unmatched irecvs 16 unmatched sends 0 Total sends 465 receives 466 collectives 0 delays 1114 wait alls 43 waits 0 send time 621104.662620 wait 225477881.411099
 Incomplete wait operation Rank 30 
 Waiting reqs:  64  63  58  68  60  57  67  56  55  69  65  66  61  59 
 LP 65 unmatched irecvs 4 unmatched sends 0 Total sends 386 receives 387 collectives 0 delays 948 wait alls 41 waits 0 send time 492731.802641 wait 70131012.117029
 Incomplete wait operation Rank 31 
 Waiting reqs:  35  34  31  29  33  73  74  43  40  42  28  26  25  30  41  44  45  36  37  38  39  32 
 LP 66 unmatched irecvs 5 unmatched sends 0 Total sends 482 receives 484 collectives 0 delays 1139 wait alls 39 waits 0 send time 631793.668395 wait 156747900.040103
 Incomplete wait operation Rank 32 
 Waiting reqs:  21  7  35  20  18  2  27  30  8  28  15  29  31  32  33  24  25  19  26  34 
 LP 72 unmatched irecvs 2 unmatched sends 0 Total sends 430 receives 428 collectives 0 delays 1033 wait alls 41 waits 0 send time 582770.279080 wait 119410836.262377
 Incomplete wait operation Rank 33 
 Waiting reqs:  62  69  67  66  65  63  37  61  60  64  74  75  59  7  76  77  78  79  70  71  33  68 
 LP 73 unmatched irecvs 1 unmatched sends 3 Total sends 495 receives 498 collectives 0 delays 1168 wait alls 41 waits 0 send time 636794.193487 wait 224821681.637685
 Incomplete wait operation Rank 34 
 Waiting reqs:  41  18  10  6  17  22  21  2  7  26  25  11  4  5  23  24  47  15 
 LP 74 unmatched irecvs 2 unmatched sends 3 Total sends 447 receives 445 collectives 0 delays 1070 wait alls 42 waits 0 send time 597357.322811 wait 72027746.364132
 Incomplete wait operation Rank 35 
 Waiting reqs:  32  42  31  41  39  40  36  33  37  38  30  43  34  35  44  45  46  47  48  49  50  51  59  58  57  56  55  54  53  52 
 LP 75 unmatched irecvs 13 unmatched sends 0 Total sends 344 receives 344 collectives 0 delays 869 wait alls 41 waits 0 send time 449193.280498 wait 182209806.518895
 Incomplete wait operation Rank 36 
 Waiting reqs:  24  41  11  15  2  21  6  17  22  23  16  18  8  14  20  12  5  4  13  42  3  39 
 LP 81 unmatched irecvs 4 unmatched sends 0 Total sends 424 receives 424 collectives 0 delays 1021 wait alls 39 waits 0 send time 594081.530264 wait 221352444.202042
 Incomplete wait operation Rank 37 
 Waiting reqs:  62  82  81  74  27  71  102  78  72  73  75  67  103  77  60  76  56  83  59  57  68  39  64  80  66  58  70  61  95  63  79  55 
 LP 82 unmatched irecvs 5 unmatched sends 0 Total sends 643 receives 645 collectives 0 delays 1463 wait alls 41 waits 0 send time 872389.783086 wait 72929674.924307
 Incomplete wait operation Rank 38 
 Waiting reqs:  16  18  22  7  60  53  23  38  21  52  51  67  10  50  62  54  49  69  9  20  19  64  61  63  66  65  56  59  14  58  57  55  68  39 
 LP 83 unmatched irecvs 4 unmatched sends 0 Total sends 587 receives 586 collectives 0 delays 1348 wait alls 41 waits 0 send time 823891.619611 wait 183556742.561434
 Incomplete wait operation Rank 39 
 Waiting reqs:  55  56  42  39  64  59  66  40  54  15  51  52  17  63  67  68  49  48  47  46  45  50  44  43  65  60  61  58  41  69  57  62  3  53 
 LP 84 unmatched irecvs 2 unmatched sends 9 Total sends 506 receives 507 collectives 0 delays 1183 wait alls 36 waits 0 send time 735645.442597 wait 178389885.636460
 Incomplete wait operation Rank 40 
 Waiting reqs:  52  67  70  10  57  74  16  59  58  71  5  63  17  73  60  50  48  54 
 LP 90 unmatched irecvs 3 unmatched sends 5 Total sends 689 receives 686 collectives 0 delays 1556 wait alls 45 waits 0 send time 913883.621259 wait 112878643.473401
 Incomplete wait operation Rank 41 
 Waiting reqs:  45  54  44  53  62  52  58  17  61  47  46  48  49  60  51  64  59  56  57  55  50  82  77  65  39  66  67  68  69  70  71  72  74  75  76  86  85  2  84  83  28  81  14  80  89  88  79  37  87  78 
 LP 91 unmatched irecvs 19 unmatched sends 0 Total sends 532 receives 535 collectives 0 delays 1250 wait alls 43 waits 0 send time 695408.583851 wait 223741327.604716
 Incomplete wait operation Rank 42 
 Waiting reqs:  39  38  30  50  55  49  54  48  47  46  45  64  4  62  61  3  52  5  40  53  37  41  51  74  80  56  42  43  59  44  57  60  58  35 
 LP 92 unmatched irecvs 2 unmatched sends 10 Total sends 622 receives 622 collectives 0 delays 1416 wait alls 40 waits 0 send time 854452.957987 wait 156297193.398113
 Incomplete wait operation Rank 43 
 Waiting reqs:  90  66  97  45  53  67  69  10  68  64  85  76  57  59  79  77  65  52  60  63  87  92  80  58  81  91  95  93  89  74  75  96  73  72  61  70  88  78  56  86  84  62  40  94  83  54  82  71  55  7 
 LP 93 unmatched irecvs 2 unmatched sends 15 Total sends 700 receives 704 collectives 0 delays 1571 wait alls 35 waits 0 send time 992501.570096 wait 121593931.712635
 Incomplete wait operation Rank 44 
 Waiting reqs:  79  59  62  89  54  55  56  78  63  84  90  71  69  70  87  61  85  67  50  51  52  57  60  74  75  58  77  73  86  91  53  83  80  81  82  88  65  66  76  64  72  68  47  43  44  46  39  45  48  49 
 LP 99 unmatched irecvs 1 unmatched sends 18 Total sends 683 receives 685 collectives 0 delays 1535 wait alls 35 waits 0 send time 987891.295208 wait 70577062.269733
 Incomplete wait operation Rank 45 
 Waiting reqs:  94  96  90  46  11  54  49  162  89  124  84  52  98  143  77  75  82  13  88  40  97  73  86  74  48  50  47  95  10  58  59  66  18  24  87  91  99  67  68  69  85  51  70  72  62  83  71  119  25  76  128  14 
 LP 100 unmatched irecvs 4 unmatched sends 7 Total sends 1187 receives 1194 collectives 0 delays 2562 wait alls 47 waits 0 send time 1503943.252478 wait 222360671.726050
 Incomplete wait operation Rank 46 
 Waiting reqs:  80  34  41  74  19  56  69  36  43  47  53  59  48  55  63  76  68  57  42  45  2  46  78  28  66  44 
 LP 101 unmatched irecvs 5 unmatched sends 6 Total sends 723 receives 722 collectives 0 delays 1618 wait alls 37 waits 0 send time 1024052.578291 wait 176985892.975780
 Incomplete wait operation Rank 47 
 Waiting reqs:  83  85  58  60  65  55  80  67  68  78  53  97  71  57  64  77  72  70  79  63  82  61  81  69  54  62  13  59  76  36  84  73  66  56  87  88  89  101  100  99  98  29  96  95  15  94  93  92  91  23  10  6  40  90 
 LP 102 unmatched irecvs 21 unmatched sends 0 Total sends 689 receives 688 collectives 0 delays 1560 wait alls 43 waits 0 send time 917020.521835 wait 157565164.695846
 Incomplete wait operation Rank 48 
 Waiting reqs:  57  45  16  11  24  54  26  3  32  25  23  34  7  20  60  36  19  31  56  17  18  22  8  9  37  55  58  4  6  28  33  53  15  52 
 LP 108 unmatched irecvs 4 unmatched sends 6 Total sends 557 receives 553 collectives 0 delays 1282 wait alls 40 waits 0 send time 756990.209168 wait 60194053.953313
 Incomplete wait operation Rank 49 
 Waiting reqs:  112  76  71  113  74  64  97  14  73  17  104  106  148  67  94  96  88  101  85  95  37  84  83  87  89  165  90  92  103  72  100  107  70  102  105  98  99  9  77  78  80  81  93  91  75  86  110  82  108  111  63  109 
 LP 109 unmatched irecvs 4 unmatched sends 9 Total sends 1108 receives 1110 collectives 0 delays 2395 wait alls 45 waits 0 send time 1399905.626459 wait 237349958.367240
 Incomplete wait operation Rank 50 
 Waiting reqs:  64  58  60  98  87  92  49  97  48  62  61  66  51  50  65  88  41  89  91  57  90  74  75  56  78  83  84  82  80  59  70  69  81  79  63  52  93  67  68  71  72  73  96  95  76  77  94  55  86  53  54  85 
 LP 110 unmatched irecvs 2 unmatched sends 15 Total sends 698 receives 704 collectives 0 delays 1569 wait alls 35 waits 0 send time 1009795.997359 wait 229455271.898413
 Incomplete wait operation Rank 51 
 Waiting reqs:  78  102  103  79  8  9  74  88  34  98  15  104  105  2  112  113  86  85  84  89  87  32  83  82  72  71  100  96  107  81  80  73  75  101  95  77  70  99  97  93  92  47  91  90  109  110  111  76  46  106  108  94 
 LP 111 unmatched irecvs 6 unmatched sends 8 Total sends 755 receives 758 collectives 0 delays 1683 wait alls 36 waits 0 send time 1055981.032040 wait 240316716.907090
 Incomplete wait operation Rank 52 
 Waiting reqs:  71  73  54  70  40  68  5  15  51  100  9  38  50  59  67  64  49  81  118  13  47  6  37  53  76  8  27  34  61  30  52  3  65  60  10 
 LP 117 unmatched irecvs 10 unmatched sends 7 Total sends 983 receives 992 collectives 0 delays 2153 wait alls 42 waits 0 send time 1280892.304312 wait 225938091.734472
 Incomplete wait operation Rank 53 
 Waiting reqs:  42  65  8  45  35  62  63  38  39  40  37  64  44  52  47  53 
 LP 118 unmatched irecvs 3 unmatched sends 7 Total sends 502 receives 505 collectives 0 delays 1180 wait alls 37 waits 0 send time 729833.401229 wait 180603053.149610
 Incomplete wait operation Rank 54 
 Waiting reqs:  51  52  53  38  39  3  57  23  56  40  34  14  58  54  41  46  44  33  22  37  45  42  47  26  16  18  50  27  49  59  43  55  48  36 
 LP 119 unmatched irecvs 5 unmatched sends 6 Total sends 483 receives 483 collectives 0 delays 1133 wait alls 35 waits 0 send time 703551.743481 wait 10817853.613181
 Incomplete wait operation Rank 55 
 Waiting reqs:  52  43  46  2  51  57  18  3  36  6  5  19  55  44  49  31  48  50  85  45  15  78  11  30  72  37  29  13  28  14  59  58  56  53  54  34  70  40  47  8  73  20  32  10  22  21  16  27  33  74  42  67 
 LP 120 unmatched irecvs 5 unmatched sends 9 Total sends 854 receives 850 collectives 0 delays 1876 wait alls 40 waits 0 send time 1157768.623220 wait 33429506.418997
 Incomplete wait operation Rank 56 
 Waiting reqs:  101  124  135  119  112  111  129  117  6  121  100  99  105  106  92  131  103  108  136  123  116  107  104  7  89  90  137  68  37  133  110  118  127  91  113  138  122  170  128  102  120  93  132  109  115  35  130  94  95  96  97  134 
 LP 126 unmatched irecvs 2 unmatched sends 15 Total sends 986 receives 989 collectives 0 delays 2147 wait alls 40 waits 0 send time 1305325.922029 wait 181760121.551250
 Incomplete wait operation Rank 57 
 Waiting reqs:  86  85  84  104  119  29  35  97  75  43  68  16  118  96  103  80  77  28  120  159  32  79  78  107  73  82  54  100  98  72  27  87  164  143  83  95  106  81  117  116  102  108  121  74  101  76  48  105  99  69  70  71 
 LP 127 unmatched irecvs 6 unmatched sends 7 Total sends 1027 receives 1035 collectives 0 delays 2237 wait alls 41 waits 0 send time 1338626.191166 wait 183518685.470464
 Incomplete wait operation Rank 58 
 Waiting reqs:  83  102  110  79  75  115  111  119  80  73  106  90  109  76  88  91  94  95  96  82  118  117  120  104 
 LP 128 unmatched irecvs 5 unmatched sends 4 Total sends 888 receives 884 collectives 0 delays 1950 wait alls 42 waits 0 send time 1170714.449134 wait 241918458.954235
 Incomplete wait operation Rank 59 
 Waiting reqs:  73  19  87  86  89  88  22  81  84  97  122  82  137  83  92  90  76  96  91  37  95  93  121  62  133  75  3  94 
 LP 129 unmatched irecvs 5 unmatched sends 5 Total sends 760 receives 768 collectives 0 delays 1706 wait alls 42 waits 0 send time 1010737.934148 wait 178848664.520125
 Incomplete wait operation Rank 60 
 Waiting reqs:  19  3  26  2  18  47  45  8  36  30  39  54  43  13  28  38  49  53  12  27  10  46  42  41  44  11  33  37  23  4  40  48  34  5 
 LP 135 unmatched irecvs 3 unmatched sends 6 Total sends 590 receives 593 collectives 0 delays 1355 wait alls 40 waits 0 send time 854564.977316 wait 235385902.224326
 Incomplete wait operation Rank 61 
 Waiting reqs:  11  27  41  66  40  39  80  33  44  50  59  8  45  73  65  79  57  35  56  54  55  52  37  67  60  71  2  13  4  21  49  53  74  58  61  62  47  68  78  76  75  77  51  64  63  31  72  3  70  69 
 LP 136 unmatched irecvs 3 unmatched sends 9 Total sends 650 receives 653 collectives 0 delays 1470 wait alls 35 waits 0 send time 966271.003273 wait 233886664.111022
 Incomplete wait operation Rank 62 
 Waiting reqs:  69  74  83  42  12  20  24  30  71  28  19  5  82  33  61  73  54  57  60  7  22  18  67  27  3  65  72  25  84  75  23  37  64  66  63  68  77  81  80  58  21  46  70  55  56  59  76  48  11  31 
 LP 137 unmatched irecvs 1 unmatched sends 15 Total sends 914 receives 910 collectives 0 delays 2001 wait alls 45 waits 0 send time 1233989.182825 wait 235447323.738768
 Incomplete wait operation Rank 63 
 Waiting reqs:  98  99  100  101  102  25  96  78  3  79  80  105  111  86  90  85  13  21  49  89  4  29  92  108  103  18  91  23  93  88  27  97  84  83  82  6  95  54  81  33  7  2  109  107  110  30  104  94  17  106  40  48 
 LP 138 unmatched irecvs 5 unmatched sends 8 Total sends 729 receives 738 collectives 0 delays 1637 wait alls 36 waits 0 send time 999751.116131 wait 233888539.551010
 Incomplete wait operation Rank 64 
 Waiting reqs:  5  62  67  103  107  83  40  53  32  89  102  79  78  68  106  86  73  105  77  82  64  92  76  74  41  12  108  141  94 
 LP 144 unmatched irecvs 9 unmatched sends 3 Total sends 1103 receives 1101 collectives 0 delays 2385 wait alls 45 waits 0 send time 1412194.849593 wait 225247572.573398
 Incomplete wait operation Rank 65 
 Waiting reqs:  10  42  11  52  26  53  54  62  74  28  24  65  70  68  61  58  55  64  56  57  67 
 LP 145 unmatched irecvs 2 unmatched sends 3 Total sends 719 receives 719 collectives 0 delays 1619 wait alls 45 waits 0 send time 929237.109041 wait 240901656.889668
 Incomplete wait operation Rank 66 
 Waiting reqs:  31  29  22  28  27  26  36  21  35  34  32  53  8  52  37  30  11  54  33  23  24  25 
 LP 146 unmatched irecvs 1 unmatched sends 4 Total sends 448 receives 445 collectives 0 delays 1063 wait alls 38 waits 0 send time 618644.442145 wait 227821121.612826
 Incomplete wait operation Rank 67 
 Waiting reqs:  36  7  5  3  39  32  40  41  46  42  43  44  20  6  27  37  19  23  4  33  49  30  38  31  34  35  48  21  18  29  13  47  45  25 
 LP 147 unmatched irecvs 1 unmatched sends 7 Total sends 464 receives 465 collectives 0 delays 1096 wait alls 35 waits 0 send time 688654.994738 wait 231998620.902572
 Incomplete wait operation Rank 68 
 Waiting reqs:  97  96  95  94  93  105  92  90  89  88  100  85  174  84  104  106  86  164  163  103  108  109  110  111  112  98  99  83  82  101  87  107  102  91 
 LP 153 unmatched irecvs 7 unmatched sends 0 Total sends 923 receives 924 collectives 0 delays 2025 wait alls 44 waits 0 send time 1175772.320346 wait 44119207.380957
 Incomplete wait operation Rank 69 
 Waiting reqs:  51  52  56  5  57  36  47  35  48  39  46  10  6  34  38  44  2  4  37  9  43  55  42  41  40  3  45  14  54  11  53  26 
 LP 154 unmatched irecvs 2 unmatched sends 3 Total sends 532 receives 531 collectives 0 delays 1238 wait alls 41 waits 0 send time 745011.565271 wait 226453981.358568
 Incomplete wait operation Rank 70 
 Waiting reqs:  31  40  42  27  44  24  23  12  14  37  36  63  4  51  50  25  7  49  30  48  47  43 
 LP 155 unmatched irecvs 5 unmatched sends 2 Total sends 598 receives 595 collectives 0 delays 1371 wait alls 42 waits 0 send time 819043.239576 wait 239825155.110212
 Incomplete wait operation Rank 71 
 Waiting reqs:  33  36  38  43  35  47  44  37  45  29  50  42  41  27 
 LP 156 unmatched irecvs 2 unmatched sends 2 Total sends 458 receives 456 collectives 0 delays 1092 wait alls 42 waits 0 send time 607409.531918 wait 225499402.248393
 Incomplete wait operation Rank 72 
 Waiting reqs:  39  35  14  15  33  17  6  8  10  41  37  12  13  51  9  40  34  36  29  42  38  28 
 LP 162 unmatched irecvs 6 unmatched sends 0 Total sends 519 receives 520 collectives 0 delays 1217 wait alls 44 waits 0 send time 688791.752902 wait 155734867.983061
 Incomplete wait operation Rank 73 
 Waiting reqs:  75  100  69  70  60  55  80  62  76  59  43  54  41  74  67  61  64  58  65  56  68  66  73  8  40  25  12  57  9  44  72  63  71  77 
 LP 163 unmatched irecvs 9 unmatched sends 0 Total sends 747 receives 745 collectives 0 delays 1670 wait alls 44 waits 0 send time 961916.126683 wait 26346768.509698
 Incomplete wait operation Rank 74 
 Waiting reqs:  15  18  59  27  46  2  9  17  24  23  78  54  45  55  50  25  4  34  41  85  6  66  42  29  35  12  73  40  49  3  51  48  52  38 
 LP 164 unmatched irecvs 6 unmatched sends 0 Total sends 744 receives 745 collectives 0 delays 1667 wait alls 44 waits 0 send time 950530.446562 wait 151277034.615951
 Incomplete wait operation Rank 75 
 Waiting reqs:  53  58  34  44  54  50  41  36  56  55  40  61  47  60  59  39  37  57  35  33  49  32  38  14  46  45  52  43  5  48  42  51 
 LP 165 unmatched irecvs 3 unmatched sends 6 Total sends 476 receives 475 collectives 0 delays 1121 wait alls 36 waits 0 send time 698547.291129 wait 158862306.955688
 Incomplete wait operation Rank 76 
 Waiting reqs:  64  62  80  63  59  58  51  50  47  45  46  44  61  42  52  41  67  66  48 
 LP 171 unmatched irecvs 1 unmatched sends 4 Total sends 632 receives 625 collectives 0 delays 1435 wait alls 42 waits 0 send time 855833.473701 wait 41156135.611026
 Incomplete wait operation Rank 77 
 Waiting reqs:  46  16  48  36  35  12  14  6  40  49  45  19  47  42  38  37  39  41  7  44  13  43  17  3  5  15  18  4  20  10  9  11  8  2  21  22  23  24  25  26  27  28  34  32  31  30  29  33 
 LP 172 unmatched irecvs 18 unmatched sends 0 Total sends 429 receives 428 collectives 0 delays 1040 wait alls 43 waits 0 send time 577743.762210 wait 231065138.489997
 Incomplete wait operation Rank 78 
 Waiting reqs:  77  39  6  41  34  33  31  30  28  55  9  27  10  50  47  46  38  37  45  12  40  44  36  43  7  42  11  32  25  26  4  49  35  2 
 LP 173 unmatched irecvs 4 unmatched sends 6 Total sends 595 receives 596 collectives 0 delays 1363 wait alls 40 waits 0 send time 829592.646004 wait 47732961.680939
 Incomplete wait operation Rank 79 
 Waiting reqs:  57  68  69  47  48  59  66  78  73  75  72  74  3  62  61  46  16  29  79  14  12  25  7  67  65  15  51  38  33  41  40  5  10  56  52  58  44  17  13  77  60  71  32  64  76  19  49  63  50  70  42  39 
 LP 174 unmatched irecvs 4 unmatched sends 9 Total sends 670 receives 663 collectives 0 delays 1500 wait alls 35 waits 0 send time 958006.790923 wait 225767428.774469
 Incomplete wait operation Rank 80 
 Waiting reqs:  94  88  87  73  74  57  63  71  91  75  80  79  77  65  78  84  67  42  43  48  49  50  70  66  90  62  56  93  52  60  95  69  61  64  92  72  89  83  86  85  58  51  59  68  76  81  82  54  53  55 
 LP 180 unmatched irecvs 2 unmatched sends 14 Total sends 697 receives 695 collectives 0 delays 1559 wait alls 35 waits 0 send time 1007748.184401 wait 52763028.954308
 Incomplete wait operation Rank 81 
 Waiting reqs:  83  81  59  32  28  51  20  60  50  49  68  22  63  64  65  100  57  69  101  61  33  46  48  75  76  84  77  11  73  74  78  79  80  45  85  86  58  87  55  88  6  89  62  71  90  53  72  47  52  36  7  56 
 LP 181 unmatched irecvs 6 unmatched sends 0 Total sends 931 receives 932 collectives 0 delays 2038 wait alls 41 waits 0 send time 1225200.261587 wait 180485517.691000
 Incomplete wait operation Rank 82 
 Waiting reqs:  64  71  59  57  84  41  39  55  28  47  58  48  61  44  86  43  38  37  50  29  14  3  17  70  15  69  68  82  33  67  46  66  53  52  56  60  51 
 LP 182 unmatched irecvs 10 unmatched sends 6 Total sends 994 receives 1003 collectives 0 delays 2175 wait alls 42 waits 0 send time 1292828.180730 wait 223521158.103520
 Incomplete wait operation Rank 83 
 Waiting reqs:  96  50  4  70  99  69  59  28  54  56  79  77  52  45  53  51  84  39  60 
 LP 183 unmatched irecvs 2 unmatched sends 6 Total sends 688 receives 690 collectives 0 delays 1556 wait alls 42 waits 0 send time 877292.646687 wait 229977947.898053
 Incomplete wait operation Rank 84 
 Waiting reqs:  30  2  19  32  3  7  68  23  64  5  18  26  14  29  56  57  12  16  69  31  9  27  21  6  15 
 LP 189 unmatched irecvs 1 unmatched sends 10 Total sends 638 receives 638 collectives 0 delays 1443 wait alls 39 waits 0 send time 888155.423899 wait 186688135.485612
 Incomplete wait operation Rank 85 
 Waiting reqs:  111  109  46  131  107  28  108  147  143  112  141  77  146  125  144  97  113  117  95  118  119  145  128  110  94  121  122  132  99  129  130  134  123  133  100 
 LP 190 unmatched irecvs 1 unmatched sends 19 Total sends 1011 receives 1006 collectives 0 delays 2184 wait alls 39 waits 0 send time 1310664.840379 wait 116389767.640484
 Incomplete wait operation Rank 86 
 Waiting reqs:  103  26  98  56  90  115  43  111  6  38  110  93  88  91  85  105  99  100  102  86  78  92  113  97  84  106  79  77  109  80  81  94  119  116  107  82  112  83  114  89  96  118  101  108  104  87  95  27  33  117 
 LP 191 unmatched irecvs 4 unmatched sends 8 Total sends 727 receives 727 collectives 0 delays 1621 wait alls 35 waits 0 send time 1007816.950441 wait 222517545.982363
 Incomplete wait operation Rank 87 
 Waiting reqs:  35  30  56  31  55  54  9  71  62  65  69  52  60  58  89  90  64  59  3  47  81  15  38  68  77  36  66  67  48  11  57  45  85  63  53  72  73  74  46  37  70  88  50  75  76  61  96  51  49  8 
 LP 192 unmatched irecvs 10 unmatched sends 1 Total sends 979 receives 981 collectives 0 delays 2135 wait alls 41 waits 0 send time 1273916.914978 wait 226154996.391650
 Incomplete wait operation Rank 88 
 Waiting reqs:  77  75  74  73  72  71  70  69  50  53  79  48  67  82  68  88  80  93  59  96  95  94  78  92  91  99  97  64  55  76  62  90  86  85  66  65  41  87  98  84  81  89  61  63  60  57  54  58  83  56  51  49 
 LP 198 unmatched irecvs 3 unmatched sends 4 Total sends 710 receives 710 collectives 0 delays 1590 wait alls 36 waits 0 send time 1020866.743401 wait 218880527.657061
 Incomplete wait operation Rank 89 
 Waiting reqs:  82  72  37  100  74  80  11  88  81  73  94  92  97  93  35  78  91  89  10  84  83  95  85  76  96  77  71  86  90  99  98  75  87  79 
 LP 199 unmatched irecvs 2 unmatched sends 15 Total sends 836 receives 841 collectives 0 delays 1857 wait alls 46 waits 0 send time 1060212.905821 wait 239418142.516397
 Incomplete wait operation Rank 90 
 Waiting reqs:  40  74  49  24  12  72  55  66  69  62  61  68  79  75  108  45  16  60  58  5  73  78  19  46  64  67 
 LP 200 unmatched irecvs 1 unmatched sends 7 Total sends 922 receives 922 collectives 0 delays 2017 wait alls 45 waits 0 send time 1192005.844566 wait 184038159.285001
 Incomplete wait operation Rank 91 
 Waiting reqs:  64  57  103  13  131  99  59  8  33  93  123  69  118  49  48  70  58  66  68  60  102  62  71  72  73  22  74  67  106  61  75  98  76  77  53  10  153  65  4  54  55 
 LP 201 unmatched irecvs 1 unmatched sends 16 Total sends 993 receives 992 collectives 0 delays 2152 wait alls 39 waits 0 send time 1320060.022264 wait 227218276.761937
 Incomplete wait operation Rank 92 
 Waiting reqs:  181  222  90  77  171  167  168  184  147  174  142  182  183  155  172  159  158  157  154  173  180  179  164  185  160  162  177  170  153  152  151  140  163  166  165  156  149  148  161  88  169  178  7  176  175  150  141  143  144  145  204  146 
 LP 207 unmatched irecvs 5 unmatched sends 7 Total sends 1042 receives 1043 collectives 0 delays 2257 wait alls 40 waits 0 send time 1376548.330786 wait 175715325.922586
 Incomplete wait operation Rank 93 
 Waiting reqs:  31  48  36  88  117  49  14  41  37  11  3  12  38  50  42  6  43  87  32  4  35  19  28  116  45  29  18  44  20  47  25  23  10  5  9  118  114  30  46  33  40  34  17  8  119  115  121  7  120  15 
 LP 208 unmatched irecvs 12 unmatched sends 2 Total sends 936 receives 937 collectives 0 delays 2048 wait alls 41 waits 0 send time 1261195.294427 wait 188162470.994487
 Incomplete wait operation Rank 94 
 Waiting reqs:  81  87  114  115  101  109  84  119  117  123  110  4  34  116  86  91  51  93  120  113  88  118  128  92  94  95  122  108  96  97  103  85  127  105  124  99  125  100  98  107  121  102  111  90  129  22  89  112  104  35  106  126 
 LP 209 unmatched irecvs 6 unmatched sends 1 Total sends 981 receives 979 collectives 0 delays 2135 wait alls 41 waits 0 send time 1282889.858071 wait 238217582.795058
 Incomplete wait operation Rank 95 
 Waiting reqs:  31  70  62  29  61  20  51  55  49  71  56  44  34  43  52  45  46  60  59  64  58  63  48  47  57  69  32  65  68  53  54  50  66  67 
 LP 210 unmatched irecvs 4 unmatched sends 3 Total sends 515 receives 514 collectives 0 delays 1199 wait alls 36 waits 0 send time 743806.879047 wait 230040677.527528
 Incomplete wait operation Rank 96 
 Waiting reqs:  87  43  52  62  63  12  23  19  42  65  56  32  27  64  66  57  45  60  20  36  68  6  47  7  44  48  59  49 
 LP 216 unmatched irecvs 1 unmatched sends 11 Total sends 607 receives 608 collectives 0 delays 1382 wait alls 39 waits 0 send time 832089.265448 wait 176001146.283110
 Incomplete wait operation Rank 97 
 Waiting reqs:  33  21  98  28  47  96  9  65  10  95  18  7  61  60  29  53  51  59  27  3  66  67  58  64  63  37  62  57  44  92  2  56  45  50  15  35  14  41  34  39  5  12  43  36  17  4  11  52  55  19  16  101 
 LP 217 unmatched irecvs 7 unmatched sends 1 Total sends 908 receives 909 collectives 0 delays 1989 wait alls 40 waits 0 send time 1201075.471305 wait 115354951.805900
 Incomplete wait operation Rank 98 
 Waiting reqs:  42  80  15  81  100  90  89  63  70  71  72  92  73  94  78  88  74  77  98  79  65  76  99  102  85  83  59  60  75  87  68  61  62  84  91  93  101  24  66  82  28  40  26  105  27  86  69  104 
 LP 218 unmatched irecvs 2 unmatched sends 6 Total sends 707 receives 703 collectives 0 delays 1577 wait alls 35 waits 0 send time 1014700.476281 wait 35310999.798924
 Incomplete wait operation Rank 99 
 Waiting reqs:  76  138  62  46  81  48  72  5  70  95  93  83  53  103  24  59  92  85  60  89  9  88  84  71  102  82  150  86  87  57  104  10  74  91  78  64  13  69  61  79  90  101  154  75  68  96  149  63  18  67 
 LP 219 unmatched irecvs 13 unmatched sends 0 Total sends 1000 receives 998 collectives 0 delays 2173 wait alls 41 waits 0 send time 1305354.191244 wait 48923342.109951
 Incomplete wait operation Rank 100 
 Waiting reqs:  75  62  66  5  2  87  58  9  79  88  65  71  53  23  81  82  67  85  39  74  70  27  15  10  80  83  72  61  36  86  48  24  12  28  26  7  57  56  63  60  69  22  73  30  54  37  76  64  41  68  84  89 
 LP 225 unmatched irecvs 9 unmatched sends 5 Total sends 720 receives 715 collectives 0 delays 1605 wait alls 36 waits 0 send time 1029183.752810 wait 124695940.687420
 Incomplete wait operation Rank 101 
 Waiting reqs:  42  52  27  14  3  38  45  59  26  41  75  44  8  46  40  74  23  4  20  15  24  48  29  43  31  47  28  39  35  49  12  22  51  50 
 LP 226 unmatched irecvs 6 unmatched sends 7 Total sends 658 receives 656 collectives 0 delays 1489 wait alls 41 waits 0 send time 893126.844359 wait 116743680.529007
 Incomplete wait operation Rank 102 
 Waiting reqs:  71  64  70  69  80  79  66  78  61  60  63  73  75  77  72  65  62  74  76  59  67  68 
 LP 227 unmatched irecvs 3 unmatched sends 0 Total sends 497 receives 497 collectives 0 delays 1166 wait alls 40 waits 0 send time 676703.058167 wait 161746602.636812
 Incomplete wait operation Rank 103 
 Waiting reqs:  51  58  54  55  43  44  57  53  45  46  9  32  52  33  37  38  59  34  14  35  40  39  25  42  41  36  24  10  56  16  50  49  48  47 
 LP 228 unmatched irecvs 3 unmatched sends 1 Total sends 493 receives 493 collectives 0 delays 1153 wait alls 35 waits 0 send time 727340.543262 wait 118101218.738743
 Incomplete wait operation Rank 104 
 Waiting reqs:  58  15  57  46  48  56  55  76  75  71  47  74  73  72  61  68  63  64  45  60  49  70  25  67  54  59  66  62  69  65  50  51  52  53 
 LP 234 unmatched irecvs 1 unmatched sends 5 Total sends 715 receives 712 collectives 0 delays 1599 wait alls 40 waits 0 send time 968110.656419 wait 162075311.768565
 Incomplete wait operation Rank 105 
 Waiting reqs:  44  43  42  90  41  40  15  38  64  37  48  58  24  57  65  53  28  54  55  52  56  23  88  36  63  66  45  46  47  49  50  59  51  39 
 LP 235 unmatched irecvs 9 unmatched sends 1 Total sends 685 receives 679 collectives 0 delays 1539 wait alls 41 waits 0 send time 903326.217396 wait 71007679.974428
 Incomplete wait operation Rank 106 
 Waiting reqs:  50  8  42  58  19  41  5  40  49  54  64  10  37  60  52  53  18  59  55  51  13  61  16  62  63  38  6  43  44  45  46  47  48  39 
 LP 236 unmatched irecvs 6 unmatched sends 4 Total sends 657 receives 656 collectives 0 delays 1491 wait alls 44 waits 0 send time 881147.110978 wait 179683475.654244
 Incomplete wait operation Rank 107 
 Waiting reqs:  23  6  30  32  21  33  2  34  35  29  5  24  25  17  4  31  22  19  26  27  28  20 
 LP 237 unmatched irecvs 4 unmatched sends 5 Total sends 466 receives 467 collectives 0 delays 1108 wait alls 41 waits 0 send time 624887.538036 wait 121942167.532839
 Incomplete wait operation Rank 108 
 Waiting reqs:  28  27  51  22  25  10  7  26  19  29  11  21  23  5  3  24  30  31  32  33  43  9 
 LP 243 unmatched irecvs 6 unmatched sends 0 Total sends 471 receives 473 collectives 0 delays 1119 wait alls 41 waits 0 send time 628830.021842 wait 224300504.079492
 Incomplete wait operation Rank 109 
 Waiting reqs:  95  56  36  79  88  58  96  86  91  83  4  77  89  143  93  92  94  23  81  59  144  78  90  28  11  57  82  76  55  87  60  65  80  85 
 LP 244 unmatched irecvs 9 unmatched sends 0 Total sends 775 receives 776 collectives 0 delays 1726 wait alls 41 waits 0 send time 1010906.049233 wait 222334577.025746
 Incomplete wait operation Rank 110 
 Waiting reqs:  69  70  50  65  66  53  51  48  76  77  29  47  46  2  63  75  62  72  55  68  71  73  52  58  67  64  54  57  49  61  56  74  59  60 
 LP 245 unmatched irecvs 6 unmatched sends 0 Total sends 584 receives 587 collectives 0 delays 1346 wait alls 41 waits 0 send time 816978.003368 wait 56938232.041468
 Incomplete wait operation Rank 111 
 Waiting reqs:  16  40  36  21  13  6  20  27  37  38  34  39  41  44  42  4  14  11  64  22  7  15  17  47  25  23  33  35  46  43  28  24  31  32 
 LP 246 unmatched irecvs 3 unmatched sends 3 Total sends 584 receives 584 collectives 0 delays 1343 wait alls 41 waits 0 send time 815721.452055 wait 228406472.408076
 Incomplete wait operation Rank 112 
 Waiting reqs:  75  45  53  26  63  52  61  59  17  78  77  114  60  31  16  105  32  76  54  49  62  72  117  73  48 
 LP 252 unmatched irecvs 5 unmatched sends 5 Total sends 719 receives 719 collectives 0 delays 1616 wait alls 42 waits 0 send time 945936.521212 wait 116081327.904030
 Incomplete wait operation Rank 113 
 Waiting reqs:  38  34  39  28  35  27  41  26  43  42  40  29  47  30  44  31  36  32  33  45  37  46  25  24  23  7  20  8  6  5  21  16  2  4  3  18  9  13  10  12  17  11  22  19  15  14 
 LP 253 unmatched irecvs 19 unmatched sends 0 Total sends 378 receives 373 collectives 0 delays 929 wait alls 38 waits 0 send time 536155.532718 wait 49766434.575916
 Incomplete wait operation Rank 114 
 Waiting reqs:  51  18  23  53  38  49  40  26  42  47  37  32  54  22  3  20  21  41  39  12  2  17  16  15  14  6  4  50  19  48  44  46  43  45 
 LP 254 unmatched irecvs 6 unmatched sends 6 Total sends 457 receives 456 collectives 0 delays 1080 wait alls 35 waits 0 send time 643158.368798 wait 219152075.708004
 Incomplete wait operation Rank 115 
 Waiting reqs:  69  49  2  36  9  6  68  60  44  65  51  43  135  50  59  48  63  62  54  13  64  101  23  85  57  30  102  67  25  96  97  32  72  10  66  71  134  58  56  133  17  31  55  61  16  53  39  45  77  83  46  15 
 LP 255 unmatched irecvs 6 unmatched sends 9 Total sends 923 receives 924 collectives 0 delays 2019 wait alls 40 waits 0 send time 1214122.049143 wait 123244657.871093
 Incomplete wait operation Rank 116 
 Waiting reqs:  62  252  175  53  171  251  259  61  254  256  253  248  180  64  207  63  174  258  173  188  189  106  238  257  255  241  240  237  242  243  247  186  249  244  246  205  3  36  236  234  239  45  245  235  250  260  54  187  152  181  179  172 
 LP 261 unmatched irecvs 3 unmatched sends 15 Total sends 1041 receives 1048 collectives 0 delays 2261 wait alls 40 waits 0 send time 1304001.924950 wait 171081615.359239
 Incomplete wait operation Rank 117 
 Waiting reqs:  81  104  94  62  52  86  37  14  108  107  92  96  101  59  95  74  64  30  5  63  8  76  65  66  58  109  103  89  102  72  67  61  93  60  75  91  69  106  70  105  68  90  77  87  48  71  88  25  7  41 
 LP 262 unmatched irecvs 5 unmatched sends 9 Total sends 1212 receives 1215 collectives 0 delays 2607 wait alls 46 waits 0 send time 1511060.575010 wait 177691141.800938
 Incomplete wait operation Rank 118 
 Waiting reqs:  48  98  77  95  96  61  84  43  104  78  68  75  81  39  73  79  71  74  80  87  100  63  72  83  99  69  64  41  82  62  65  35 
 LP 263 unmatched irecvs 9 unmatched sends 6 Total sends 729 receives 735 collectives 0 delays 1637 wait alls 37 waits 0 send time 1020455.651242 wait 76343389.854564
 Incomplete wait operation Rank 119 
 Waiting reqs:  55  53  59  52  30  56  26  7  17  35  9  29  50  18  24  42  20  36  44  48  27 
 LP 264 unmatched irecvs 2 unmatched sends 6 Total sends 586 receives 587 collectives 0 delays 1351 wait alls 42 waits 0 send time 803627.386503 wait 183225621.447034
 Incomplete wait operation Rank 120 
 Waiting reqs:  74  41  55  77  43  78  46  47  44  37  45  67  42  5  68  49  51  56  57  58  48  59  60  61  53  40  34  50  54  52 
 LP 270 unmatched irecvs 1 unmatched sends 6 Total sends 775 receives 777 collectives 0 delays 1724 wait alls 44 waits 0 send time 1008841.256354 wait 181605891.522131
 Incomplete wait operation Rank 121 
 Waiting reqs:  23  10  30  74  28  56  51  49  71  58  29  75  62  52  50  69  3  76  54  63  60  57  87  41  68  43  4  5  59  26  66  47  32  42 
 LP 271 unmatched irecvs 1 unmatched sends 15 Total sends 905 receives 898 collectives 0 delays 1970 wait alls 39 waits 0 send time 1186534.131827 wait 236314355.359807
 Incomplete wait operation Rank 122 
 Waiting reqs:  119  123  120  104  100  86  79  80  76  115  84  82  112  87  99  121  83  101  103  113  92  102  77  81  98  73  117  88  111  116  72  75  89  93  90  85  78  105  114  74  106  110  91  97  118  122  95  109  108  107  96  94 
 LP 272 unmatched irecvs 6 unmatched sends 7 Total sends 739 receives 737 collectives 0 delays 1643 wait alls 35 waits 0 send time 1049270.698569 wait 185231601.838708
 Incomplete wait operation Rank 123 
 Waiting reqs:  81  76  64  4  58  62  77  70  96  86  87  92  60  82  61  65  91  90  42  39  73  56  54  45  53  84  68  97  59  85  57  66  69  89  93  88  123  63  74  94  52  79  75  78  80  98  67  71  51  72  55  95 
 LP 273 unmatched irecvs 13 unmatched sends 1 Total sends 965 receives 964 collectives 0 delays 2104 wait alls 41 waits 0 send time 1304040.580325 wait 185657302.897457
 Incomplete wait operation Rank 124 
 Waiting reqs:  21  29  30  52  54  25  13  16  32  4  42  71  76  24  74  70  9  5  68  7  31  3  10  36  86  20  39  2  46  27  77  6  17  72  45  33  23  34  35  44  43  73  11  75  14  41  12  69  40  78  22  15 
 LP 279 unmatched irecvs 6 unmatched sends 2 Total sends 852 receives 848 collectives 0 delays 1875 wait alls 41 waits 0 send time 1186552.367287 wait 40079479.631163
 Incomplete wait operation Rank 125 
 Waiting reqs:  31  7  35  10  22  2  5  3  40  16  24  44  28  6  37  9  36  4  43  21  29  17  34  27  38  41  20  18  12  25  11  30  23  42 
 LP 280 unmatched irecvs 4 unmatched sends 7 Total sends 624 receives 622 collectives 0 delays 1421 wait alls 41 waits 0 send time 865917.399273 wait 54473264.993919
 Incomplete wait operation Rank 126 
 Waiting reqs:  54  73  69  75  64  78  74  63  48  30  67  19  49  58  76  61  59  52  35  65  66  53  51  79  57  62  56  60  70  50  71  72 
 LP 281 unmatched irecvs 1 unmatched sends 14 Total sends 484 receives 486 collectives 0 delays 1129 wait alls 33 waits 0 send time 695605.854112 wait 240352589.988020
 Incomplete wait operation Rank 127 
 Waiting reqs:  10  8  55  9  37  34  75  67  40  14  88  96  21  25  66  68  78  95  4  73  48  63  62  70  39  6  74  72  71  65  43  11  50  89  69  60 
 LP 282 unmatched irecvs 1 unmatched sends 9 Total sends 879 receives 879 collectives 0 delays 1925 wait alls 39 waits 0 send time 1178513.445918 wait 238638211.103677
 Incomplete wait operation Rank 128 
 Waiting reqs:  84  109  116  45  88  106  18  124  114  82  85  80  107  108  79  113  78  123  122  118  74  73  77  103  76  75  81  119  120  121  87  86  95  110  21  105  117  94  96  104  115  90  83  112  91  111  92  93  89  97 
 LP 288 unmatched irecvs 9 unmatched sends 4 Total sends 724 receives 716 collectives 0 delays 1607 wait alls 35 waits 0 send time 1055383.116342 wait 240329033.425370
 Incomplete wait operation Rank 129 
 Waiting reqs:  11  2  19  20  40  53  49  55  52  24  21  25  47  16  60  50  95  4  46  120  14  32  45  23  41  56  22  29  9  35  36  30  61  10  93  59  48  99  58  51  33  94  34  5  8  13  57  54  31  96  98  28 
 LP 289 unmatched irecvs 3 unmatched sends 10 Total sends 917 receives 918 collectives 0 delays 2007 wait alls 40 waits 0 send time 1226906.981203 wait 238207971.592006
 Incomplete wait operation Rank 130 
 Waiting reqs:  40  21  124  100  104  70  103  57  72  3  67  51  142  66  47  102  71  65  64  69  56  34  75  123  36  77  49  73  74  115  76  78  52  50  59  32  68  99  112  110  148  14  22  28  58  60  35  61  62  116 
 LP 290 unmatched irecvs 2 unmatched sends 14 Total sends 1012 receives 1013 collectives 0 delays 2197 wait alls 40 waits 0 send time 1338506.066016 wait 230966325.157066
 Incomplete wait operation Rank 131 
 Waiting reqs:  107  103  87  111  93  99  85  84  102  96  125  114  88  90  97  92  98  115  91  86  101  116  104  95  100  109  89  105  31  47  36  94  106  113 
 LP 291 unmatched irecvs 1 unmatched sends 10 Total sends 765 receives 762 collectives 0 delays 1699 wait alls 40 waits 0 send time 998537.570034 wait 227690303.518324
 Incomplete wait operation Rank 132 
 Waiting reqs:  69  74  81  84  107  75  95  67  65  25  94  85  73  82  83  72  79  86  87  88  71  12  76  89  90  80 
 LP 297 unmatched irecvs 1 unmatched sends 8 Total sends 694 receives 695 collectives 0 delays 1556 wait alls 39 waits 0 send time 941392.325686 wait 226827946.642237
 Incomplete wait operation Rank 133 
 Waiting reqs:  68  62  163  35  5  75  54  25  77  32  33  61  90  88  57  74  56  134  92  91  14  70  51  76  79  48  23  4  50  52  80  81  55  71  87  86 
 LP 298 unmatched irecvs 1 unmatched sends 16 Total sends 1086 receives 1086 collectives 0 delays 2344 wait alls 44 waits 0 send time 1391281.556278 wait 53753977.834944
 Incomplete wait operation Rank 134 
 Waiting reqs:  93  94  81  80  89  54  82  10  92  65  26  70  90  132  83  71  131  79  78  75  88  77  27  84  91  61  76  69  38  64  48  60  19  73  29  9  72  56  23  57  63  58  59  42  68  49  13  130  85  86  87  133 
 LP 299 unmatched irecvs 7 unmatched sends 3 Total sends 956 receives 950 collectives 0 delays 2078 wait alls 40 waits 0 send time 1272967.772659 wait 42545751.644466
 Incomplete wait operation Rank 135 
 Waiting reqs:  118  121  123  85  54  103  108  100  95  94  102  101  99  98  34  124  84  96  75  51  113  91  93  126  122  80  73  43  76  117  92  2  19  29  88  89  81  97  83  74  17  90  82  107  114  104  105  142  106  125  4  79 
 LP 300 unmatched irecvs 4 unmatched sends 6 Total sends 1029 receives 1030 collectives 0 delays 2231 wait alls 40 waits 0 send time 1338141.727805 wait 179408359.317265
 Incomplete wait operation Rank 136 
 Waiting reqs:  63  86  25  31  65  68  28  7  37  2  84  14  9  66  29  19  24  45  78  69  83  43  61  4  67  23  77  17  52  81  79  15  51  82  70  16  55  75  59  50  49  88  71  76  13  72  54  27  80  64  18  53 
 LP 306 unmatched irecvs 3 unmatched sends 9 Total sends 682 receives 687 collectives 0 delays 1536 wait alls 35 waits 0 send time 972731.193271 wait 233093977.433114
 Incomplete wait operation Rank 137 
 Waiting reqs:  108  104  103  101  78  110  11  2  106  12  94  95  100  47  99  91  105  18  98  79  102  82  6  96  93  80  83  90  85  86  138  107  109  92 
 LP 307 unmatched irecvs 1 unmatched sends 6 Total sends 717 receives 716 collectives 0 delays 1605 wait alls 40 waits 0 send time 940892.558373 wait 223897397.326177
 Incomplete wait operation Rank 138 
 Waiting reqs:  31  30  2  68  25  37  20  26  3  55  11  35  36  24  27  67  32  33  34  10  28  29 
 LP 308 unmatched irecvs 4 unmatched sends 0 Total sends 474 receives 476 collectives 0 delays 1122 wait alls 40 waits 0 send time 639517.429503 wait 236639288.353591
 Incomplete wait operation Rank 139 
 Waiting reqs:  13  3  41  50  49  48  18  46  39  70  59  40  58  57  56  47  37  55  53  23  52  51  85  36  54  35  4  34  38  42  43  2  44  45 
 LP 309 unmatched irecvs 5 unmatched sends 0 Total sends 660 receives 658 collectives 0 delays 1490 wait alls 40 waits 0 send time 907059.689931 wait 229590999.367288
 Incomplete wait operation Rank 140 
 Waiting reqs:  36  49  54  55  4  48  50  35  39  43  40  47  22  63  37  41  38  51  42  45  33  53  62  64  52  44  34  46  61  60  59  58 
 LP 315 unmatched irecvs 4 unmatched sends 1 Total sends 462 receives 459 collectives 0 delays 1088 wait alls 35 waits 0 send time 655916.978824 wait 227166650.432868
 Incomplete wait operation Rank 141 
 Waiting reqs:  60  59  58  71  70  69  68  67  66  104  56  52  65  78  79  77  83  75  80  81  82  76  74  73  27  72  61  53  54  55  103  62  63  64 
 LP 316 unmatched irecvs 4 unmatched sends 4 Total sends 668 receives 673 collectives 0 delays 1513 wait alls 40 waits 0 send time 893845.719341 wait 239992318.745634
 Incomplete wait operation Rank 142 
 Waiting reqs:  49  59  58  57  55  43  45  48  14  54  56  74  73  72  44  71  70  64  66  46  16  67  68  65  69  50  63  62  51  18  52  53  60  61 
 LP 317 unmatched irecvs 3 unmatched sends 6 Total sends 603 receives 605 collectives 0 delays 1380 wait alls 40 waits 0 send time 811723.276830 wait 233283981.548872
 Incomplete wait operation Rank 143 
 Waiting reqs:  35  29  30  14  22  23  24  27  25  26  31  40  28  18  41  39  38  37  36  34  33  32 
 LP 318 unmatched irecvs 1 unmatched sends 4 Total sends 358 receives 359 collectives 0 delays 887 wait alls 38 waits 0 send time 507052.330040 wait 239551785.121361
 Incomplete wait operation Rank 144 
 Waiting reqs:  22  9  10  20  14  12  5  15  70  19  82  3  2  21  49  55  41  52  53  54  50  51 
 LP 324 unmatched irecvs 6 unmatched sends 0 Total sends 568 receives 568 collectives 0 delays 1314 wait alls 44 waits 0 send time 727918.539226 wait 46230875.338416
 Incomplete wait operation Rank 145 
 Waiting reqs:  12  47  17  35  15  38  9  3  5  31  16  45  27  26  25  18  34  43  33  28  14  42  7  41  44  39  10  24  13  2  20  8  6  46 
 LP 325 unmatched irecvs 9 unmatched sends 0 Total sends 519 receives 519 collectives 0 delays 1213 wait alls 41 waits 0 send time 741573.915939 wait 237676335.028420
 Incomplete wait operation Rank 146 
 Waiting reqs:  95  106  99  114  90  88  100  101  86  77  97  84  103  94  89  102  108  85  93  107  82  104  79  92  98  96  81  87  105  80  76  83  78  91 
 LP 326 unmatched irecvs 6 unmatched sends 0 Total sends 733 receives 734 collectives 0 delays 1642 wait alls 41 waits 0 send time 948072.946951 wait 226187027.271170
 Incomplete wait operation Rank 147 
 Waiting reqs:  43  57  6  44  29  27  55  54  53  38  52  51  42  49  56  3  50  2  35  34  13  28  15  22  40  4  47  45  26  30  14  41  46  37 
 LP 327 unmatched irecvs 3 unmatched sends 5 Total sends 483 receives 482 collectives 0 delays 1135 wait alls 36 waits 0 send time 701532.446801 wait 236426633.508975
 Incomplete wait operation Rank 148 
 Waiting reqs:  28  42  23  47  24  64  37  44  30  34  10  39  17  4  65  40  45  41  3 
 LP 333 unmatched irecvs 3 unmatched sends 4 Total sends 616 receives 619 collectives 0 delays 1413 wait alls 42 waits 0 send time 834594.201506 wait 233750290.926251
 Incomplete wait operation Rank 149 
 Waiting reqs:  67  56  64  63  71  66  62  59  69  57  75  74  65  68  61  73  22  72  15  58  70  60  55  76  87  86  85  84  83  82  20  81  80  79  78  77 
 LP 334 unmatched irecvs 15 unmatched sends 0 Total sends 497 receives 496 collectives 0 delays 1176 wait alls 43 waits 0 send time 637359.327446 wait 225739892.040511
 Incomplete wait operation Rank 150 
 Waiting reqs:  67  20  62  61  19  73  74  83  87  70  56  86  89  85  72  65  80  81  82  77  76  63  75  88  57  71  69  64  84  66  68  58  59  60 
 LP 335 unmatched irecvs 4 unmatched sends 6 Total sends 579 receives 579 collectives 0 delays 1330 wait alls 40 waits 0 send time 801532.443500 wait 237216724.393344
 Incomplete wait operation Rank 151 
 Waiting reqs:  55  97  76  21  50  19  43  46  69  45  12  48  107  41  3  90  27  24  34  56  28  6  4  31  11  26  17  103  104  37  47  108  44  99  22  39  80  106  57  77  109  89  15  7  95  25  100  96  98  105  88  30 
 LP 336 unmatched irecvs 4 unmatched sends 9 Total sends 973 receives 977 collectives 0 delays 2125 wait alls 43 waits 0 send time 1237424.357001 wait 239649834.684524
 Incomplete wait operation Rank 152 
 Waiting reqs:  56  104  83  84  85  105  82  81  99  66  98  97  67  64  80  95  73  62  101  60  70  103  65  100  59  71  102  94  74  96  91  92  44  54  89  88  26  9  90  93  61  69  58  68  86  87  55  57  76  63  79  78 
 LP 342 unmatched irecvs 2 unmatched sends 15 Total sends 713 receives 717 collectives 0 delays 1597 wait alls 35 waits 0 send time 1030461.940135 wait 236986245.965003
 Incomplete wait operation Rank 153 
 Waiting reqs:  82  67  59  94  23  34  11  80  65  75  12  29  48  89  71  68  72  77  76  70  90  55  73  30  88  85  32  78  57  93  62  87  79  81  84  69  50  58  9  19  74  66  56  63  61  83  64  91  60  95  51  31 
 LP 343 unmatched irecvs 6 unmatched sends 2 Total sends 715 receives 713 collectives 0 delays 1598 wait alls 36 waits 0 send time 1017701.401893 wait 52624393.617688
 Incomplete wait operation Rank 154 
 Waiting reqs:  92  16  46  24  76  42  64  26  55  27  69  62  34  37  88  50  40  61  47  48  12  70  33  58  44  90  31  10  105  9  57  45  5 
 LP 344 unmatched irecvs 7 unmatched sends 4 Total sends 924 receives 921 collectives 0 delays 2023 wait alls 42 waits 0 send time 1220829.604284 wait 236340819.196100
 Incomplete wait operation Rank 155 
 Waiting reqs:  54  55  23  61  35  46  59  64  50  51  47  56  53  42  34  52  44  43  45  63  65 
 LP 345 unmatched irecvs 4 unmatched sends 3 Total sends 523 receives 523 collectives 0 delays 1219 wait alls 37 waits 0 send time 750487.522096 wait 227540785.594514
 Incomplete wait operation Rank 156 
 Waiting reqs:  43  96  42  44  41  12  50  51  38  35  52  53  88  76  39  20  36  45  89  2  71  34  94  48 
 LP 351 unmatched irecvs 1 unmatched sends 10 Total sends 658 receives 658 collectives 0 delays 1483 wait alls 39 waits 0 send time 894788.747478 wait 237643421.900779
 Incomplete wait operation Rank 157 
 Waiting reqs:  84  87  44  86  3  60  27  89  94  96  42  102  29  14  31  23  30  88  57  36  43  34  95  103  104  82  70  69  22  35  83  68  93  8  98  80  67  100  97  74  105  66 
 LP 352 unmatched irecvs 1 unmatched sends 19 Total sends 699 receives 700 collectives 0 delays 1561 wait alls 34 waits 0 send time 1010961.500881 wait 236760464.675151
 Incomplete wait operation Rank 158 
 Waiting reqs:  140  17  108  110  109  99  107  70  106  68  86  59  60  81  87  115  50  85  69  67  125  83  38  88  126  124  84  72  114  113  112  117  102  89  111  123  61  80  100  31  156  96  118  82  75  103  119  73  120  71 
 LP 353 unmatched irecvs 4 unmatched sends 5 Total sends 957 receives 958 collectives 0 delays 2087 wait alls 40 waits 0 send time 1251479.537781 wait 235066313.711576
 Incomplete wait operation Rank 159 
 Waiting reqs:  93  94  95  24  96  98  99  109  100  89  108  82  35  80  97  79  81  86  106  17  85  113  91  92  67  72  18  66  69  68  71  32  8  74  70  6  78  77  76  75  103  110  88  112  111  87  84  23 
 LP 354 unmatched irecvs 13 unmatched sends 1 Total sends 736 receives 734 collectives 0 delays 1640 wait alls 36 waits 0 send time 1052334.798315 wait 242604412.831997
 Incomplete wait operation Rank 160 
 Waiting reqs:  98  7  108  85  102  72  24  104  90  91  38  87  46  20  2  21  86  71  80  89  103  79  67  18  82  88  81  105  95  96  70  107  6  78  74  68  77  3  75  106  39  69  84  92  97  100  19  27  36  83  93  101 
 LP 360 unmatched irecvs 9 unmatched sends 6 Total sends 743 receives 746 collectives 0 delays 1659 wait alls 36 waits 0 send time 1045651.598487 wait 230946674.466644
 Incomplete wait operation Rank 161 
 Waiting reqs:  26  8  4  6  23  19  28  7  11  22  31  15  32  17  35  9  64  34  5  21  14  20  79  27  16  77  78  18  10  12  76  2  13  36 
 LP 361 unmatched irecvs 6 unmatched sends 5 Total sends 640 receives 642 collectives 0 delays 1457 wait alls 41 waits 0 send time 855076.923761 wait 233075144.360030
 Incomplete wait operation Rank 162 
 Waiting reqs:  49  50  13  54  67  69  66  43  59  65  64  70  47  48  63  18  56  72  53  68  62  51  46  61  33  45 
 LP 362 unmatched irecvs 1 unmatched sends 6 Total sends 478 receives 480 collectives 0 delays 1120 wait alls 34 waits 0 send time 727848.474966 wait 224287376.107759
 Incomplete wait operation Rank 163 
 Waiting reqs:  52  31  27  26  48  38  72  80  56  50  40  57  66  71  83  19  28  29  24  22  73  75  47  21  44  55  54  15  51  82  70  67  69  68  2  81 
 LP 363 unmatched irecvs 1 unmatched sends 14 Total sends 639 receives 643 collectives 0 delays 1444 wait alls 34 waits 0 send time 950676.700736 wait 232113819.281934
 Incomplete wait operation Rank 164 
 Waiting reqs:  93  18  83  144  76  79  134  40  77  88  75  19  81  86  78  68  44  74  121  80  120  84  95  94  82  3  5  176  174  70  143  97  99  98  8  34  87  85  96  92  91  90  89  73  69  119  123  145  167  130  71  72 
 LP 369 unmatched irecvs 9 unmatched sends 3 Total sends 1063 receives 1065 collectives 0 delays 2300 wait alls 40 waits 0 send time 1382957.078557 wait 239603269.299510
 Incomplete wait operation Rank 165 
 Waiting reqs:  77  67  88  126  5  84  119  125  78  41  160  89  90  83  22  73  42  72  37  66  16  117  161  54  159  158  82  121  120  79  75  71  137  74  85  69  6  70  123  80  118  122  9  152  68  81  86  53  87  34  76  127 
 LP 370 unmatched irecvs 6 unmatched sends 6 Total sends 1025 receives 1030 collectives 0 delays 2227 wait alls 40 waits 0 send time 1315875.984375 wait 232473730.735596
 Incomplete wait operation Rank 166 
 Waiting reqs:  38  87  37  81  22  21  15  8  6  80  30  78  36  2  52  84  74  13  86  3  49  55  26  17  73  66  67  65  39  7  9  42  31  72  71  16  83  59  32  85  27  70  23  69  60  61  82  68  48  45  79  64 
 LP 371 unmatched irecvs 4 unmatched sends 9 Total sends 886 receives 884 collectives 0 delays 1942 wait alls 40 waits 0 send time 1202555.039014 wait 237931427.538023
 Incomplete wait operation Rank 167 
 Waiting reqs:  48  52  40  62  39  22  8  66  54  57  53  59  41  63  51  44  49  60  37  67  56  47  46  61  58  55  7  75  65  64  43  45  76  42 
 LP 372 unmatched irecvs 2 unmatched sends 6 Total sends 599 receives 597 collectives 0 delays 1368 wait alls 40 waits 0 send time 827388.116300 wait 234794104.163349
 Incomplete wait operation Rank 168 
 Waiting reqs:  21  20  23  32  7  10  34  27  36  17  15  18  30  3  31  6  33  59  26  4  16  69  24  8  28  25  13 
 LP 378 unmatched irecvs 1 unmatched sends 11 Total sends 589 receives 592 collectives 0 delays 1348 wait alls 39 waits 0 send time 822923.660716 wait 27130482.778358
 Incomplete wait operation Rank 169 
 Waiting reqs:  85  84  62  20  56  81  35  83  22  31  12  15  44  43  11  57  58  5  51  24  10  2  14  18  28  16  46  8  54  7  36  50  55  19  25  30  33  17  13  40  82  49  89  59  53  9  39  52  32  60  61  3 
 LP 379 unmatched irecvs 10 unmatched sends 0 Total sends 817 receives 816 collectives 0 delays 1805 wait alls 40 waits 0 send time 1106286.910703 wait 222096791.703831
 Incomplete wait operation Rank 170 
 Waiting reqs:  28  128  92  88  74  148  108  147  17  32  82  86  91  26  80  48  78  81  93  13  85  43  31  84  95  104  94  96  97  98  129  89  60 
 LP 380 unmatched irecvs 1 unmatched sends 17 Total sends 911 receives 911 collectives 0 delays 1989 wait alls 39 waits 0 send time 1210214.045688 wait 42998359.283730
 Incomplete wait operation Rank 171 
 Waiting reqs:  78  43  79  72  34  52  60  77  81  18  56  20  75  80  83  65  25  54  82  64  76  14  3  66  36  2  68  16  58  67  69  63  71  22 
 LP 381 unmatched irecvs 1 unmatched sends 17 Total sends 642 receives 642 collectives 0 delays 1446 wait alls 34 waits 0 send time 944219.789192 wait 35893653.759773
 Incomplete wait operation Rank 172 
 Waiting reqs:  30  2  124  51  35  38  36  62  126  34  49  60  144  40  101  37  56  61  27  128  46  63  146  19  123  125  138  8  79  47  141  55  143  78  127 
 LP 387 unmatched irecvs 1 unmatched sends 20 Total sends 1004 receives 1009 collectives 0 delays 2183 wait alls 42 waits 0 send time 1298098.301219 wait 119787364.025795
 Incomplete wait operation Rank 173 
 Waiting reqs:  3  39  27  36  28  53  33  5  21  6  59  30  56  26  54  16  4  60  52  57  38  55  13  19  24  49  17  58  14  2  18  11  15  51 
 LP 388 unmatched irecvs 3 unmatched sends 0 Total sends 545 receives 544 collectives 0 delays 1261 wait alls 40 waits 0 send time 773822.692062 wait 234383866.953383
 Incomplete wait operation Rank 174 
 Waiting reqs:  34  20  30  52  29  38  37  49  48  33  47  45  44  43  42  41  40  28  31  39  35  36 
 LP 389 unmatched irecvs 3 unmatched sends 0 Total sends 409 receives 408 collectives 0 delays 989 wait alls 40 waits 0 send time 575141.709534 wait 237397599.871707
 Incomplete wait operation Rank 175 
 Waiting reqs:  6  33  58  34  17  3  4  20  5  12  18  7  9  56  27  37  13  36  32  35  31  30  53  29  19  28  21  16  55  8  54  10  26  2 
 LP 390 unmatched irecvs 6 unmatched sends 0 Total sends 573 receives 574 collectives 0 delays 1319 wait alls 40 waits 0 send time 807728.424783 wait 75695249.292380
 Incomplete wait operation Rank 176 
 Waiting reqs:  105  60  21  6  104  4  2  59  58  57  23  123  68  8  67  66  65  64  63  62  38  61  103  102  18  13  15  124  5  53  54  55  84  56 
 LP 396 unmatched irecvs 8 unmatched sends 0 Total sends 824 receives 822 collectives 0 delays 1824 wait alls 46 waits 0 send time 1048676.887529 wait 180080917.931109
 Incomplete wait operation Rank 177 
 Waiting reqs:  43  78  49  19  6  67  30  38  79  69  7  128  22  35  28  68  70  18  31  65  2  138  66  132  101  99  34 
 LP 397 unmatched irecvs 1 unmatched sends 8 Total sends 836 receives 838 collectives 0 delays 1844 wait alls 42 waits 0 send time 1054123.271098 wait 177851958.105009
 Incomplete wait operation Rank 178 
 Waiting reqs:  14  86  85  84  83  82  81  116  76  92  80  96  95  9  94  90  93  6  89  87  11  25  88  91  97  98  13  75  5  10  77  7 
 LP 398 unmatched irecvs 6 unmatched sends 0 Total sends 671 receives 671 collectives 0 delays 1514 wait alls 40 waits 0 send time 895158.622424 wait 182627730.222064
 Incomplete wait operation Rank 179 
 Waiting reqs:  98  97  103  102  101  100  122  119  92  91  117  93  89  56  116  118  90  12  99  94  95  96 
 LP 399 unmatched irecvs 3 unmatched sends 0 Total sends 569 receives 568 collectives 0 delays 1309 wait alls 40 waits 0 send time 732778.279205 wait 224709270.126223
 Incomplete wait operation Rank 180 
 Waiting reqs:  70  64  66  65  67  2  10  63  71  68  62  72  91  69 
 LP 405 unmatched irecvs 4 unmatched sends 0 Total sends 392 receives 391 collectives 0 delays 958 wait alls 41 waits 0 send time 500942.940573 wait 234040620.574470
 Incomplete wait operation Rank 181 
 Waiting reqs:  28  27  31  8  18  24  23  22  51  21  10  30  33  32  34  25  35  13  26  36  29  50 
 LP 406 unmatched irecvs 6 unmatched sends 0 Total sends 426 receives 426 collectives 0 delays 1025 wait alls 39 waits 0 send time 582407.466609 wait 181061401.610297
 Incomplete wait operation Rank 182 
 Waiting reqs:  51  36  37  35  52  34  44  41  32  40  42  43  21  65  45  50  33  15  38  9  39  4 
 LP 407 unmatched irecvs 4 unmatched sends 0 Total sends 468 receives 470 collectives 0 delays 1111 wait alls 39 waits 0 send time 623253.789927 wait 186497171.614167
 Incomplete wait operation Rank 183 
 Waiting reqs:  61  60  55  73  9  57  103  56  54  58  53  68  6  74  75  59  10  76  77  62  11  67 
 LP 408 unmatched irecvs 2 unmatched sends 5 Total sends 614 receives 613 collectives 0 delays 1407 wait alls 46 waits 0 send time 779260.434585 wait 159619882.105420
 Incomplete wait operation Rank 184 
 Waiting reqs:  18  17  3  46  20  40  43  19  24  21  14  15  45  52 
 LP 414 unmatched irecvs 2 unmatched sends 3 Total sends 497 receives 495 collectives 0 delays 1173 wait alls 45 waits 0 send time 653416.157644 wait 229746040.201061
 Incomplete wait operation Rank 185 
 Waiting reqs:  47  3  45  15  16  46  76  52  53  49  50  51  10  48  54  55  56  57  58  59  60  61  62  63  64  65  40  72  71  14  70  69  9  68  67  66 
 LP 415 unmatched irecvs 16 unmatched sends 0 Total sends 449 receives 452 collectives 0 delays 1087 wait alls 46 waits 0 send time 544660.354731 wait 159883159.230915
 Incomplete wait operation Rank 186 
 Waiting reqs:  12  41  9  40  38  18  37  32  35  19  17  6  5  13  62  61  39  36  2  8  16  7 
 LP 416 unmatched irecvs 2 unmatched sends 4 Total sends 485 receives 481 collectives 0 delays 1139 wait alls 41 waits 0 send time 662674.801945 wait 180692137.387183
 Incomplete wait operation Rank 187 
 Waiting reqs:  63  86  85  95  78  15  94  28  57  64  93  65  90  11  92  88  91  58  59  60  87  29  61  19  79  70  62  89  3  80  81  82  83  84 
 LP 417 unmatched irecvs 2 unmatched sends 6 Total sends 820 receives 819 collectives 0 delays 1816 wait alls 45 waits 0 send time 1035886.914113 wait 231511020.463662
 Incomplete wait operation Rank 188 
 Waiting reqs:  52  53  54  55  35  50  36  51  37  38  65  39  63  40  41  42  44  43  66  64  45  67  56  62  46  34  47  61  60  59  58  57  49  48 
 LP 423 unmatched irecvs 1 unmatched sends 10 Total sends 468 receives 467 collectives 0 delays 1102 wait alls 35 waits 0 send time 697907.694575 wait 160413236.929688
 Incomplete wait operation Rank 189 
 Waiting reqs:  54  40  10  37  33  55  57  45  46  47  48  34  35  59  36  19  53  29  52  27  56  11  43  51  44  39  58  50  42  41  49  38 
 LP 424 unmatched irecvs 4 unmatched sends 4 Total sends 482 receives 481 collectives 0 delays 1133 wait alls 36 waits 0 send time 708523.979662 wait 228836461.300325
 Incomplete wait operation Rank 190 
 Waiting reqs:  108  99  78  91  79  27  105  18  98  64  82  125  80  104  93  94  83  84  31  86  97  90  87 
 LP 425 unmatched irecvs 7 unmatched sends 3 Total sends 757 receives 761 collectives 0 delays 1696 wait alls 42 waits 0 send time 987486.577405 wait 227067186.074840
 Incomplete wait operation Rank 191 
 Waiting reqs:  16  47  46  59  42  54  53  56  50  43  51  55  58 
 LP 426 unmatched irecvs 3 unmatched sends 3 Total sends 466 receives 467 collectives 0 delays 1111 wait alls 42 waits 0 send time 620170.924733 wait 228033227.600788
 Incomplete wait operation Rank 192 
 Waiting reqs:  31  17  35  33  6  7  32  37  30  13  38  34  45  39  36  47  46  43  44  40  42  69 
 LP 432 unmatched irecvs 4 unmatched sends 0 Total sends 447 receives 449 collectives 0 delays 1068 wait alls 40 waits 0 send time 602339.317132 wait 38112913.325953
 Incomplete wait operation Rank 193 
 Waiting reqs:  72  32  71  62  61  70  7  15  69  74  31  20  50  68  73  56  67  19  64  51  63  65  49  53  58  52  66  57  59  60  11  21  54  55 
 LP 433 unmatched irecvs 4 unmatched sends 0 Total sends 658 receives 658 collectives 0 delays 1488 wait alls 40 waits 0 send time 903391.188692 wait 72490516.487971
 Incomplete wait operation Rank 194 
 Waiting reqs:  12  8  28  25  16  17  4  15  20  34  38  29  22  5  9  42  14  41  40  39  6  35  32  37  33  7  21  31  2  36  24  13  3  23 
 LP 434 unmatched irecvs 2 unmatched sends 4 Total sends 611 receives 612 collectives 0 delays 1395 wait alls 40 waits 0 send time 843492.813498 wait 225915874.387311
 Incomplete wait operation Rank 195 
 Waiting reqs:  66  9  65  71  70  69  13  68  59  74  3  72  28  75  58  29  60  56  62  57  76  61  5  15  19  77  73  63  78  64  79  80  81  11 
 LP 435 unmatched irecvs 10 unmatched sends 3 Total sends 724 receives 723 collectives 0 delays 1622 wait alls 41 waits 0 send time 959214.138639 wait 227079469.717342
 Incomplete wait operation Rank 196 
 Waiting reqs:  52  53  60  61  49  18  54  50  45  55  51  44  6  58  47  48  22  15  38  28  43  8  12  16  31  4  57  27  19  56  29  13 
 LP 441 unmatched irecvs 6 unmatched sends 2 Total sends 530 receives 525 collectives 0 delays 1230 wait alls 41 waits 0 send time 753584.025258 wait 235504270.002406
 Incomplete wait operation Rank 197 
 Waiting reqs:  19  61  55  65  92  16  91  62  64  12  93  66  18  105  98  67  68  17  63  106  90  60 
 LP 442 unmatched irecvs 4 unmatched sends 3 Total sends 541 receives 543 collectives 0 delays 1259 wait alls 41 waits 0 send time 706216.792705 wait 120018675.221664
 Incomplete wait operation Rank 198 
 Waiting reqs:  94  130  97  117  98  99  95  86  16  96  100  102  85  89  25  90  88  87  91  92  93  101 
 LP 443 unmatched irecvs 5 unmatched sends 0 Total sends 559 receives 558 collectives 0 delays 1289 wait alls 40 waits 0 send time 726714.791065 wait 163054352.953672
 Incomplete wait operation Rank 199 
 Waiting reqs:  52  51  71  50  59  55  35  87  47  90  88  67  56  66  53  95  54  65  70  58  48  33  64  61  57  89  101  91  86  102  16  69  103  5 
 LP 444 unmatched irecvs 7 unmatched sends 0 Total sends 674 receives 678 collectives 0 delays 1524 wait alls 40 waits 0 send time 901419.584821 wait 159688782.107741
 Incomplete wait operation Rank 200 
 Waiting reqs:  11  2  24  12  31  23  36  48  41  27  30  45  80  17  47  19  46  78  81  3  37  6  34  42  79  88  15  16  43  74  7  21  39  40 
 LP 450 unmatched irecvs 6 unmatched sends 2 Total sends 719 receives 717 collectives 0 delays 1613 wait alls 45 waits 0 send time 965398.627735 wait 119260199.691725
 Incomplete wait operation Rank 201 
 Waiting reqs:  55  65  36  54  110  51  18  72  64  52  111  62  19  66  32  109  63  57  22  67  59  9  37  105  56  108  112  58  39  61  68  50  53  20 
 LP 451 unmatched irecvs 6 unmatched sends 4 Total sends 678 receives 676 collectives 0 delays 1526 wait alls 40 waits 0 send time 886719.974095 wait 118752177.815691
 Incomplete wait operation Rank 202 
 Waiting reqs:  60  55  64  12  23  73  6  37  43  80  70  31  54  58  63  22  59  3  69  61  71  62  34  38  82  72  56  81  24  36  4  5  21  79 
 LP 452 unmatched irecvs 4 unmatched sends 6 Total sends 932 receives 934 collectives 0 delays 2044 wait alls 46 waits 0 send time 1156215.642649 wait 225452075.400854
 Incomplete wait operation Rank 203 
 Waiting reqs:  56  48  46  47  53  55  52  73  58  19  41  2  51  54  50  57  42  43  44  45  49  40 
 LP 453 unmatched irecvs 2 unmatched sends 4 Total sends 432 receives 431 collectives 0 delays 1035 wait alls 40 waits 0 send time 584308.808246 wait 117545840.467611
 Incomplete wait operation Rank 204 
 Waiting reqs:  60  58  65  67  66  55  43  56  54  50  22  63  57  52  45  42  61  59  64  53  51  62 
 LP 459 unmatched irecvs 3 unmatched sends 0 Total sends 553 receives 553 collectives 0 delays 1281 wait alls 43 waits 0 send time 712761.002194 wait 122189061.119801
 Incomplete wait operation Rank 205 
 Waiting reqs:  7  42  44  38  8  24  61  31  25  58  6  12  18  11  14  43  4  23  30  45  10  32  3  39  33  34  17  64  21  20  52  22 
 LP 460 unmatched irecvs 5 unmatched sends 0 Total sends 573 receives 574 collectives 0 delays 1319 wait alls 40 waits 0 send time 787676.612148 wait 185590566.709371
 Incomplete wait operation Rank 206 
 Waiting reqs:  26  103  22  69  98  89  142  104  92  90  99  145  101  144  105  20  37  94  93  28  112  115  96  111  97 
 LP 461 unmatched irecvs 1 unmatched sends 10 Total sends 698 receives 697 collectives 0 delays 1562 wait alls 39 waits 0 send time 906830.248164 wait 181005683.600417
 Incomplete wait operation Rank 207 
 Waiting reqs:  46  64  51  38  67  50  65  68  54  6  5  11  44  66  56  52  48  24  45  25  43  49  37  30  41  23  47  63 
 LP 462 unmatched irecvs 1 unmatched sends 7 Total sends 514 receives 515 collectives 0 delays 1196 wait alls 39 waits 0 send time 738989.907905 wait 75286925.540013
 Incomplete wait operation Rank 208 
 Waiting reqs:  3  4  15  39  40  5  54  52  45  62  23  46  51  55  43  58  12  47  33  13  26  48  19  42 
 LP 468 unmatched irecvs 1 unmatched sends 11 Total sends 595 receives 596 collectives 0 delays 1358 wait alls 39 waits 0 send time 794058.574247 wait 115817991.370815
 Incomplete wait operation Rank 209 
 Waiting reqs:  13  22  12  6  39  15  9  4  42  3  41  8  18  44  11  45  40  43  14  17  21  5 
 LP 469 unmatched irecvs 3 unmatched sends 0 Total sends 387 receives 386 collectives 0 delays 945 wait alls 40 waits 0 send time 557305.837423 wait 229232309.398013
 Incomplete wait operation Rank 210 
 Waiting reqs:  25  30  29  28  37  34  31  35  33  36  32  24  26  27 
 LP 470 unmatched irecvs 1 unmatched sends 0 Total sends 288 receives 288 collectives 0 delays 746 wait alls 38 waits 0 send time 391426.786534 wait 159902867.324291
 Incomplete wait operation Rank 211 
 Waiting reqs:  5  9  11  23  20  2  45  8  31  15  16  10  3  17  28  29  27  13  12  30  14  48 
 LP 471 unmatched irecvs 4 unmatched sends 0 Total sends 459 receives 456 collectives 0 delays 1090 wait alls 43 waits 0 send time 628312.576622 wait 184190372.431409
 Incomplete wait operation Rank 212 
 Waiting reqs:  70  67  66  68  65  79  80  82  83  69  84  76  85  78  75  81  86  77 
 LP 477 unmatched irecvs 1 unmatched sends 5 Total sends 509 receives 507 collectives 0 delays 1183 wait alls 39 waits 0 send time 633921.371344 wait 46854550.838312
 Incomplete wait operation Rank 213 
 Waiting reqs:  4  52  55  5  60  59  58  57  56  69  68  67  3  66  65  64  53  61  62  54  63  11 
 LP 478 unmatched irecvs 1 unmatched sends 10 Total sends 454 receives 454 collectives 0 delays 1072 wait alls 38 waits 0 send time 591685.456195 wait 234550789.769487
 Incomplete wait operation Rank 214 
 Waiting reqs:  2  35  9  37  36  12  39  5  3  15  20  13  4  6  23  11  8  18  22 
 LP 479 unmatched irecvs 1 unmatched sends 5 Total sends 360 receives 361 collectives 0 delays 888 wait alls 39 waits 0 send time 514414.011427 wait 232156430.490685
 Incomplete wait operation Rank 215 
 Waiting reqs:  43  39  4  35  45  42  41  40  31  36  33  37  34  44 
 LP 480 unmatched irecvs 3 unmatched sends 0 Total sends 289 receives 288 collectives 0 delays 747 wait alls 38 waits 0 send time 377589.192183 wait 225067855.128203
	: Running Time = 36.1261 seconds

TW Library Statistics:
	Total Events Processed                                16297212
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     7793565
	Event Ties Detected in PE Queues                             0
	Efficiency                                                8.35 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                 322490
	Percent Remote Events                                     3.79 %

	Total Roll Backs                                       1014363
	Primary Roll Backs                                      258820
	Secondary Roll Backs                                    755543
	Fossil Collect Attempts                                 236236
	Total GVT Computations                                   59059

	Net Events Processed                                   8503647
	Event Rate (events/sec)                               235388.2
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        202065
	Memory Allocated                                        165437
	Memory Wasted                                              486

TW Network Statistics:
	Remote sends                                           1212152
	Remote recvs                                           1212152

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                2.0605
	AVL Tree (insert/delete)                                0.3196
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                       55.1865
	Event Cancel                                            9.9569
	Event Abort                                             0.0000

	GVT                                                    83.6201
	Fossil Collect                                          1.2738
	Primary Rollbacks                                       3.4529
	Network Read                                            5.3445
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)     86.5017

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                   59059
	Total All Reduce Calls                                  168729
	Average Reduction / GVT                                   2.86

 Total bytes sent 111841160 recvd 111839608 
 max runtime 0.000000 ns avg runtime 0.000000 
 max comm time 0.000000 avg comm time -663078607.134259 
 max send time 1511060.575010 avg send time 889440.103256 
 max recv time 1213307691.560074 avg recv time 468543486.400605 
 max wait time 242604412.831997 avg wait time 163842657.896257 
LP-IO: writing output to amg-216-trace-13620-1495791621/
LP-IO: data files:
   amg-216-trace-13620-1495791621/dragonfly-router-traffic
   amg-216-trace-13620-1495791621/dragonfly-router-stats
   amg-216-trace-13620-1495791621/dragonfly-msg-stats
   amg-216-trace-13620-1495791621/model-net-category-all
   amg-216-trace-13620-1495791621/model-net-category-test
   amg-216-trace-13620-1495791621/mpi-replay-stats
 Average number of hops traversed 3.012948 average chunk latency 2.235076 us maximum chunk latency 17.016149 us avg message size 784.364746 bytes finished messages 142584 finished chunks 537940 

 ADAPTIVE ROUTING STATS: 433337 chunks routed minimally 104603 chunks routed non-minimally completed packets 537940 

 Total packets generated 329450 finished 329450
#+END_EXAMPLE

****** Fichiers de sortie
On retrouve les mêmes fichiers de sortie que précedemment. Cependant,
on voit qu'il y plus d'information sur la sortie (les lignes ont
maintenant des valeurs). On remarque aussi un nombre important de
RollBack, et un taux d'efficacité de seulement 8%

**** Mail Arnaud 2

Salut Maxime,

Voici quelques éléments de réponse avant d'aller enfin m'occuper de mon
jardin! :)

HPL
===

> Hpl semble être une topologie en anneau virtuel (demander
> confirmation à Christian/Tom)

Pour l'instant, ça ressemble à un anneau car tu es sur une petite
taille mais en fait c'est une grille 2D. Tu donnes d'ailleurs dans
HPL.dat deux paramètres

#+BEGIN_EXAMPLE
1            # of process grids (P x Q)
2            Ps
2            Qs
#+END_EXAMPLE
Donc, ça c'est une grille 2x2 mais si tu veux une grille avec
63*37=2331 processus MPI, il suffit de metre
#+BEGIN_EXAMPLE
1            # of process grids (P x Q)
63            Ps
37            Qs
#+END_EXAMPLE

AMG
===

Ah, très bien, tu as lancé AMG. J'avoue n'avoir jamais joué avec mais
ça peut faire un use case intéressant. C'est un autre benchmark du
genre d'HPL et couramment utilisé pour étudier tel ou tel truc. Et
donc, simuler AMG sur une topologie dragonfly, ça n'a visiblement posé
aucun problème. C'est cool. Par contre, la sortie indique
systématiquement " *** START SEQUENTIAL SIMULATION ***". C'est donc
des simulations séquentielles pour l'instant et ça vaut le coup que tu
trouves comment les lancer avec plusieur processus (avec MPI
j'imagine). Sinon, je n'arrive pas à trouver le temps pris par la
simulation dans les sorties de ROSS. C'est long ou ça va vite ?
Rajoute bien un /usr/bin/time avant de lancer tes simulations
("/usr/bin/time model-net-mpi-replay --sync=1 ...").

DUMPI + OTF2
============

> installation de libotf-dev. Cependant le configure ne trouve pas la
> librairie... 

Pour savoir si un lib est installée et utilisable, le script configure
crée un petit programme C minimal qu'il compile et execute. Je ne sais
pas quel message d'erreur tu as eu mais ça peut avoir échoué pour
trois raisons:
- pas trouvé otf.h (c'est le compilateur)
- pas trouve libotf.so (c'est le linker)
- lib non fonctionnelle (l'exéction a renvoyé un =EXIT_FAILURE=).
Il me semble qu'il y a moyen de demander à configure d'être un peu
bavard et de t'expliquer ce qui ne va pas.

Bon courage.

A+
    Arnaud

**** Types de simulation                                      :CODES:ROSS:
#+BEGIN_EXAMPLE
./your-model --synch=1               // sequential mode
  mpirun -np 2 ./your-model --synch=2  // conservative mode
  mpirun -np 2 ./your-model --synch=3  // optimistic mode
  ./your-model --synch=4               // optimistic debug mode (note: not a parallel execution!)
#+END_EXAMPLE

**** Option ROSS                                              :CODES:ROSS:
#+BEGIN_EXAMPLE
ROSS MPI Kernel:
  --read-buffer=n       network read buffer size in # of events (default 50000)
  --send-buffer=n       network send buffer size in # of events (default 50000)

ROSS Kernel:
  --synch=n             Sychronization Protocol: SEQUENTIAL=1, CONSERVATIVE=2, OPTIMISTIC=3 (default 0)
  --nkp=n               number of kernel processes (KPs) per pe (default 1)
  --end=ts              simulation end timestamp (default 100000.00)
  --batch=n             messages per scheduler block (default 16)

ROSS MPI GVT:
  --gvt-interval=n      GVT Interval (default 16)
  --report-interval=ts
                        percent of runtime to print GVT (default 0.05)

ROSS Timing:
  --clock-rate=ts       CPU Clock Rate (default 1000000000.00)
  --help                show this message

ROSS Kernel Build Options:
  MEMORY
  timing
  QUEUE=splay
  RAND=clcg4
  NETWORK=mpi
  CLOCK=amd64
  GVT=mpi_allreduce
#+END_EXAMPLE

**** Temps de simulation 
Toute la premiere partie de l'affichage console (jusqu'a statistiques
sur GTV) proviennent de ROSS. Le running time provient donc de ROSS.

**** Grille 2D
Pour faire une grille 2D, je sais pas si je peux utiliser le TORUS
comme base. En effet, ca se rapporte plus à une sphère si on reste en
2D (à ce que j'ai compris). Il va peut-être falloir que je créé mon
propre modèle sur CODES/ROSS (ou repartir d'un de ces [[https://github.com/carothersc/ROSS-Models][modèles]])

Ce que je comprend pas, c'est que CODES ou ROSS fait des UNDEFINED
MESSAGES lorsque je simule HPL.

**** Mail Arnaud/Florence

***** Contenu
Salut Maxime,

Je te transmets les réponses d'Arnaud à mes questions... ça pourra t'être utile. 

Bon courage et bon week-end 
Florence 

Envoyé de mon iPhone

Début du message transféré :

Expéditeur: Arnaud Legrand <arnaud.legrand@imag.fr>
Date: 25 mai 2017 à 10:20:40 UTC+2
Destinataire: Florence Perronnin <florence.perronnin@imag.fr>
Objet: Rép :⁨ [Stage] Topologie HPL⁩

Hello,

Tu pourras faire suivre les parties qui te paraissent pertinentes à
Maxime.

Le 24/05/2017 22:47, Florence Perronnin a écrit :

Je t’avoue que je ne comprends pas trop pourquoi la topologie ne
joue pas. 

Attends, si si, bien sûr que la topologie joue. C'est essentiel. Mais
dans un premier temps il faut qu'il arrive à rejouer sa trace sur une
topologie physique la plus simple possible. Par exemple, une topologie
où toute machine est connectée directement à chacune des autres. Ce
n'est pas réaliste mais c'est celle qui permet le plus d'indépendance
car les communications n'interfèrent alors pas entre elles.

Quand Maxime a regardé les traces qu’il avait générées sur
4 noeuds, ces derniers ont communiqué selon le schéma : 0 <-> 1 <->
2 <-> 3 <-> 0

Ça, c'est la topologie logique si je comprends bien. La topologie
logique, c'est la façon dont les processus communiquent entre eux. Il
faut distinguer les deux. On utilise une topologie logique quand on
conçoit l'application et on se contraint à penser comme si les
processus étaient physiquement organisés selon cette topologie même si
en pratique après ça ne sera pas le cas. Pourquoi fait-on celà ? Pour
plusieurs raisons:
- S'il n'y avait pas de contrainte du tout et qu'on permettait à
 n'importe qui de communiquer à tout instant avec n'importe qui, le
 shéma de communication qui en résulterait générerait certainement
 énormément de contention et l'application ne s'exécuterait
 absolument pas comme on avait prévu.
- De la contrainte naît la créativité!  Quand on peut faire
 totalement n'importe quoi, on a en fait du mal à écrire ces
 algorithmes. La topologie la plus simple qui est à l'autre bout du
 spectre, c'est un anneau. C'est pour ça qu'on part de là dans le
 livre et qu'on illustre déjà ce qui peut être fait avec ce type de
 topologie logique. Mais il se peut que ça soit un peu réducteur.
- On ne va pas réécrire complètement l'application spécifiquement pour
 chaque nouvelle machine qui a une topologie exotique et ces derniers
 temps, je peux te dire que les gens ont ressorti des choses super
 exotiques (tappered fat-tree, dragonfly, slimfly, small-world
 graphs, etc.)
- On pourrait avoir de la chance et la topologie physique pourrait
 correspondre à ce que l'on avait choisi. C'est rarement le cas et il
 faut alors projeter la topologie logique sur la topologie physique
 de façon a créer le moins de conflits réseaux possible tout en
 maintenant une certaine localité.
Tout ceci sera probablement plus clair quand vous aurez lu les
chapitres là dessus.

Ensuite quand il a simulé ces traces dans CODES avec une autre
topologie ça n’a pas marché (temps de simulation 0.000 et plein de
messages d’erreur) 

Oui, j'ai vu ça. Je ne sais pas pourquoi. S'il n'arrive pas à trouver
pourquoi, il faudra qu'on se plonge un peu dans le code de CODES pour
comprendre ce qui cause ce comportement. Est-ce un bug ou bien est-ce
qu'on donne un fichier d'entrée incohérent ?...

Je crois que tu es encore en déplacement la semaine prochaine… je
vais évidemment lire les chapitres que tu m’as conseillés, mais je
crois que tes input sont vraiment très utiles à Maxime 

Je ferai mon possible pour continuer à réagir à ce qu'il y a dans son
journal. En tous cas, c'est bien qu'ils prennent plus de notes. Sans
ça, je ne pourrais pas suivre ce qu'il fait. Réutiliser le code d'un
autre groupe de recherche, c'est toujours compliqué car on est un peu
seul face à ses problèmes...  Tu peux le rassurer avec ça si
besoin. C'est un sujet difficile mais il avance.

Bon week-end,

   Arnaud

***** Talk to me
J'ai l'intention de commencer à lire le livre d'Arnaud rapidement,
mais effectivement, pour la topologie je pensais bien que même si
c'était pas le bonne ca devait tourner normalement. Le problèmes c'est
les =Undefined data type= 
**** Simulation plus grande
Après avoir voulu faire une simulation HPL de 30000 sur 8 LP (2x4), et
laisser tourner mon pc plusieurs heures, l'application a planté (plus
d'espace disque +20Go). Tant pis...
*** 2017-05-29 lundi
**** Mail CODES-ROSS-users
***** Content 
Hi,
I'm trying to replay HPL's DUMPI trace generated on my computer with CODES. Unfortunatly, I get a lot of "Undefined data type" errors (see the trace below).
I have already replayed AMG traces (downloaded here) and replayed my own generated AMG traces. It has worked fine.
So I'm wondering if I did something bad, or if it's HPL fault.

Best regards,
Maxime

Avec la trace d'execution de HPL sur CODES.
**** Doc ROSS
J'ai trouvé quelques parametres avec lesquels on peut jouer pour la
simulation : [[https://github.com/carothersc/ROSS/issues/82][link]]
**** Meeting avec Florence
En attendant la réponse du mail pour HPL, je vais commencer avec
AMG. Il faut dans un premier temps savoir ce qu'est le *running time*,
si c'est le temps d'execution du simulateur (ce qui serait bien), il
faudra trouver comment il est calculé. Sinon, il faudra faire avec
l'outil time, en désactivant l'affichage et les sorties dans les
fichiers.
Ensuite, il faudra déerminer ce qu'est l'efficacité indiqué en sortie.
Il faudra faire à la fois des mesures en parallèle et en séquentiel,
plusieurs fois vu que le temps d'execution d'un programme n'est jamais
le même, mais avant cela, il faudra fixer les paramètres d'entrées.
Bien sûr, il faut continuer les investigations sur l'OTF.
**** DUMPI + OTF
Je pense que j'y suis presque, mais je n'arrive pas à linker la
librairie...
#+BEGIN_EXAMPLE
configure:17252: checking wheter OTF library and header file can be found
configure:17277: mpicc -o conftest -DMPICH_SUPPRESS_PROTOTYPES=1 -DHAVE_PRAGMA_HP_SEC_DEF=1 -pthread -I/usr/local/include/freetype2 -I/usr/include  -L/usr/local/lib -lfreetype -L/usr/lib/x86_64-linux-gnu -lotf conftest.c  -lotf -lz >&5
conftest.c: In function 'main':
conftest.c:35:4: error: unknown type name 'OTF_FileManager'
    OTF_FileManager *mgr = NULL;
    ^~~~~~~~~~~~~~~
conftest.c:36:10: warning: implicit declaration of function 'OTF_FileManager_open' [-Wimplicit-function-declaration]
    mgr = OTF_FileManager_open(10);
          ^~~~~~~~~~~~~~~~~~~~
conftest.c:36:8: warning: assignment makes pointer from integer without a cast [-Wint-conversion]
    mgr = OTF_FileManager_open(10);
        ^
conftest.c:40:4: warning: implicit declaration of function 'OTF_FileManager_close' [-Wimplicit-function-declaration]
    OTF_FileManager_close(mgr);
    ^~~~~~~~~~~~~~~~~~~~~
configure:17277: $? = 1
#+END_EXAMPLE
**** Signification du *RUNNING TIME*
Comme je trouve rien pour le moment en ligne, je lance une génération
de trace AMG pour comparer les différents temps.
***** Génération de traces
#+begin_src sh :results output :exports both
/usr/bin/time mpirun -np 16 amg2013 -laplace -n 80 80 80 -P 2 4 2
#+end_src
#+Results:
#+BEGIN_EXAMPLE
  3D 7-point Laplace problem on a cube
  (nx_global, ny_global, nz_global) = (160, 320, 160)
  (Px, Py, Pz) = (2, 4, 2)
  (cx, cy, cz) = (1.000000, 1.000000, 1.000000)

=============================================
Setup matrix and rhs:
=============================================
Setup matrix and rhs:
Setup matrix and rhs  wall clock time = 1.469449 seconds
Setup matrix and rhs  cpu clock time  = 0.344692 seconds
=============================================
Setup phase times:
=============================================
PCG Setup:
PCG Setup  wall clock time = 29.687746 seconds
PCG Setup  cpu clock time  = 7.026403 seconds

System Size / Setup Phase Time: 2.759388e+05

=============================================
Solve phase times:
=============================================
PCG Solve:
PCG Solve  wall clock time = 37.440000 seconds
PCG Solve  cpu clock time  = 9.303457 seconds

AMG2013 Benchmark version 1.0
Iterations = 13
Final Relative Residual Norm = 7.345223e-07

System Size * Iterations / Solve Phase Time: 2.844444e+06

252.09user 4.76system 1:09.38elapsed 370%CPU (0avgtext+0avgdata 153516maxresident)k
0inputs+30440outputs (0major+668341minor)pagefaults 0swaps

#+END_EXAMPLE
Ca a donc pris en tout 1:09, sachant qu'il y a la génération des
traces DUMPI.
***** Replay sur CODES
****** Premier replay
#+begin_src sh :results output :exports both
/usr/bin/time model-net-mpi-replay --sync=1 --num_net_traces=16 --workload_file=dumpi-2017.05.29.15.00.43- --workload_type=dumpi --lp-io-dir=amg-16-seq --lp-io-use-suffix=1 -- ../../logiciels/CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Mon May 29 15:08:42 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

GVT #0: simulation 1% complete, max event queue size 136 (GVT = 3000418690.7835).
AVL tree size: 0
GVT #0: simulation 2% complete, max event queue size 402 (GVT = 6035527191.2941).
AVL tree size: 0
GVT #0: simulation 3% complete, max event queue size 688 (GVT = 11483113110.0620).
AVL tree size: 0
GVT #0: simulation 4% complete, max event queue size 688 (GVT = 12009052862.3804).
AVL tree size: 0
GVT #0: simulation 5% complete, max event queue size 755 (GVT = 15041415977.3348).
AVL tree size: 0
GVT #0: simulation 6% complete, max event queue size 755 (GVT = 18002513250.3600).
AVL tree size: 0
GVT #0: simulation 7% complete, max event queue size 755 (GVT = 21000392946.9969).
AVL tree size: 0
GVT #0: simulation 8% complete, max event queue size 755 (GVT = 24001429862.3904).
AVL tree size: 0
GVT #0: simulation 9% complete, max event queue size 755 (GVT = 27016181539.3666).
AVL tree size: 0
GVT #0: simulation 10% complete, max event queue size 755 (GVT = 30000421301.8214).
AVL tree size: 0
GVT #0: simulation 11% complete, max event queue size 755 (GVT = 33000388626.2790).
AVL tree size: 0
GVT #0: simulation 12% complete, max event queue size 755 (GVT = 36000490201.6426).
AVL tree size: 0
 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 5195 receives 5196 collectives 165 delays 13765 wait alls 818 waits 0 send time 31049590.884462 wait 6896811063.124624
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 5320 receives 5320 collectives 165 delays 14014 wait alls 818 waits 0 send time 27406535.422290 wait 4902476557.148384
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 7528 receives 7578 collectives 165 delays 18495 wait alls 833 waits 0 send time 48394690.382042 wait 7307331091.361316
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 7513 receives 7497 collectives 165 delays 18384 wait alls 818 waits 0 send time 52232192.354479 wait 4480098136.370174
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 7725 receives 7733 collectives 165 delays 18832 wait alls 818 waits 0 send time 55760528.873564 wait 4495196328.271902
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 7224 receives 7206 collectives 165 delays 17804 wait alls 818 waits 0 send time 55449250.169176 wait 6989604606.631558
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 5134 receives 5124 collectives 165 delays 13593 wait alls 786 waits 0 send time 25451387.954232 wait 8660401684.884769
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 5011 receives 4998 collectives 165 delays 13383 wait alls 818 waits 0 send time 23487805.313354 wait 7571367496.128121
 LP 18 unmatched irecvs 0 unmatched sends 0 Total sends 4971 receives 4971 collectives 165 delays 13277 wait alls 786 waits 0 send time 27980908.753104 wait 5488817203.298100
 LP 19 unmatched irecvs 0 unmatched sends 0 Total sends 5156 receives 5146 collectives 165 delays 13676 wait alls 818 waits 0 send time 24128321.802325 wait 7623737540.569324
 LP 20 unmatched irecvs 0 unmatched sends 0 Total sends 6995 receives 6951 collectives 165 delays 17281 wait alls 786 waits 0 send time 46167576.805130 wait 4851373601.992508
 LP 21 unmatched irecvs 0 unmatched sends 0 Total sends 7635 receives 7685 collectives 165 delays 18709 wait alls 833 waits 0 send time 44505846.432115 wait 4975400608.077621
 LP 27 unmatched irecvs 0 unmatched sends 0 Total sends 7405 receives 7377 collectives 165 delays 18156 wait alls 818 waits 0 send time 47102605.814613 wait 4562468563.792100
 LP 28 unmatched irecvs 0 unmatched sends 0 Total sends 7655 receives 7700 collectives 165 delays 18744 wait alls 833 waits 0 send time 43722919.167827 wait 4438655118.330465
 LP 29 unmatched irecvs 0 unmatched sends 0 Total sends 5338 receives 5335 collectives 165 delays 14047 wait alls 818 waits 0 send time 28659629.223436 wait 7415602451.839598
 LP 30 unmatched irecvs 0 unmatched sends 0 Total sends 4415 receives 4403 collectives 165 delays 12090 wait alls 723 waits 0 send time 24836982.610907 wait 8429789123.096380
	: Running Time = 16.1525 seconds

TW Library Statistics:
	Total Events Processed                                20237447
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             6
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                  20237447
	Event Rate (events/sec)                              1252898.9
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        469247
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)     38.6761

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 564713492 recvd 564713492 
 max runtime 38171957179.288765 ns avg runtime 38076505282.498756 
 max comm time 8660526284.320320 avg comm time 6193220647.686256 
 max send time 55760528.873564 avg send time 37896048.247691 
 max recv time 36342589195.592293 avg recv time 21138164904.627800 
 max wait time 8660401684.884769 avg wait time 6193070698.432308 
LP-IO: writing output to amg-16-trace-21355-1496063322/
LP-IO: data files:
   amg-16-trace-21355-1496063322/dragonfly-router-traffic
   amg-16-trace-21355-1496063322/dragonfly-router-stats
   amg-16-trace-21355-1496063322/dragonfly-msg-stats
   amg-16-trace-21355-1496063322/model-net-category-all
   amg-16-trace-21355-1496063322/model-net-category-test
   amg-16-trace-21355-1496063322/mpi-replay-stats
 Average number of hops traversed 1.447529 average chunk latency 8.990396 us maximum chunk latency 129.941465 us avg message size 5634.738281 bytes finished messages 100220 finished chunks 2265893 

 ADAPTIVE ROUTING STATS: 2265893 chunks routed minimally 0 chunks routed non-minimally completed packets 2265893 

 Total packets generated 1168810 finished 1168810 
12.98user 2.14system 1:45.14elapsed 14%CPU (0avgtext+0avgdata 5154948maxresident)k
8160inputs+25600outputs (36major+1056367minor)pagefaults 0swaps
#+END_EXAMPLE
****** Second replay
#+BEGIN_EXAMPLE
Mon May 29 15:13:42 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

GVT #0: simulation 1% complete, max event queue size 136 (GVT = 3000418690.7835).
AVL tree size: 0
GVT #0: simulation 2% complete, max event queue size 402 (GVT = 6035527191.2941).
AVL tree size: 0
GVT #0: simulation 3% complete, max event queue size 688 (GVT = 11483113110.0620).
AVL tree size: 0
GVT #0: simulation 4% complete, max event queue size 688 (GVT = 12009052862.3804).
AVL tree size: 0
GVT #0: simulation 5% complete, max event queue size 755 (GVT = 15041415977.3348).
AVL tree size: 0
GVT #0: simulation 6% complete, max event queue size 755 (GVT = 18002513250.3600).
AVL tree size: 0
GVT #0: simulation 7% complete, max event queue size 755 (GVT = 21000392946.9969).
AVL tree size: 0
GVT #0: simulation 8% complete, max event queue size 755 (GVT = 24001429862.3904).
AVL tree size: 0
GVT #0: simulation 9% complete, max event queue size 755 (GVT = 27016181539.3666).
AVL tree size: 0
GVT #0: simulation 10% complete, max event queue size 755 (GVT = 30000421301.8214).
AVL tree size: 0
GVT #0: simulation 11% complete, max event queue size 755 (GVT = 33000388626.2790).
AVL tree size: 0
GVT #0: simulation 12% complete, max event queue size 755 (GVT = 36000490201.6426).
AVL tree size: 0
 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 5195 receives 5196 collectives 165 delays 13765 wait alls 818 waits 0 send time 31049590.884462 wait 6896811063.124624
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 5320 receives 5320 collectives 165 delays 14014 wait alls 818 waits 0 send time 27406535.422290 wait 4902476557.148384
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 7528 receives 7578 collectives 165 delays 18495 wait alls 833 waits 0 send time 48394690.382042 wait 7307331091.361316
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 7513 receives 7497 collectives 165 delays 18384 wait alls 818 waits 0 send time 52232192.354479 wait 4480098136.370174
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 7725 receives 7733 collectives 165 delays 18832 wait alls 818 waits 0 send time 55760528.873564 wait 4495196328.271902
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 7224 receives 7206 collectives 165 delays 17804 wait alls 818 waits 0 send time 55449250.169176 wait 6989604606.631558
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 5134 receives 5124 collectives 165 delays 13593 wait alls 786 waits 0 send time 25451387.954232 wait 8660401684.884769
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 5011 receives 4998 collectives 165 delays 13383 wait alls 818 waits 0 send time 23487805.313354 wait 7571367496.128121
 LP 18 unmatched irecvs 0 unmatched sends 0 Total sends 4971 receives 4971 collectives 165 delays 13277 wait alls 786 waits 0 send time 27980908.753104 wait 5488817203.298100
 LP 19 unmatched irecvs 0 unmatched sends 0 Total sends 5156 receives 5146 collectives 165 delays 13676 wait alls 818 waits 0 send time 24128321.802325 wait 7623737540.569324
 LP 20 unmatched irecvs 0 unmatched sends 0 Total sends 6995 receives 6951 collectives 165 delays 17281 wait alls 786 waits 0 send time 46167576.805130 wait 4851373601.992508
 LP 21 unmatched irecvs 0 unmatched sends 0 Total sends 7635 receives 7685 collectives 165 delays 18709 wait alls 833 waits 0 send time 44505846.432115 wait 4975400608.077621
 LP 27 unmatched irecvs 0 unmatched sends 0 Total sends 7405 receives 7377 collectives 165 delays 18156 wait alls 818 waits 0 send time 47102605.814613 wait 4562468563.792100
 LP 28 unmatched irecvs 0 unmatched sends 0 Total sends 7655 receives 7700 collectives 165 delays 18744 wait alls 833 waits 0 send time 43722919.167827 wait 4438655118.330465
 LP 29 unmatched irecvs 0 unmatched sends 0 Total sends 5338 receives 5335 collectives 165 delays 14047 wait alls 818 waits 0 send time 28659629.223436 wait 7415602451.839598
 LP 30 unmatched irecvs 0 unmatched sends 0 Total sends 4415 receives 4403 collectives 165 delays 12090 wait alls 723 waits 0 send time 24836982.610907 wait 8429789123.096380
	: Running Time = 11.4814 seconds

TW Library Statistics:
	Total Events Processed                                20237447
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             6
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                  20237447
	Event Rate (events/sec)                              1762632.1
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        469247
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)     27.4914

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 564713492 recvd 564713492 
 max runtime 38171957179.288765 ns avg runtime 38076505282.498756 
 max comm time 8660526284.320320 avg comm time 6193220647.686256 
 max send time 55760528.873564 avg send time 37896048.247691 
 max recv time 36342589195.592293 avg recv time 21138164904.627800 
 max wait time 8660401684.884769 avg wait time 6193070698.432308 
LP-IO: writing output to amg-16-seq-21656-1496063622/
LP-IO: data files:
   amg-16-seq-21656-1496063622/dragonfly-router-traffic
   amg-16-seq-21656-1496063622/dragonfly-router-stats
   amg-16-seq-21656-1496063622/dragonfly-msg-stats
   amg-16-seq-21656-1496063622/model-net-category-all
   amg-16-seq-21656-1496063622/model-net-category-test
   amg-16-seq-21656-1496063622/mpi-replay-stats
 Average number of hops traversed 1.447529 average chunk latency 8.990396 us maximum chunk latency 129.941465 us avg message size 5634.738281 bytes finished messages 100220 finished chunks 2265893 

 ADAPTIVE ROUTING STATS: 2265893 chunks routed minimally 0 chunks routed non-minimally completed packets 2265893 

 Total packets generated 1168810 finished 1168810 
12.25user 1.30system 0:19.16elapsed 70%CPU (0avgtext+0avgdata 5137592maxresident)k
37880inputs+25600outputs (27major+973699minor)pagefaults 0swaps
#+END_EXAMPLE
****** Commentaires
- La première execution
  - Temps d'execution : 1 minute 45 secondes
  - Running time : 16 secondes
- Seconde execution
  - Temps d'execution : 19 secondes
  - Running time : 11 secondes

Il y a donc une variabilité énorme entre ces deux éxécutions ! Peut
être qu'il y avait encore des choses dans les caches/RAM. Je vais donc
en lancer une troisième après un temps d'attente de quelques minutes

****** Troisième replay

#+begin_src sh :results output :exports both
/usr/bin/time model-net-mpi-replay --sync=1 --num_net_traces=16 --workload_file=dumpi-2017.05.29.15.00.43- --workload_type=dumpi --lp-io-dir=amg-16-seq --lp-io-use-suffix=1 -- ../../logiciels/CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf
#+end_src
#+Results:
#+BEGIN_EXAMPLE
Mon May 29 15:36:59 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

GVT #0: simulation 1% complete, max event queue size 136 (GVT = 3000418690.7835).
AVL tree size: 0
GVT #0: simulation 2% complete, max event queue size 402 (GVT = 6035527191.2941).
AVL tree size: 0
GVT #0: simulation 3% complete, max event queue size 688 (GVT = 11483113110.0620).
AVL tree size: 0
GVT #0: simulation 4% complete, max event queue size 688 (GVT = 12009052862.3804).
AVL tree size: 0
GVT #0: simulation 5% complete, max event queue size 755 (GVT = 15041415977.3348).
AVL tree size: 0
GVT #0: simulation 6% complete, max event queue size 755 (GVT = 18002513250.3600).
AVL tree size: 0
GVT #0: simulation 7% complete, max event queue size 755 (GVT = 21000392946.9969).
AVL tree size: 0
GVT #0: simulation 8% complete, max event queue size 755 (GVT = 24001429862.3904).
AVL tree size: 0
GVT #0: simulation 9% complete, max event queue size 755 (GVT = 27016181539.3666).
AVL tree size: 0
GVT #0: simulation 10% complete, max event queue size 755 (GVT = 30000421301.8214).
AVL tree size: 0
GVT #0: simulation 11% complete, max event queue size 755 (GVT = 33000388626.2790).
AVL tree size: 0
GVT #0: simulation 12% complete, max event queue size 755 (GVT = 36000490201.6426).
AVL tree size: 0
 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 5195 receives 5196 collectives 165 delays 13765 wait alls 818 waits 0 send time 31049590.884462 wait 6896811063.124624
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 5320 receives 5320 collectives 165 delays 14014 wait alls 818 waits 0 send time 27406535.422290 wait 4902476557.148384
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 7528 receives 7578 collectives 165 delays 18495 wait alls 833 waits 0 send time 48394690.382042 wait 7307331091.361316
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 7513 receives 7497 collectives 165 delays 18384 wait alls 818 waits 0 send time 52232192.354479 wait 4480098136.370174
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 7725 receives 7733 collectives 165 delays 18832 wait alls 818 waits 0 send time 55760528.873564 wait 4495196328.271902
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 7224 receives 7206 collectives 165 delays 17804 wait alls 818 waits 0 send time 55449250.169176 wait 6989604606.631558
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 5134 receives 5124 collectives 165 delays 13593 wait alls 786 waits 0 send time 25451387.954232 wait 8660401684.884769
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 5011 receives 4998 collectives 165 delays 13383 wait alls 818 waits 0 send time 23487805.313354 wait 7571367496.128121
 LP 18 unmatched irecvs 0 unmatched sends 0 Total sends 4971 receives 4971 collectives 165 delays 13277 wait alls 786 waits 0 send time 27980908.753104 wait 5488817203.298100
 LP 19 unmatched irecvs 0 unmatched sends 0 Total sends 5156 receives 5146 collectives 165 delays 13676 wait alls 818 waits 0 send time 24128321.802325 wait 7623737540.569324
 LP 20 unmatched irecvs 0 unmatched sends 0 Total sends 6995 receives 6951 collectives 165 delays 17281 wait alls 786 waits 0 send time 46167576.805130 wait 4851373601.992508
 LP 21 unmatched irecvs 0 unmatched sends 0 Total sends 7635 receives 7685 collectives 165 delays 18709 wait alls 833 waits 0 send time 44505846.432115 wait 4975400608.077621
 LP 27 unmatched irecvs 0 unmatched sends 0 Total sends 7405 receives 7377 collectives 165 delays 18156 wait alls 818 waits 0 send time 47102605.814613 wait 4562468563.792100
 LP 28 unmatched irecvs 0 unmatched sends 0 Total sends 7655 receives 7700 collectives 165 delays 18744 wait alls 833 waits 0 send time 43722919.167827 wait 4438655118.330465
 LP 29 unmatched irecvs 0 unmatched sends 0 Total sends 5338 receives 5335 collectives 165 delays 14047 wait alls 818 waits 0 send time 28659629.223436 wait 7415602451.839598
 LP 30 unmatched irecvs 0 unmatched sends 0 Total sends 4415 receives 4403 collectives 165 delays 12090 wait alls 723 waits 0 send time 24836982.610907 wait 8429789123.096380
	: Running Time = 12.3117 seconds

TW Library Statistics:
	Total Events Processed                                20237447
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             6
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                  20237447
	Event Rate (events/sec)                              1643760.4
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        469247
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)     29.4795

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 564713492 recvd 564713492 
 max runtime 38171957179.288765 ns avg runtime 38076505282.498756 
 max comm time 8660526284.320320 avg comm time 6193220647.686256 
 max send time 55760528.873564 avg send time 37896048.247691 
 max recv time 36342589195.592293 avg recv time 21138164904.627800 
 max wait time 8660401684.884769 avg wait time 6193070698.432308 
LP-IO: writing output to amg-16-seq-23159-1496065019/
LP-IO: data files:
   amg-16-seq-23159-1496065019/dragonfly-router-traffic
   amg-16-seq-23159-1496065019/dragonfly-router-stats
   amg-16-seq-23159-1496065019/dragonfly-msg-stats
   amg-16-seq-23159-1496065019/model-net-category-all
   amg-16-seq-23159-1496065019/model-net-category-test
   amg-16-seq-23159-1496065019/mpi-replay-stats
 Average number of hops traversed 1.447529 average chunk latency 8.990396 us maximum chunk latency 129.941465 us avg message size 5634.738281 bytes finished messages 100220 finished chunks 2265893 

 ADAPTIVE ROUTING STATS: 2265893 chunks routed minimally 0 chunks routed non-minimally completed packets 2265893 

 Total packets generated 1168810 finished 1168810 
13.02user 1.63system 0:21.47elapsed 68%CPU (0avgtext+0avgdata 5121596maxresident)k
36856inputs+25600outputs (31major+985685minor)pagefaults 0swaps

#+END_EXAMPLE
****** Commentaire
C'est donc de l'ordre de 15-20 secondes. Il se peut que lors de la
première execution, des données aient été transféré sur le swap (qui
est particulièrement lent sur mon ordinateur). Mais le *running time*
est toujours inferieur. Je pense donc que c'est le temps d'execution
de la simulation, et qu'ensuite, les statistiques sont extraites des
buffer/fichier.bin .
****** Vérification
On peut cependant pas se permettre des suppositions, du coup je vais
faire un tour dans le code. L'affichage est fait via la fonction
=tw_stats= définie dans le fichier /tw-stats.c/ .
On voit que l'information fait partie d'une structure de type
=tw_statistics= qui contient tout un tas de choses, notamment le double
=s_max_run_time= .
Voici le @brief de la structure 
#+BEGIN_EXAMPLE
  * tw_statistics
  * @brief Statistics tallied over the duration of the simulation.
#+END_EXAMPLE

Après recherche, il s'avère que le Running Time affiché est le temps
d'exécution maximum des PE (parmis les PEs ont affiche le temps
d'execution du plus long).
#+BEGIN_EXAMPLE
tw_wtime start_time; /**< @brief When this PE first started execution */
tw_wtime end_time; /**< @brief When this PE finished its execution */
#+END_EXAMPLE
#+BEGIN_EXAMPLE
...
for(pe = NULL; (pe = tw_pe_next(pe));)
	{
		tw_wtime rt;

		tw_wall_sub(&rt, &pe->end_time, &pe->start_time);

		s->s_max_run_time = ROSS_MAX(s->s_max_run_time, tw_wall_to_double(&rt));
...
#+END_EXAMPLE

*** 2017-05-30 mardi
**** Mail Arnaud
***** Content
Salut,

  my two cents of the day.

> Je pense que j'y suis presque, mais je n'arrive pas à linker la
> librairie...

#+BEGIN_EXAMPLE
configure:17252: checking wheter OTF library and header file can be found
configure:17277: mpicc -o conftest -DMPICH_SUPPRESS_PROTOTYPES=1
-DHAVE_PRAGMA_HP_SEC_DEF=1 -pthread -I/usr/local/include/freetype2
-I/usr/include  -L/usr/local/lib -lfreetype -L/usr/lib/x86_64-linux-gnu
-lotf conftest.c  -lotf -lz >&5
conftest.c: In function 'main':
conftest.c:35:4: error: unknown type name 'OTF_FileManager'
    OTF_FileManager *mgr = NULL;
    ^~~~~~~~~~~~~~~
#+END_EXAMPLE

Ça, c'est un problème d'include, pas un problème de link. Ce
"conftest.c" généré par configure fait un #include <otf.h> (d'après ce
que je peux lire dans ton extrait du configure de mercredi dernier) et
ton mpicc le cherche dans =/usr/local/lib= et dans
=/usr/lib/x86_64-linux-gnu=. Est-ce qu'il y a un tel fichier otf.h dans
ces chemins là ? Est-ce que quand tu l'ouvres tu y trouves traces de
=OTF_FileManager= ?

> C'est donc de l'ordre de 15-20 secondes. Il se peut que lors de la
> première execution, des données aient été transféré sur le swap (qui
> est particulièrement lent sur mon ordinateur). 

Oui, une minute 45 seconde d'exécution, par rapport à 20 secondes,
c'est un truc de fou. Je ne sais pas ce qu'il s'est passé mais je ne
pense pas que ça soit le swap. time t'indique "0swaps". C'est assez
pénible de connaître le consommation mémoire réelle des processus, je
t'expliquerai à mon retour comment faire s'il y a besoin. En
attendant, je te suggère de surveiller ton occupation mémoire pendant
les exécutions en lançant htop. Si tu swappes, tu le verras tout de
suite.

Tu verras aussi si tu utilises plusieurs coeurs... Là, time t'indique
un %CPU inférieur à 100, ce qui signifie qu'il n'arrive pas à utiliser
un coeur 100% du temps. Alors, parfois, c'est compliqué à mesurer si
tu fais time toto et que toto lance des processus qui occupent le CPU
et que toto attends gentiment qu'ils aient fini, time va passer
complètement à côté. Mais là, visiblement, ça model-net-mpi-replay
bosse. Mais avec un seul coeur à mon avis, d'où une efficacité
annoncée de 100%... :) Tu devrais essayer de changer ce "--sync=1" en
"--sync=4" pour voir.

> Mais le *running time* est toujours inferieur. Je pense donc que c'est
> le temps d'execution de la simulation, et qu'ensuite, les
> statistiques sont extraites des buffer/fichier.bin.

Sinon, c'est bien d'avoir regardé le code pour comprendre ce qu'il
affichait exactement.

A+
    Arnaud
***** Talk to me
Ok, je vais regarder pour OTF, mais le fichier =otf.h= se trouve :
- =./home/chevamax/Documents/Stage_LIG_2017/logiciels/libotf-0.9.13/src/otf.h=
- =./usr/local/include/otf.h=
- =./usr/include/otf.h=
D'où mon rajout de l'option -I/usr/include

En effet, pour le swap je me suis mal exprimé, je pense que se sont
mes autres applications ouvertent qui sont passé dans le swap pour
libérer de la RAM (j'ai observé une augmentation du swap sur le
moniteur system). Je vais donc également observer avec HTOP.
**** OTF
Effectivement le fichier otf.h ne contient pas la definition vers ce
type. J'ai du installer une autre librairie...
J'ai donc installé OTF, mais le deuxieme du nom, qui n'est donc pas
compatible avec DUMPI. Je tente avec la version 1.2.18. http://www.paratools.com/otf/
Tout est installé est c'est de la bonne forme (otf.h et pas otf2.h).
Lien de téléchargement : 
Cependant, j'ai maintenant l'erreur suivante :
#+BEGIN_EXAMPLE
configure:17252: checking wheter OTF library and header file can be found
configure:17277: mpicc -o conftest -DMPICH_SUPPRESS_PROTOTYPES=1 -DHAVE_PRAGMA_HP_SEC_DEF=1 -pthread  -I/usr/local/include -L/usr/local/lib   conftest.c  -lotf -lz >&5
/tmp/ccRnDXS4.o: In function `main':
conftest.c:(.text+0x16): undefined reference to `OTF_FileManager_open'
conftest.c:(.text+0x34): undefined reference to `OTF_FileManager_close'
collect2: error: ld returned 1 exit status
#+END_EXAMPLE

Or ces fonction sont définie dans des fichiers .h au niveau
=/usr/local/include= qui sont appelé par des includes successifs via le
fichier =otf.h= . La librairie est elle dans =/usr/local/lib/libotf.a=

Je lance le configure via :
#+BEGIN_EXAMPLE
 ../configure --enable-libdumpi --enable-test --prefix=/home/chevamax/Documents/Stage_LIG_2017/logiciels/dumpiOTF/sst-dumpi/install CC=mpicc CXX=mpiCC CFLAGS="-DMPICH_SUPPRESS_PROTOTYPES=1 -DHAVE_PRAGMA_HP_SEC_DEF=1 -pthread  -I/usr/local/include -L/usr/local/lib"
#+END_EXAMPLE

J'ai réussi à installer correctement la lib sur ma machine virtuelle
(qui est clean), et a faire fonctionner le petit exemple. 

Après avoir clean mon pc par rapport à OTF et OTF2, j'ai réinstallé la
lib et fait tourné l'exemple.

En lancant la meme commande, c'est OK.

J'ai pus convertir les traces dumpi en otf. Maintenant faut voir si on
peut les utiliser.

Je vais tenter avec l'outil Visual Trace Explorer.
Ca fonctionne pas avec cet outil :
#+BEGIN_EXAMPLE
...
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : DefCollOp definition is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
0 : CollOp event is actually not handled in ViTE
...
#+END_EXAMPLE
Deux options : 
- soit le parser fonctionne pas.
- soit la version OTF n'est pas prise en charge par VITE (mais j'en
  doute)

**** Replay sur CODES                                              :CODES:
***** Premier replay --synch=4
#+begin_src sh :results output :exports both
/usr/bin/time model-net-mpi-replay --sync=4 --num_net_traces=16 --workload_file=dumpi-2017.05.29.15.00.43- --workload_type=dumpi --lp-io-dir=amg-16-par-debug --lp-io-use-suffix=1 -- ../../logiciels/CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
model-net-mpi-replay --sync=4 --num_net_traces=16 --workload_file=dumpi-2017.05.29.15.00.43- --workload_type=dumpi --lp-io-dir=amg-16-par-debug --lp-io-use-suffix=1 -- ../../logiciels/CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf 

Tue May 30 10:51:29 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (1)] 1
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

/***************************************************************************/
/***** WARNING: Starting Optimistic Debug Scheduler!! **********************/
This schedule assumes the following: 
 1) One 1 Processor/Core is used.
 2) One 1 KP is used.
    NOTE: use the --nkp=1 argument to the simulation to ensure that
          it only uses 1 KP.
 3) Events ARE NEVER RECLAIMED (LP Commit Functions are not called).
 4) Executes til out of memory (16 events left) and 
    injects rollback to first before primodal init event.
 5) g_tw_rollback_time = 0.000000001000 
/***************************************************************************/
/******************* Starting Rollback Phase ******************************/
/******************* Completed Rollback Phase ******************************/
 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 18 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 19 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 20 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 21 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 27 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 28 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 29 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 30 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 0 collectives 0 delays 0 wait alls 0 waits 0 send time 0.000000 wait 0.000000
	: Running Time = 0.6377 seconds

TW Library Statistics:
	Total Events Processed                                  657157
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                      657157
	Event Ties Detected in PE Queues                             0
	Efficiency                                                -inf %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     -nan %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     -nan %

	Total Roll Backs                                             1
	Primary Roll Backs                                           1
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                         0
	Event Rate (events/sec)                                    0.0
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        479985
	Memory Wasted                                               43

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.0000

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 0 recvd 0 
 max runtime 0.000000 ns avg runtime 0.000000 
 max comm time 0.000000 avg comm time 0.000000 
 max send time 0.000000 avg send time 0.000000 
 max recv time 0.000000 avg recv time 0.000000 
 max wait time 0.000000 avg wait time 0.000000 
LP-IO: writing output to amg-16-par-debug-4472-1496134289/
LP-IO: data files:
   amg-16-par-debug-4472-1496134289/dragonfly-router-traffic
   amg-16-par-debug-4472-1496134289/dragonfly-router-stats
   amg-16-par-debug-4472-1496134289/dragonfly-msg-stats
   amg-16-par-debug-4472-1496134289/model-net-category-all
   amg-16-par-debug-4472-1496134289/model-net-category-test
   amg-16-par-debug-4472-1496134289/mpi-replay-stats
 Average number of hops traversed -nan average chunk latency -nan us maximum chunk latency 0.000000 us avg message size -nan bytes finished messages 0 finished chunks 0 

 ADAPTIVE ROUTING STATS: 0 chunks routed minimally 0 chunks routed non-minimally completed packets 0 

 Total packets generated 0 finished 0 
1.41user 1.34system 0:02.82elapsed 97%CPU (0avgtext+0avgdata 5082308maxresident)k
0inputs+11112outputs (0major+1138863minor)pagefaults 0swaps
#+END_EXAMPLE
****** Commentaires
Bon et bien ca ne fonctionne pas. Dans la documentation de CODES, ils
ne parlent que des =sync=3= et =sync=1=  
***** Second replay --synch=3
#+begin_src sh :results output :exports both
/usr/bin/time mpirun -np 4 model-net-mpi-replay --sync=3 --num_net_traces=16 --workload_file=dumpi-2017.05.29.15.00.43- --workload_type=dumpi -- ../../logiciels/CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Tue May 30 11:07:41 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            152065
	Network events                                           50000
	Total events                                            202064

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #269: simulation 1% complete, max event queue size 136 (GVT = 3001388599.2208).
AVL tree size: 0
GVT #5003: simulation 2% complete, max event queue size 402 (GVT = 6088586397.1089).
AVL tree size: 0
GVT #13886: simulation 3% complete, max event queue size 688 (GVT = 11483170310.2218).
AVL tree size: 0
GVT #13924: simulation 4% complete, max event queue size 688 (GVT = 12009128760.0480).
AVL tree size: 0
GVT #27424: simulation 5% complete, max event queue size 755 (GVT = 15073084855.9814).
AVL tree size: 0
GVT #39200: simulation 6% complete, max event queue size 755 (GVT = 18002545337.9855).
AVL tree size: 0
GVT #45757: simulation 7% complete, max event queue size 755 (GVT = 21003132908.7307).
AVL tree size: 0
GVT #52120: simulation 8% complete, max event queue size 755 (GVT = 24001513016.8279).
AVL tree size: 0
GVT #58486: simulation 9% complete, max event queue size 755 (GVT = 27016203755.9683).
AVL tree size: 0
GVT #64968: simulation 10% complete, max event queue size 755 (GVT = 30005142406.1074).
AVL tree size: 0
GVT #69938: simulation 11% complete, max event queue size 755 (GVT = 33023577444.8782).
AVL tree size: 0
GVT #75505: simulation 12% complete, max event queue size 755 (GVT = 36000754131.1067).
AVL tree size: 0
GVT #79052: simulation 100% complete, max event queue size 755 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 5195 receives 5196 collectives 165 delays 13765 wait alls 818 waits 0 send time 31049590.884462 wait 6896811063.124624
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 5320 receives 5320 collectives 165 delays 14014 wait alls 818 waits 0 send time 27406535.422290 wait 4902476557.148384
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 7528 receives 7578 collectives 165 delays 18495 wait alls 833 waits 0 send time 48394690.382042 wait 7307331091.361316
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 7513 receives 7497 collectives 165 delays 18384 wait alls 818 waits 0 send time 52232192.354479 wait 4480098136.370174
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 7725 receives 7733 collectives 165 delays 18832 wait alls 818 waits 0 send time 55760528.873564 wait 4495196328.271902
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 7224 receives 7206 collectives 165 delays 17804 wait alls 818 waits 0 send time 55449250.169176 wait 6989604606.631558
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 5134 receives 5124 collectives 165 delays 13593 wait alls 786 waits 0 send time 25451387.954232 wait 8660401684.884769
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 5011 receives 4998 collectives 165 delays 13383 wait alls 818 waits 0 send time 23487805.313354 wait 7571367496.128121
 LP 18 unmatched irecvs 0 unmatched sends 0 Total sends 4971 receives 4971 collectives 165 delays 13277 wait alls 786 waits 0 send time 27980908.753104 wait 5488817203.298100
 LP 19 unmatched irecvs 0 unmatched sends 0 Total sends 5156 receives 5146 collectives 165 delays 13676 wait alls 818 waits 0 send time 24128321.802325 wait 7623737540.569324
 LP 20 unmatched irecvs 0 unmatched sends 0 Total sends 6995 receives 6951 collectives 165 delays 17281 wait alls 786 waits 0 send time 46167576.805130 wait 4851373601.992508
 LP 21 unmatched irecvs 0 unmatched sends 0 Total sends 7635 receives 7685 collectives 165 delays 18709 wait alls 833 waits 0 send time 44505846.432115 wait 4975400608.077621
 LP 27 unmatched irecvs 0 unmatched sends 0 Total sends 7405 receives 7377 collectives 165 delays 18156 wait alls 818 waits 0 send time 47102605.814613 wait 4562468563.792100
 LP 28 unmatched irecvs 0 unmatched sends 0 Total sends 7655 receives 7700 collectives 165 delays 18744 wait alls 833 waits 0 send time 43722919.167827 wait 4438655118.330465
 LP 29 unmatched irecvs 0 unmatched sends 0 Total sends 5338 receives 5335 collectives 165 delays 14047 wait alls 818 waits 0 send time 28659629.223436 wait 7415602451.839598
 LP 30 unmatched irecvs 0 unmatched sends 0 Total sends 4415 receives 4403 collectives 165 delays 12090 wait alls 723 waits 0 send time 24836982.610907 wait 8429789123.096380
	: Running Time = 24.7522 seconds

TW Library Statistics:
	Total Events Processed                                20237447
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             6
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                 316212
	Total GVT Computations                                   79053

	Net Events Processed                                  20237447
	Event Rate (events/sec)                               817602.4
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        202065
	Memory Allocated                                        165437
	Memory Wasted                                              486

TW Network Statistics:
	Remote sends                                                 0
	Remote recvs                                                 0

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                2.3156
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                       44.7163
	Event Cancel                                            0.0440
	Event Abort                                             0.0000

	GVT                                                    59.0665
	Fossil Collect                                          1.8686
	Primary Rollbacks                                       0.0000
	Network Read                                            1.9920
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)     59.2676

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                   79053
	Total All Reduce Calls                                  158106
	Average Reduction / GVT                                   2.00

 Total bytes sent 564713492 recvd 564713492 
 max runtime 38171957179.288765 ns avg runtime 38076505282.498756 
 max comm time 8660526284.320320 avg comm time 6193220647.686256 
 max send time 55760528.873564 avg send time 37896048.247691 
 max recv time 36342589195.592293 avg recv time 21138164904.627800 
 max wait time 8660401684.884769 avg wait time 6193070698.432308 
 Average number of hops traversed 1.447529 average chunk latency 8.990396 us maximum chunk latency 129.941465 us avg message size 5634.738281 bytes finished messages 100220 finished chunks 2265893 

 ADAPTIVE ROUTING STATS: 2265893 chunks routed minimally 0 chunks routed non-minimally completed packets 2265893 

 Total packets generated 1168810 finished 1168810 
98.50user 4.24system 0:26.56elapsed 386%CPU (0avgtext+0avgdata 1401848maxresident)k
0inputs+17256outputs (0major+1196539minor)pagefaults 0swaps

#+END_EXAMPLE

****** Commentaires
J'ai un apriori sur le déroulement de la simulation. Je trouve étrange
que GTV annonce qu'il est à 12 %, avancant de % en %, et qu'il saute à 100%.

Pour l'efficacité à 100% je sais toujours pas le calcul, je vais aller
voir dans le code.

J'ai également remarqué qu'avec un nombre de process supérieur à mon
nombre de coeurs, la simulation devient très lente (dès mpirun -np
5 ...).

***** Troisieme replay --synch=3
#+begin_src sh :results output :exports both
/usr/bin/time mpirun -np 5 model-net-mpi-replay --sync=3 --num_net_traces=16 --workload_file=dumpi-2017.05.29.15.00.43- --workload_type=dumpi -- ../../logiciels/CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Tue May 30 11:38:38 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 5 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  5
	Total Processors                                   [Nodes (5) x PE_per_Node (1)] 5
	Total KPs                                          [Nodes (5) x KPs (16)] 80
	Total LPs                                                 2380
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            121857
	Network events                                           50000
	Total events                                            171856

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #269: simulation 1% complete, max event queue size 136 (GVT = 3001388599.2208).
AVL tree size: 0
GVT #5003: simulation 2% complete, max event queue size 402 (GVT = 6088586397.1089).
AVL tree size: 0
GVT #13886: simulation 3% complete, max event queue size 688 (GVT = 11483170310.2218).
AVL tree size: 0
GVT #13924: simulation 4% complete, max event queue size 688 (GVT = 12009128760.0480).
AVL tree size: 0
GVT #27424: simulation 5% complete, max event queue size 755 (GVT = 15073084855.9814).
AVL tree size: 0
GVT #39200: simulation 6% complete, max event queue size 755 (GVT = 18002545337.9855).
AVL tree size: 0
GVT #45757: simulation 7% complete, max event queue size 755 (GVT = 21003132908.7307).
AVL tree size: 0
GVT #52120: simulation 8% complete, max event queue size 755 (GVT = 24001513016.8279).
AVL tree size: 0
GVT #58486: simulation 9% complete, max event queue size 755 (GVT = 27016203755.9683).
AVL tree size: 0
GVT #64968: simulation 10% complete, max event queue size 755 (GVT = 30005142406.1074).
AVL tree size: 0
GVT #69938: simulation 11% complete, max event queue size 755 (GVT = 33023577444.8782).
AVL tree size: 0
GVT #75505: simulation 12% complete, max event queue size 755 (GVT = 36000754131.1067).
AVL tree size: 0
GVT #79052: simulation 100% complete, max event queue size 755 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 5195 receives 5196 collectives 165 delays 13765 wait alls 818 waits 0 send time 31049590.884462 wait 6896811063.124624
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 5320 receives 5320 collectives 165 delays 14014 wait alls 818 waits 0 send time 27406535.422290 wait 4902476557.148384
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 7528 receives 7578 collectives 165 delays 18495 wait alls 833 waits 0 send time 48394690.382042 wait 7307331091.361316
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 7513 receives 7497 collectives 165 delays 18384 wait alls 818 waits 0 send time 52232192.354479 wait 4480098136.370174
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 7725 receives 7733 collectives 165 delays 18832 wait alls 818 waits 0 send time 55760528.873564 wait 4495196328.271902
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 7224 receives 7206 collectives 165 delays 17804 wait alls 818 waits 0 send time 55449250.169176 wait 6989604606.631558
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 5134 receives 5124 collectives 165 delays 13593 wait alls 786 waits 0 send time 25451387.954232 wait 8660401684.884769
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 5011 receives 4998 collectives 165 delays 13383 wait alls 818 waits 0 send time 23487805.313354 wait 7571367496.128121
 LP 18 unmatched irecvs 0 unmatched sends 0 Total sends 4971 receives 4971 collectives 165 delays 13277 wait alls 786 waits 0 send time 27980908.753104 wait 5488817203.298100
 LP 19 unmatched irecvs 0 unmatched sends 0 Total sends 5156 receives 5146 collectives 165 delays 13676 wait alls 818 waits 0 send time 24128321.802325 wait 7623737540.569324
 LP 20 unmatched irecvs 0 unmatched sends 0 Total sends 6995 receives 6951 collectives 165 delays 17281 wait alls 786 waits 0 send time 46167576.805130 wait 4851373601.992508
 LP 21 unmatched irecvs 0 unmatched sends 0 Total sends 7635 receives 7685 collectives 165 delays 18709 wait alls 833 waits 0 send time 44505846.432115 wait 4975400608.077621
 LP 27 unmatched irecvs 0 unmatched sends 0 Total sends 7405 receives 7377 collectives 165 delays 18156 wait alls 818 waits 0 send time 47102605.814613 wait 4562468563.792100
 LP 28 unmatched irecvs 0 unmatched sends 0 Total sends 7655 receives 7700 collectives 165 delays 18744 wait alls 833 waits 0 send time 43722919.167827 wait 4438655118.330465
 LP 29 unmatched irecvs 0 unmatched sends 0 Total sends 5338 receives 5335 collectives 165 delays 14047 wait alls 818 waits 0 send time 28659629.223436 wait 7415602451.839598
 LP 30 unmatched irecvs 0 unmatched sends 0 Total sends 4415 receives 4403 collectives 165 delays 12090 wait alls 723 waits 0 send time 24836982.610907 wait 8429789123.096380
	: Running Time = 2242.5680 seconds

TW Library Statistics:
	Total Events Processed                                20237447
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             6
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                 395265
	Total GVT Computations                                   79053

	Net Events Processed                                  20237447
	Event Rate (events/sec)                                 9024.2
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        171857
	Memory Allocated                                        144112
	Memory Wasted                                              571

TW Network Statistics:
	Remote sends                                                 0
	Remote recvs                                                 0

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    720

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                3.0818
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                       59.4017
	Event Cancel                                            0.0586
	Event Abort                                             0.0000

	GVT                                                  5369.0031
	Fossil Collect                                          5.2655
	Primary Rollbacks                                       0.0000
	Network Read                                            3.2991
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)   5369.6879

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                   79053
	Total All Reduce Calls                                  158106
	Average Reduction / GVT                                   2.00

 Total bytes sent 564713492 recvd 564713492 
 max runtime 38171957179.288765 ns avg runtime 38076505282.498756 
 max comm time 8660526284.320320 avg comm time 6193220647.686256 
 max send time 55760528.873564 avg send time 37896048.247691 
 max recv time 36342589195.592293 avg recv time 21138164904.627800 
 max wait time 8660401684.884769 avg wait time 6193070698.432308 
 Average number of hops traversed 1.447529 average chunk latency 8.990396 us maximum chunk latency 129.941465 us avg message size 5634.738281 bytes finished messages 100220 finished chunks 2265893 

 ADAPTIVE ROUTING STATS: 2265893 chunks routed minimally 0 chunks routed non-minimally completed packets 2265893 

 Total packets generated 1168810 finished 1168810 
8641.83user 160.24system 37:24.45elapsed 392%CPU (0avgtext+0avgdata 1175872maxresident)k
0inputs+16728outputs (0major+1180704minor)pagefaults 0swaps
#+END_EXAMPLE

On voit ici avec np=5 que la simulation dure 37 minutes !
L'efficacité ne change pas.

***** Efficacité                                             :CODES:ROSS:
L'efficacité est calculée par :
#+BEGIN_EXAMPLE
100.0 * (1.0 - ((double) s.s_e_rbs / (double) s.s_net_events))
#+END_EXAMPLE

Avec 
- =long s_e_rbs; /**< @brief Number of events rolled back by this LP */=
- =s.s_net_events= le nombre total d'évènements de type "net" (réseau)

Du coup ca permet de voir rapidement s'il n'y a pas eu trop de
rollback.

***** Commentaire
Je remarque une différence en nombre d'event du model entre les
version synch=3, synch=1 et selon le nombre de np. Je comprend pas
encore pourquoi ca change, mais je vais regarder ca.

*** 2017-05-31 mercredi
**** Mail Arnaud
***** Content
Salut,
  donc si je comprends bien, tu obtiens:

- En séquentiel:
  #+begin_src sh :results output :exports both
  /usr/bin/time model-net-mpi-replay --sync=1 --num_net_traces=16
--workload_file=dumpi-2017.05.29.15.00.43- --workload_type=dumpi
--lp-io-dir=amg-16-seq --lp-io-use-suffix=1 --
../../logiciels/CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf
  
  : 13.02user 1.63system 0:21.47elapsed 68%CPU (0avgtext+0avgdata 5121596maxresident)k
  #+end_src
- En parallèle:
  #+begin_src sh :results output :exports both
  /usr/bin/time mpirun -np 4 model-net-mpi-replay --sync=3
--num_net_traces=16 --workload_file=dumpi-2017.05.29.15.00.43-
--workload_type=dumpi --
../../logiciels/CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf
: 98.50user 4.24system 0:26.56elapsed 386%CPU (0avgtext+0avgdata 1401848maxresident)k
  #+end_src

Ça a l'air d'être le même workload en entrée et les mêmes sorties sauf
que la version séquentielle prend 21 secondes en occupant peu le CPU
alors que la version parallèle prend 26 seconde en occupant bien les 4
coeurs. C'est normal, c'est souvent comme ça les codes
parallèles... :) Euh, c'est bien exécuté sur une vraie machine et pas
dans une VM tout ça, hein ?

Il faudra essayer d'obtenir une trace un peu plus grosse pour voir si
ça génère plus de parallélisme et si la version parallèle fini par
aller plus vite que la version séquentielle...

Sinon, l'effort de visualisation n'est pas vain du tout. C'est
cool. Persévère.

A+
    Arnaud
***** Commentaires
Oui c'est bien ca! Tout est exécuté sur ma vraie machine, quand je
parle de la virtuelle c'est plutot pour faire les configurations sur
une machine "clean" (vu que j'installe beaucoups de chose sur mon
linux et que comme avec OTF ca peut faire des conflits).

Pour l'OTF, je vais continuer de regarder.

Je suis d'accord pour la trace, mais avant ça je voulais regarder un
peut plus en détail AMG (et ses paramètres d'entrée) pour ne pas
lancer au hasard. J'ai également fait un récap des simulations que
j'ai lancé sur une feuille, je met ca dans le journal rapidement.

Je vais aussi regarder pour avoir un autre PC, afin de pouvoir faire
tourner des simulations et continuer à avancer sur ce PC.

J'ai également recu un mail de réponse pour HPL je met le contenu
ci-dessous.

***** HPL mailing list                                  :CODES:DUMPI:HPL:
Hi Maxime,

Thanks for your message. There seems to be a data type that is either
not supported by DUMPI or CODES. Are you familiar with what data types
are being used by the HPL trace? I will find out if the support for
them can be added in the code. 

Regards,
Misbah

****** Commentaire
Je vais donc aussi me renseigner sur ce format de trace, en
commencant par demander à Christian.

**** Objectifs
- [ ] Se renseigner sur AMG
- [ ] Demander un PC
- [ ] Simulations plus grandes
- [ ] Ajouter le récap des simulations
- [ ] Structure HPL
- 

**** /!\ OTF + EMACS
Attention lors de l'instalation d'Open Trace Format. Il faut lui
donner un prefix, car sinon il ecrase la librairie Open Text Format
utilisé par Emacs, et vous allez perdre du temps comme moi...

Il est peut être possible d'installer l'open trace format via apt-get (qui
gère peut être ce genre de conflit).

Du coup il faut installer dumpi avec le nouveau chemin d'OTF(trace).

**** Meeting avec Florence
On est revenu sur les points suivants :
- UNDEINED DATA TYPE :
  - voir où l'execption est levé dans CODES
  - Determiner les structures HPL
- Analyse fichier OTF1
  - Demander à Guillaume, Jean Marc ou Lucas
- Voir les options de simulations pour savoir qu'est ce qui prend du
  temps (pour comprendre pourquoi avec mpirun -np 5 ca prend 37 minutes)
- Savoir ce que représentent les events dans CODES (model, net,
  events, ...)
- Faire une dizaine de simulation de chaque type pour vérifier si le
  temps est stable et que le 1 minute 45 soit un "outsider"
- Regarder si on peut déterminer une seed (pour reproduire des cas de
  rollback, sans rollback, ...).

***** Convertisseur OTF2PAJE                                      :DUMPI:
Lucas Schnorr a mit à jour l'outil pour que ca fonctionne avec la
derniere version de OTF.

****** Installation
Via [[https://github.com/schnorr/akypuera/wiki/OTFWithAkypuera][github]] avec l'option --recursive
Je suis finalement allé voir Lucas pour configurer le cmake, ayant
deux fichiers otf.h sur mon pc pour text et trace ...

apt-get install libboost-all-dev
Via [[https://github.com/schnorr/pajeng/wiki/Install][github]] pour pajeng

****** Conversion
Laconversion se fait comme suit :
#+BEGIN_EXAMPLE
otf2paje test.otf | pj_dump > test.csv
#+END_EXAMPLE

****** Analyse
Il y a une erreur lors de la conversion, en effet, aucuns conteneur
n'est défini via otf2page (soit la faute du logiciel, soit (et
surement) celle de dumpi2otf).
J'ai donc vu avec Lucas pour savoir comment avancer

****** Script
Dans un premier temps Lucas a ajouté à la main la définition des
container.
Via la commande suivante :

#+begin_src sh :results output :exports both
cat TEST.paje | grep ^0 | cut -d" " -f3 | sort --version-sort | uniq | 
sed -e "s/\(p.*\)/10 0.0 \1 root PROCESS \1/"
#+end_src

puis un copié coller au bon endroit dans le fichier (juste avant
l'erreur).

Il faut maintenant que j'écrive un script qui automatise tout ca.

** 2017-06 juin
*** 2017-06-01 jeudi
**** Mail Arnaud
***** Contenu 
Salut Maxime,

> - [ ] Demander un PC

C'est pas que je ne veuille pas t'en donner un mais "man grid5000" ;)

Sans rire, tu vas sur
https://www.grid5000.fr/mediawiki/index.php/Grid5000:Get_an_account. Tu
indiques alegrand comme responsable local. Pierre Neyron et Baptiste
Pichot sont admins grid5000, la doc de grid5000 est très bien fichue et
tous (ou presque) les doctorants l'utilisent donc si tu as un problème
c'est facile.

Si tu as des choses particulières à installer et qui demandent d'être
root pour être installées, il se peut qu'il faille que tu déploies une
image mais je ne suis pas sûr que tu aies besoin d'en passer par là.

A+
    Arnaud
***** Commentaire
Ok je vais me renseigner

**** Format HPL
http://www.netlib.org/benchmark/hpl/algorithm.html
Via cette page, j'ai vu qu'il y avait deux manières d'Update j'ai donc
tenté les deux pour voir si CODES aurait accepté les traces, mais non.
J'ai également testé avec l'option de compilation =-DHPL_COPY_L= pour
savoir si ca aller changer quelque chose, mais non plus...

J'ai vu également qu'il y a une option de compilation pour avoir des
times détaillés. On va peut être pouvoir avoir plus d'information avec
ca.

J'ai crus comprendre ne parcourant internet que HPL utilise un format
MPI-LIKE. Je met un lien vers la doc car je n'arrive pas à voir ce qui
change par rapport à du mpi traditionnel :
http://www.netlib.org/benchmark/hpl/documentation.html

**** Script OTF

Pour rappel, dans la conversion vers paje il manque la déclration des
container. Via ce scrip on les défini :

#+begin_src sh :results output :exports both
cat TEST.paje | grep ^0 | cut -d" " -f3 | sort --version-sort | uniq | 
sed -e "s/\(p.*\)/10 0.0 \1 root PROCESS \1/"
#+end_src

Il faut maintenant les ajouters au bon endroit.
J'ai fait un petit fichier C (premiere version) pour faire la
passerelle (je connais pas la manipulation de fichier en perl/bash ni
les expressions régulières...). 
#+BEGIN_EXAMPLE
#include <stdlib.h>
#include <stdio.h>
#include <string.h>

#define TAILLE_MAX 1000
 
int main(int argc, char *argv[])
{
    //Recuperer les infos a ajouter
	FILE *fp = NULL;	
	char path[1035];
	char stdoutName[]="azefgbvcser3658412558srdf";

	/* Open the command for reading. */
	fp = popen("cat test.paje | grep ^0 | cut -d\" \" -f3 | sort --version-sort | uniq | sed -e \"s/\\(p.*\\)/10 0.0 \\1 root PROCESS \\1/\"", "r");
	if (fp == NULL) {
	  printf("Failed to run command\n" );
	  exit(1);
	}


    char chaine[TAILLE_MAX] = "";
    
    char cherche[] = "# OTF TimerResolution:";
    
    if (stdin != NULL)
    {
        while (fgets(chaine, TAILLE_MAX, stdin) != NULL) // On lit le stdin tant qu'on ne reçoit pas d'erreur (NULL)
        {
            if(!strncmp(cherche, chaine, 22)) {
                	/* Read the output a line at a time - output it. */
				while (fgets(path, sizeof(path)-1, fp) != NULL) {
				  fprintf(stdout, "%s", path);
				}
				/* close */
				pclose(fp);
				break;
			}else{
				fprintf(stdout, "%s", chaine);
			}
        }

        while (fgets(chaine, TAILLE_MAX, stdin) != NULL) // On lit le stdin tant qu'on ne reçoit pas d'erreur (NULL)
        {
        	fprintf(stdout, "%s", chaine);
 		}


    }
 
    return 0;
}
#+END_EXAMPLE

J'ai ensuite fait le script suivant :
#+BEGIN_EXAMPLE
#!/bin/bash
otf2paje $1 | ./testScript | pj_dump > $2
#+END_EXAMPLE

Avec les traces que j'ai déjà ca fonctionne bien. Malheureusement,
j'ai des soucis sur mon pc qui font que j'arrive plus à faire de
nouvelles simulations. mpich fonctionne bien (test sur des exemples),
mais dès que j'ai un lien avec DUMPI ca ne fonctionne plus, avec des
erreurs de type :

#+BEGIN_EXAMPLE
[Black-Pearl-2:23119] *** Process received signal ***
[Black-Pearl-2:23119] Signal: Segmentation fault (11)
[Black-Pearl-2:23119] Signal code: Address not mapped (1)
[Black-Pearl-2:23119] Failing at address: 0x44000098
[Black-Pearl-2:23120] *** Process received signal ***
[Black-Pearl-2:23120] Signal: Segmentation fault (11)
[Black-Pearl-2:23120] Signal code: Address not mapped (1)
[Black-Pearl-2:23120] Failing at address: 0x44000098
[Black-Pearl-2:23119] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x11670)[0x7fd714196670]
[Black-Pearl-2:23119] [ 1] /usr/lib/mpich/lib/libmpi.so.20(MPI_Barrier+0x96)[0x7fd7143fac96]
[Black-Pearl-2:23120] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x11670)[0x7f0c7599e670]
[Black-Pearl-2:23120] [ 1] /usr/lib/mpich/lib/libmpi.so.20(MPI_Barrier+0x96)[0x7f0c75c02c96]
[Black-Pearl-2:23120] [ 2] /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libdumpi.so.7(MPI_Init+0x362)[0x7f0c75ed6721]
[Black-Pearl-2:23120] [ 3] ./ping_pong(+0xb14)[0x55d864a24b14]
[Black-Pearl-2:23120] [ 4] [Black-Pearl-2:23119] [ 2] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf1)[0x7f0c755e63f1]
[Black-Pearl-2:23120] [ 5] ./ping_pong(+0x9da)[0x55d864a249da]
[Black-Pearl-2:23120] *** End of error message ***
/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib/libdumpi.so.7(MPI_Init+0x362)[0x7fd7146ce721]
[Black-Pearl-2:23119] [ 3] ./ping_pong(+0xb14)[0x55fa4da65b14]
[Black-Pearl-2:23119] [ 4] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf1)[0x7fd713dde3f1]
[Black-Pearl-2:23119] [ 5] ./ping_pong(+0x9da)[0x55fa4da659da]
[Black-Pearl-2:23119] *** End of error message ***
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 0 on node Black-Pearl-2 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
#+END_EXAMPLE

Je pense que je vais tout réinstaller parce que je sais pas ce qui cloche...

***** Visualisation
J'ai testé avec vite sur le fichier en paje complété mais il n'y a pas
de flèches & co, quelques traces, mais c'est pas riche.

*** 2017-06-02 vendredi
:LOGBOOK:
- State "STARTED"    from "TODO"       [2017-06-05 lun. 11:08]
- State "TODO"       from              [2017-06-05 lun. 11:08]
:END:

**** Mail Florence

***** Content
Salut Maxime,

finalement je n’ai pas eu une minute dans mon bureau avant 17h aujourd’hui… 
Demain je suis bloquée le matin de 10h à 13h mais autrement c’est « souple ». On peut se voir avant 10h, ou en tout début d’après-midi, pour faire le point notamment sur les aspects suivants:
- format HPL 
- G5K
- simulations (répétitions avec les mêmes paramètres pour AMG afin de confirmer le caractère « ralenti » de la simu //) 
- events/model/net/messages, PE/LP/KP… : cf ci-dessous
- si on a le temps, j’aimerais installer l’ensemble des soft nécessaires pour reproduire certaines de tes expériences sur ma machine : je sais que tu as mis un « howto »  qqpart dans ton journal, mais il a dû évoluer, et ton aide me sera précieuse :-)
+ ce que tu auras à ajouter :-)

N’hésite pas à m’envoyer un meeting request (mail ou sms 06 85 72 49 71) si tu ne me trouves pas dans mon bureau. 

À demain ! Bonne soirée

Flo


PS: sur la terminologie dans les fichiers de sortie (de codes)
in =codes/doc/GETTING_STARTED=:
"to automate LP placement on KPs/PEs »
"DUMPI collects and reads events from MPI applications » => events = input?

in ROSS/README.md
 --nkp=n number of kernel processes (KPs) per pe (default 1)
(…)
 1 (sequential): Instructs ROSS to use a sequential engine, i.e. the simulation will consist of only one processor (PE).
(…)
ROSS Event Memory Allocation:
     Model events                                               124
     Network events                                           50000
=> tous ces termes doivent être définis dans la spec de ROSS quelquepart. 
(…)
PE=Processing Element (=processeur)
(…)
--synch={1,2,3,4}

The synch option can take the following values:
        • 1 (sequential): Instructs ROSS to use a sequential engine, i.e. the simulation will consist of only one processor (PE).
        • 2 (conservative): Instructs ROSS to use a conservative engine, i.e. all LPs will have the same notion of virtual time.
        • 3 (optimistic): Instructs ROSS to use an optimistic engine, i.e. LPs can have varying notions of virtual time, though rollbacks will be possible and a reverse event handler will be required in this case.
=> donc on peut être en séquentiel (1 seul PE) mais avec plusieurs LP (si je comprends bien) ce qui expliquerait pourquoi tu as des rollbacks dans une simu séquentielle.
        • 4 (optimistic debug): Executes ROSS in a serial simulation until it runs out of memory. Then it rolls every message back. This can be used to test rollback functions.
=> cela explique pourquoi tu avais 100% de rollbacks dans une de tes simulations avec l’option Debug.
Pour les notions d’événements "events vs messages", et leur variabilité, on regarde demain.

***** Commentaires
Ok super pour la terminologie! Du coup si je comprend bien, sur ma
machine je peux avoir qu'un seul PE  et plusieurs KP ?

**** HPL
J'expériemente d'autres manières de compiler dumpi pour voir si y'en a
pas une qui pourrait nous être utile (notamment =HPL_NO_MPI_DATATYPE= et
=HPL_DETAILED_TIMING=)

On arrive à visualiser les traces dumpi dans vite (mais il y a pleins
de warning)

***** =HPL_NO_MPI_DATATYPE=

En ajoutant cette option dans le make.<arch> avec l'option -D, on
arrive à visualiser quelque chose avec vite (je ne sais pas quoi
exactement, mais il y a des couleurs et des flèches). On arrive
également à faire un replay dans CODES sans "UNDEFINED DATA TYPE" !
Cependant c'est pas encore ca, voici la sortie avec un lancement
séquentiel (toujours pas possible de lancer mpirun avec dumpi) : 
#+BEGIN_EXAMPLE
Fri Jun  2 09:15:49 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 1 unmatched sends 0 Total sends 0 receives 2 collectives 0 delays 8 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 3 unmatched irecvs 1 unmatched sends 0 Total sends 1 receives 1 collectives 0 delays 10 wait alls 0 waits 0 send time 3.202149 wait 0.000000
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 1 collectives 0 delays 7 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 7 unmatched irecvs 1 unmatched sends 0 Total sends 1 receives 1 collectives 0 delays 10 wait alls 0 waits 0 send time 3.189207 wait 0.000000
	: Running Time = 0.0001 seconds

TW Library Statistics:
	Total Events Processed                                      56
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                        56
	Event Rate (events/sec)                               823529.4
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.0002

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 8 recvd 20 
 max runtime 0.000000 ns avg runtime 0.000000 
 max comm time 0.000000 avg comm time -69573.000000 
 max send time 3.202149 avg send time 1.597839 
 max recv time 45682.609151 avg recv time 11420.652288 
 max wait time 0.000000 avg wait time 0.000000 
#+END_EXAMPLE

Il y a du unmatch, un running time très petit, et des valeurs
négatives.

(en fait on a la même execution qu'en temps normal mais sans les
UNDEFINED DATATYPE)

***** =HPL_DETAILED_TIMING=

Avec cette option on a aussi une visualisation dans vite mais des
=undefined datatype=. 

***** BOTH

On tente avec les deux !
Même résultat qu'avec =NO_MPI_DATATYPE= 

Il en est de même avec toutes les options qu'on peut ajouter.


reminder : https://pop-coe.eu/further-information/learning-material
https://github.com/LLNL/ravel (traces dispo dans les issues)
*** 2017-06-05 lundi
**** HOW TO
***** apt-get + versions
- gcc => 6.3.020170406
- mpich => 3.2-7
- cmake => 3.7.2-1
- doxygen => 1.8.13
- libtool => 2.4.6-2
- m4 => 1.4.18-1
- automake => 1:1.15-5ubuntu1
- autoconf => 2.69-10
- libatlas-base-dev => 3.10.3-1ubuntu1
- libmpich-dev => 3.2-7 build1
- gfortran 4 => 6.3.0-2ubuntu1
- flex => 2.6.2-1.3
- bison => 2:3.0.4.dfsg-1build1
- pkg-config => 0.29.1-0ubuntu1

***** Don't work yet RAVEL
[[https://github.com/LLNL/ravel][github]]

[[https://github.com/LLNL/ravel/issues/7][issue]]

****** Otf
(peut être possible d'installer directemement le paquet libopen-trace-format1)
[[http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih/forschung/projekte/otf/index_html/document_view?set_language=en][link]]
#+begin_src sh :results output :exports both
./configure --prefix=<path>
make
make install
#+end_src
#+BEGIN_EXAMPLE
Libraries have been installed in:
   /home/chevamax/logiciels/lib

If you ever happen to want to link against installed libraries
in a given directory, LIBDIR, you must either use libtool, and
specify the full pathname of the library, or use the `-LLIBDIR'
flag during linking and do at least one of the following:
   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable
     during execution
   - add LIBDIR to the `LD_RUN_PATH' environment variable
     during linking
   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag
   - have your system administrator add LIBDIR to `/etc/ld.so.conf'

See any operating system documentation about shared libraries for
more information, such as the ld(1) and ld.so(8) manual pages.
#+END_EXAMPLE


****** Otf2
[[http://www.vi-hps.org/upload/packages/otf2/otf2-2.0.tar.gz][link]]
#+begin_src sh :results output :exports both
./configure --prefix=<path>
make
make install
#+end_src
#+BEGIN_EXAMPLE
----------------------------------------------------------------------
Libraries have been installed in:
   /home/chevamax/logiciels/lib

If you ever happen to want to link against installed libraries
in a given directory, LIBDIR, you must either use libtool, and
specify the full pathname of the library, or use the '-LLIBDIR'
flag during linking and do at least one of the following:
   - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable
     during execution
   - add LIBDIR to the 'LD_RUN_PATH' environment variable
     during linking
   - use the '-Wl,-rpath -Wl,LIBDIR' linker flag
   - have your system administrator add LIBDIR to '/etc/ld.so.conf'

See any operating system documentation about shared libraries for
more information, such as the ld(1) and ld.so(8) manual pages.
----------------------------------------------------------------------
#+END_EXAMPLE

****** Qt 5 
Il y a surement un paquet pour qt5, mais je ne l'ai pas trouvé.
[[https://www.qt.io/download-open-source/][link]] 
#+begin_src sh :results output :exports both
chmod u+x qt-unified-linux-x64-3.0.0-online.run
./qt-unified-linux-x64-3.0.0-online.run
#+end_src

Puis j'ai laissé les options par défaut. Puis j'ai ajouté les chemins
dans mes variables d'environnement.

****** Muster
[[https://github.com/LLNL/muster/archive/v1.0.1.tar.gz][link]] 

/!\ Attention ne pas faire ca, ca instal openmpi et on veut pas, trouver
le paquet avec seulement les headers
-> Apparement non, mais on peut définir notre systeme mpi préféré via
la commande =sudo update-alternatives --set mpi /usr/include/mpich=
#+begin_src sh :results output :exports both
sudo apt-get install libboost-all-dev
#+end_src

Puis suivre les instructions d'installation

****** Ravel
#+begin_src sh :results output :exports both
sudo apt install mesa-common-dev
sudo apt-get install libglu1-mesa-dev -y
#+end_src

***** DUMPI

[[https://github.com/sstsimulator/sst-dumpi][github]] 

#+begin_src sh :results output :exports both
git clone https://github.com/sstsimulator/sst-dumpi
cd sst-dumpi
#+end_src

Ensuite il faut modifier le fichier configure, et remplacer à la ligne
17344 le =-lotf= par =-lopen-trace-format= .

#+begin_src sh :results output :exports both
mkdir build
cd build
../configure --enable-libdumpi --enable-test --prefix=/home/chevamax/logiciels CC=mpicc CXX=mpiCC CFLAGS="-DMPICH_SUPPRESS_PROTOTYPES=1 -DHAVE_PRAGMA_HP_SEC_DEF=1 -pthread -I/home/chevamax/logiciels/include/open-trace-format -L/home/chevamax/logiciels/lib"
#+end_src

Puis ajouter le code ci dessous au fichier
sst-dumpi/build/dumpi/bin/Makefile à CXXFLAGS
#+BEGIN_EXAMPLE
-I/home/chevamax/logiciels/include/open-trace-format -L/home/chevamax/logiciels/lib -lopen-trace-format -lz
#+END_EXAMPLE
puis faire 
#+begin_src sh :results output :exports both
make
make install
make doc
#+end_src

#+BEGIN_EXAMPLE
----------------------------------------------------------------------
Libraries have been installed in:
   /home/chevamax/logiciels/lib

If you ever happen to want to link against installed libraries
in a given directory, LIBDIR, you must either use libtool, and
specify the full pathname of the library, or use the '-LLIBDIR'
flag during linking and do at least one of the following:
   - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable
     during execution
   - add LIBDIR to the 'LD_RUN_PATH' environment variable
     during linking
   - use the '-Wl,-rpath -Wl,LIBDIR' linker flag
   - have your system administrator add LIBDIR to '/etc/ld.so.conf'

See any operating system documentation about shared libraries for
more information, such as the ld(1) and ld.so(8) manual pages.
----------------------------------------------------------------------
#+END_EXAMPLE

J'ai quand même beaucoups de WARNING, peut être que c'est pour ca que
la visu ne fonctionne pas bien...
#+BEGIN_EXAMPLE
log.txt
../../../dumpi/test/testmpi.c: In function ‘main’:
../../../dumpi/test/testmpi.c:60:3: warning: implicit declaration of function ‘MPI_Init’ [-Wimplicit-function-declaration]
   MPI_Init(&argc, &argv);
   ^~~~~~~~
../../../dumpi/test/testmpi.c:61:3: warning: implicit declaration of function ‘MPI_Comm_rank’ [-Wimplicit-function-declaration]
   MPI_Comm_rank(MPI_COMM_WORLD, &rank);
   ^~~~~~~~~~~~~
../../../dumpi/test/testmpi.c:62:3: warning: implicit declaration of function ‘MPI_Comm_size’ [-Wimplicit-function-declaration]
   MPI_Comm_size(MPI_COMM_WORLD, &size);
   ^~~~~~~~~~~~~
../../../dumpi/test/testmpi.c:75:3: warning: implicit declaration of function ‘MPI_Finalize’ [-Wimplicit-function-declaration]
   MPI_Finalize();
   ^~~~~~~~~~~~
../../../dumpi/test/p2p.c: In function ‘sends’:
../../../dumpi/test/p2p.c:70:3: warning: implicit declaration of function ‘MPI_Buffer_attach’ [-Wimplicit-function-declaration]
   MPI_Buffer_attach(scratch, bsize);     \
   ^
../../../dumpi/test/p2p.c:139:3: note: in expansion of macro ‘DUMPI_TEST_SEND’
   DUMPI_TEST_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:79:7: warning: implicit declaration of function ‘MPI_Irecv’ [-Wimplicit-function-declaration]
       MPI_Irecv(recvarr + 1024*i, 1024, XMPI_TYPE, i,   \
       ^
../../../dumpi/test/p2p.c:139:3: note: in expansion of macro ‘DUMPI_TEST_SEND’
   DUMPI_TEST_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:85:7: warning: implicit declaration of function ‘MPI_Send’ [-Wimplicit-function-declaration]
       MPI_Send(sendarr, 1024, XMPI_TYPE, i, 1, MPI_COMM_WORLD);  \
       ^
../../../dumpi/test/p2p.c:139:3: note: in expansion of macro ‘DUMPI_TEST_SEND’
   DUMPI_TEST_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:88:3: warning: implicit declaration of function ‘MPI_Waitall’ [-Wimplicit-function-declaration]
   MPI_Waitall(size, req, MPI_STATUSES_IGNORE);    \
   ^
../../../dumpi/test/p2p.c:139:3: note: in expansion of macro ‘DUMPI_TEST_SEND’
   DUMPI_TEST_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:97:7: warning: implicit declaration of function ‘MPI_Bsend’ [-Wimplicit-function-declaration]
       MPI_Bsend(sendarr, 1024, XMPI_TYPE, i, 1, MPI_COMM_WORLD); \
       ^
../../../dumpi/test/p2p.c:139:3: note: in expansion of macro ‘DUMPI_TEST_SEND’
   DUMPI_TEST_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:107:3: warning: implicit declaration of function ‘MPI_Barrier’ [-Wimplicit-function-declaration]
   MPI_Barrier(MPI_COMM_WORLD);      \
   ^
../../../dumpi/test/p2p.c:139:3: note: in expansion of macro ‘DUMPI_TEST_SEND’
   DUMPI_TEST_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:110:7: warning: implicit declaration of function ‘MPI_Rsend’ [-Wimplicit-function-declaration]
       MPI_Rsend(sendarr, 1024, XMPI_TYPE, i, 1, MPI_COMM_WORLD); \
       ^
../../../dumpi/test/p2p.c:139:3: note: in expansion of macro ‘DUMPI_TEST_SEND’
   DUMPI_TEST_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:122:7: warning: implicit declaration of function ‘MPI_Ssend’ [-Wimplicit-function-declaration]
       MPI_Ssend(sendarr, 1024, XMPI_TYPE, i, 1, MPI_COMM_WORLD); \
       ^
../../../dumpi/test/p2p.c:139:3: note: in expansion of macro ‘DUMPI_TEST_SEND’
   DUMPI_TEST_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:127:3: warning: implicit declaration of function ‘MPI_Buffer_detach’ [-Wimplicit-function-declaration]
   MPI_Buffer_detach(&scratch, &bsize);     \
   ^
../../../dumpi/test/p2p.c:139:3: note: in expansion of macro ‘DUMPI_TEST_SEND’
   DUMPI_TEST_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:130:7: warning: implicit declaration of function ‘MPI_Request_free’ [-Wimplicit-function-declaration]
       MPI_Request_free(req+i);      \
       ^
../../../dumpi/test/p2p.c:139:3: note: in expansion of macro ‘DUMPI_TEST_SEND’
   DUMPI_TEST_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c: In function ‘isends’:
../../../dumpi/test/p2p.c:210:7: warning: implicit declaration of function ‘MPI_Isend’ [-Wimplicit-function-declaration]
       MPI_Isend(sendarr + 1024*i, 1024, XMPI_TYPE, i,   \
       ^
../../../dumpi/test/p2p.c:274:3: note: in expansion of macro ‘DUMPI_TEST_ISEND’
   DUMPI_TEST_ISEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:216:7: warning: implicit declaration of function ‘MPI_Recv’ [-Wimplicit-function-declaration]
       MPI_Recv(recvarr, 1024, XMPI_TYPE, i, 1, MPI_COMM_WORLD,  \
       ^
../../../dumpi/test/p2p.c:274:3: note: in expansion of macro ‘DUMPI_TEST_ISEND’
   DUMPI_TEST_ISEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:223:7: warning: implicit declaration of function ‘MPI_Ibsend’ [-Wimplicit-function-declaration]
       MPI_Ibsend(sendarr + 1024*i, 1024, XMPI_TYPE, i,   \
       ^
../../../dumpi/test/p2p.c:274:3: note: in expansion of macro ‘DUMPI_TEST_ISEND’
   DUMPI_TEST_ISEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:243:7: warning: implicit declaration of function ‘MPI_Irsend’ [-Wimplicit-function-declaration]
       MPI_Irsend(sendarr + 1024*i, 1024, XMPI_TYPE, i,   \
       ^
../../../dumpi/test/p2p.c:274:3: note: in expansion of macro ‘DUMPI_TEST_ISEND’
   DUMPI_TEST_ISEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:250:7: warning: implicit declaration of function ‘MPI_Issend’ [-Wimplicit-function-declaration]
       MPI_Issend(sendarr + 1024*i, 1024, XMPI_TYPE, i,   \
       ^
../../../dumpi/test/p2p.c:274:3: note: in expansion of macro ‘DUMPI_TEST_ISEND’
   DUMPI_TEST_ISEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c: In function ‘deferred_sends’:
../../../dumpi/test/p2p.c:350:7: warning: implicit declaration of function ‘MPI_Send_init’ [-Wimplicit-function-declaration]
       MPI_Send_init(sendarr, 1024, XMPI_TYPE, i,   \
       ^
../../../dumpi/test/p2p.c:435:3: note: in expansion of macro ‘DUMPI_TEST_DEFERRED_SEND’
   DUMPI_TEST_DEFERRED_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:358:3: warning: implicit declaration of function ‘MPI_Startall’ [-Wimplicit-function-declaration]
   MPI_Startall(startid, sendreq);     \
   ^
../../../dumpi/test/p2p.c:435:3: note: in expansion of macro ‘DUMPI_TEST_DEFERRED_SEND’
   DUMPI_TEST_DEFERRED_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:364:7: warning: implicit declaration of function ‘MPI_Bsend_init’ [-Wimplicit-function-declaration]
       MPI_Bsend_init(sendarr, 1024, XMPI_TYPE, i,   \
       ^
../../../dumpi/test/p2p.c:435:3: note: in expansion of macro ‘DUMPI_TEST_DEFERRED_SEND’
   DUMPI_TEST_DEFERRED_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:377:7: warning: implicit declaration of function ‘MPI_Rsend_init’ [-Wimplicit-function-declaration]
       MPI_Rsend_init(sendarr, 1024, XMPI_TYPE, i,   \
       ^
../../../dumpi/test/p2p.c:435:3: note: in expansion of macro ‘DUMPI_TEST_DEFERRED_SEND’
   DUMPI_TEST_DEFERRED_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:391:7: warning: implicit declaration of function ‘MPI_Ssend_init’ [-Wimplicit-function-declaration]
       MPI_Ssend_init(sendarr, 1024, XMPI_TYPE, i,   \
       ^
../../../dumpi/test/p2p.c:435:3: note: in expansion of macro ‘DUMPI_TEST_DEFERRED_SEND’
   DUMPI_TEST_DEFERRED_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:406:7: warning: implicit declaration of function ‘MPI_Recv_init’ [-Wimplicit-function-declaration]
       MPI_Recv_init(recvarr + i*1024, 1024, XMPI_TYPE, i,  \
       ^
../../../dumpi/test/p2p.c:435:3: note: in expansion of macro ‘DUMPI_TEST_DEFERRED_SEND’
   DUMPI_TEST_DEFERRED_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:413:5: warning: implicit declaration of function ‘MPI_Start’ [-Wimplicit-function-declaration]
     MPI_Start(recvreq+i);      \
     ^
../../../dumpi/test/p2p.c:435:3: note: in expansion of macro ‘DUMPI_TEST_DEFERRED_SEND’
   DUMPI_TEST_DEFERRED_SEND(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c: In function ‘sendrecvs’:
../../../dumpi/test/p2p.c:499:7: warning: implicit declaration of function ‘MPI_Sendrecv’ [-Wimplicit-function-declaration]
       MPI_Sendrecv(sendbuf, sendcount, XMPI_TYPE, dest, sendtag, \
       ^
../../../dumpi/test/p2p.c:531:3: note: in expansion of macro ‘DUMPI_TEST_SENDRECV’
   DUMPI_TEST_SENDRECV(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~~~~
../../../dumpi/test/p2p.c:520:7: warning: implicit declaration of function ‘MPI_Sendrecv_replace’ [-Wimplicit-function-declaration]
       MPI_Sendrecv_replace(buf, count, XMPI_TYPE,   \
       ^
../../../dumpi/test/p2p.c:582:3: note: in expansion of macro ‘DUMPI_TEST_SENDRECV_REPLACE’
   DUMPI_TEST_SENDRECV_REPLACE(rank, size, char, MPI_CHAR);
   ^~~~~~~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/coll.c: In function ‘rooted_collectives’:
../../../dumpi/test/coll.c:77:5: warning: implicit declaration of function ‘MPI_Bcast’ [-Wimplicit-function-declaration]
     MPI_Bcast(sendbuf, blen, MPI_DOUBLE, root, MPI_COMM_WORLD);
     ^~~~~~~~~
../../../dumpi/test/coll.c:81:5: warning: implicit declaration of function ‘MPI_Scatter’ [-Wimplicit-function-declaration]
     MPI_Scatter(sendbuf, 1024, MPI_DOUBLE, recvbuf, 1024, MPI_DOUBLE,
     ^~~~~~~~~~~
../../../dumpi/test/coll.c:86:5: warning: implicit declaration of function ‘MPI_Scatterv’ [-Wimplicit-function-declaration]
     MPI_Scatterv(sendbuf, cntarr, displarr, MPI_DOUBLE,
     ^~~~~~~~~~~~
../../../dumpi/test/coll.c:91:5: warning: implicit declaration of function ‘MPI_Gather’ [-Wimplicit-function-declaration]
     MPI_Gather(sendbuf, 1024, MPI_DOUBLE, recvbuf, 1024, MPI_DOUBLE,
     ^~~~~~~~~~
../../../dumpi/test/coll.c:96:5: warning: implicit declaration of function ‘MPI_Gatherv’ [-Wimplicit-function-declaration]
     MPI_Gatherv(sendbuf, 1024, MPI_DOUBLE,
     ^~~~~~~~~~~
../../../dumpi/test/coll.c:102:5: warning: implicit declaration of function ‘MPI_Reduce’ [-Wimplicit-function-declaration]
     MPI_Reduce(sendbuf, recvbuf, blen, MPI_DOUBLE, MPI_MAX,
     ^~~~~~~~~~
../../../dumpi/test/coll.c:107:5: warning: implicit declaration of function ‘MPI_Reduce_scatter’ [-Wimplicit-function-declaration]
     MPI_Reduce_scatter(sendbuf, recvbuf, cntarr, MPI_DOUBLE,
     ^~~~~~~~~~~~~~~~~~
../../../dumpi/test/coll.c: In function ‘rootless_collectives’:
../../../dumpi/test/coll.c:136:5: warning: implicit declaration of function ‘MPI_Barrier’ [-Wimplicit-function-declaration]
     MPI_Barrier(MPI_COMM_WORLD);
     ^~~~~~~~~~~
../../../dumpi/test/coll.c:138:5: warning: implicit declaration of function ‘MPI_Allgather’ [-Wimplicit-function-declaration]
     MPI_Allgather(sendbuf+1024*rank, 1024, MPI_DOUBLE,
     ^~~~~~~~~~~~~
../../../dumpi/test/coll.c:142:5: warning: implicit declaration of function ‘MPI_Allgatherv’ [-Wimplicit-function-declaration]
     MPI_Allgatherv(sendbuf+1024*rank, 1024, MPI_DOUBLE,
     ^~~~~~~~~~~~~~
../../../dumpi/test/coll.c:146:5: warning: implicit declaration of function ‘MPI_Allreduce’ [-Wimplicit-function-declaration]
     MPI_Allreduce(sendbuf, recvbuf, blen,
     ^~~~~~~~~~~~~
../../../dumpi/test/coll.c:149:5: warning: implicit declaration of function ‘MPI_Alltoall’ [-Wimplicit-function-declaration]
     MPI_Alltoall(sendbuf, 1024, MPI_DOUBLE,
     ^~~~~~~~~~~~
../../../dumpi/test/coll.c:152:5: warning: implicit declaration of function ‘MPI_Alltoallv’ [-Wimplicit-function-declaration]
     MPI_Alltoallv(sendbuf, cntarr, displarr, MPI_DOUBLE,
     ^~~~~~~~~~~~~
../../../dumpi/test/coll.c:156:5: warning: implicit declaration of function ‘MPI_Scan’ [-Wimplicit-function-declaration]
     MPI_Scan(sendbuf, recvbuf, blen, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);
     ^~~~~~~~
../../../dumpi/test/manip.c: In function ‘queries’:
../../../dumpi/test/manip.c:79:3: warning: implicit declaration of function ‘MPI_Wtime’ [-Wimplicit-function-declaration]
   MPI_Wtime();
   ^~~~~~~~~
../../../dumpi/test/manip.c:80:3: warning: implicit declaration of function ‘MPI_Wtick’ [-Wimplicit-function-declaration]
   MPI_Wtick();
   ^~~~~~~~~
../../../dumpi/test/manip.c:81:3: warning: implicit declaration of function ‘MPI_Comm_rank’ [-Wimplicit-function-declaration]
   MPI_Comm_rank(MPI_COMM_WORLD, &r);
   ^~~~~~~~~~~~~
../../../dumpi/test/manip.c:82:3: warning: implicit declaration of function ‘MPI_Comm_size’ [-Wimplicit-function-declaration]
   MPI_Comm_size(MPI_COMM_WORLD, &s);
   ^~~~~~~~~~~~~
../../../dumpi/test/manip.c: In function ‘probes’:
../../../dumpi/test/manip.c:99:7: warning: implicit declaration of function ‘MPI_Isend’ [-Wimplicit-function-declaration]
       MPI_Isend(&rank, 1, MPI_INT, peer, rank, MPI_COMM_WORLD, req+peer);
       ^~~~~~~~~
../../../dumpi/test/manip.c:106:2: warning: implicit declaration of function ‘MPI_Probe’ [-Wimplicit-function-declaration]
  MPI_Probe(peer, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
  ^~~~~~~~~
../../../dumpi/test/manip.c:110:4: warning: implicit declaration of function ‘MPI_Iprobe’ [-Wimplicit-function-declaration]
    MPI_Iprobe(peer, MPI_ANY_TAG, MPI_COMM_WORLD, &flag, &status);
    ^~~~~~~~~~
../../../dumpi/test/manip.c:115:7: warning: implicit declaration of function ‘MPI_Get_count’ [-Wimplicit-function-declaration]
       MPI_Get_count(&status, MPI_INT, &count);
       ^~~~~~~~~~~~~
../../../dumpi/test/manip.c:117:7: warning: implicit declaration of function ‘MPI_Recv’ [-Wimplicit-function-declaration]
       MPI_Recv(&i, 1, MPI_INT, peer, status.MPI_TAG, MPI_COMM_WORLD, &status2);
       ^~~~~~~~
../../../dumpi/test/manip.c: In function ‘tests’:
../../../dumpi/test/manip.c:153:7: warning: implicit declaration of function ‘MPI_Irecv’ [-Wimplicit-function-declaration]
       MPI_Irecv(matching_value+peer, 1, MPI_INT, peer, matching_tag,
       ^~~~~~~~~
../../../dumpi/test/manip.c:163:3: warning: implicit declaration of function ‘MPI_Test’ [-Wimplicit-function-declaration]
   MPI_Test(nonmatching_recv+reqid, &flag, statuses);
   ^~~~~~~~
../../../dumpi/test/manip.c:170:3: warning: implicit declaration of function ‘MPI_Testany’ [-Wimplicit-function-declaration]
   MPI_Testany(size, nonmatching_recv, &index, &flag, statuses);
   ^~~~~~~~~~~
../../../dumpi/test/manip.c:177:3: warning: implicit declaration of function ‘MPI_Testsome’ [-Wimplicit-function-declaration]
   MPI_Testsome(size, nonmatching_recv, &outcount, indices, statuses);
   ^~~~~~~~~~~~
../../../dumpi/test/manip.c:183:3: warning: implicit declaration of function ‘MPI_Testall’ [-Wimplicit-function-declaration]
   MPI_Testall(size, nonmatching_recv, &flag, statuses);
   ^~~~~~~~~~~
../../../dumpi/test/manip.c:190:3: warning: implicit declaration of function ‘MPI_Wait’ [-Wimplicit-function-declaration]
   MPI_Wait(matching_recv+reqid, statuses);
   ^~~~~~~~
../../../dumpi/test/manip.c:193:3: warning: implicit declaration of function ‘MPI_Waitany’ [-Wimplicit-function-declaration]
   MPI_Waitany(size, matching_recv, &index, statuses);
   ^~~~~~~~~~~
../../../dumpi/test/manip.c:196:3: warning: implicit declaration of function ‘MPI_Waitsome’ [-Wimplicit-function-declaration]
   MPI_Waitsome(size/2, matching_recv, &outcount, indices, statuses);
   ^~~~~~~~~~~~
../../../dumpi/test/manip.c:199:3: warning: implicit declaration of function ‘MPI_Waitall’ [-Wimplicit-function-declaration]
   MPI_Waitall(size, matching_recv, statuses);
   ^~~~~~~~~~~
../../../dumpi/test/manip.c:204:7: warning: implicit declaration of function ‘MPI_Cancel’ [-Wimplicit-function-declaration]
       MPI_Cancel(nonmatching_send+peer);
       ^~~~~~~~~~
../../../dumpi/test/manip.c:213:7: warning: implicit declaration of function ‘MPI_Test_cancelled’ [-Wimplicit-function-declaration]
       MPI_Test_cancelled(statuses, &flag);
       ^~~~~~~~~~~~~~~~~~
../../../dumpi/test/manip.c: In function ‘types’:
../../../dumpi/test/manip.c:264:3: warning: implicit declaration of function ‘MPI_Barrier’ [-Wimplicit-function-declaration]
   MPI_Barrier(MPI_COMM_WORLD);
   ^~~~~~~~~~~
../../../dumpi/test/manip.c:266:3: warning: implicit declaration of function ‘MPI_Type_contiguous’ [-Wimplicit-function-declaration]
   MPI_Type_contiguous(5, MPI_INT, &cont_type);
   ^~~~~~~~~~~~~~~~~~~
../../../dumpi/test/manip.c:267:3: warning: implicit declaration of function ‘MPI_Type_commit’ [-Wimplicit-function-declaration]
   MPI_Type_commit(&cont_type);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/manip.c:268:3: warning: implicit declaration of function ‘MPI_Type_size’ [-Wimplicit-function-declaration]
   MPI_Type_size(cont_type, &typesize);
   ^~~~~~~~~~~~~
../../../dumpi/test/manip.c:275:3: warning: implicit declaration of function ‘MPI_Type_vector’ [-Wimplicit-function-declaration]
   MPI_Type_vector(2, 3, 4, cont_type, &vec_type);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/manip.c:282:3: warning: implicit declaration of function ‘MPI_Type_hvector’ [-Wimplicit-function-declaration]
   MPI_Type_hvector(4, 3, 2, cont_type, &hvec_type);
   ^~~~~~~~~~~~~~~~
../../../dumpi/test/manip.c:291:5: warning: implicit declaration of function ‘MPI_Type_indexed’ [-Wimplicit-function-declaration]
     MPI_Type_indexed(2, indexed_blen, indexed_ind, vec_type, &ind_type);
     ^~~~~~~~~~~~~~~~
../../../dumpi/test/manip.c:300:7: warning: implicit declaration of function ‘MPI_Type_hindexed’ [-Wimplicit-function-declaration]
       MPI_Type_hindexed(2, indexed_blen, hindexed_ind, hvec_type, &hind_type);
       ^~~~~~~~~~~~~~~~~
../../../dumpi/test/manip.c:320:5: warning: implicit declaration of function ‘MPI_Type_create_struct’ [-Wimplicit-function-declaration]
     MPI_Type_create_struct(3, struct_blen, struct_disp,
     ^~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/manip.c:380:5: warning: implicit declaration of function ‘MPI_Type_create_resized’ [-Wimplicit-function-declaration]
     MPI_Type_create_resized(MPI_INT, 0, 3*sizeof(int), &resized_type);
     ^~~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/manip.c:383:5: warning: implicit declaration of function ‘MPI_Type_extent’ [-Wimplicit-function-declaration]
     MPI_Type_extent(resized_type, &typeextent);
     ^~~~~~~~~~~~~~~
../../../dumpi/test/manip.c:395:5: warning: implicit declaration of function ‘MPI_Type_create_subarray’ [-Wimplicit-function-declaration]
     MPI_Type_create_subarray(ndims, sizes, subsizes, starts, order,
     ^~~~~~~~~~~~~~~~~~~~~~~~
../../../dumpi/test/manip.c:406:3: warning: implicit declaration of function ‘MPI_Type_free’ [-Wimplicit-function-declaration]
   MPI_Type_free(&cont_type);
   ^~~~~~~~~~~~~
In file included from ../../../dumpi/test/manip.c:49:0:
../../../dumpi/test/manip.c: In function ‘ops’:
../../../dumpi/test/manip.c:483:10: warning: implicit declaration of function ‘MPI_Type_dup’ [-Wimplicit-function-declaration]
   assert(MPI_Type_dup(MPI_INT, static_aggregate_type) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:488:10: warning: implicit declaration of function ‘MPI_Op_create’ [-Wimplicit-function-declaration]
   assert(MPI_Op_create(sum_bottom_16, 1, &sumfunc) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:491:10: warning: implicit declaration of function ‘MPI_Reduce’ [-Wimplicit-function-declaration]
   assert(MPI_Reduce(indata, outdata, DATACOUNT, *static_aggregate_type,
          ^
../../../dumpi/test/manip.c:499:10: warning: implicit declaration of function ‘MPI_Allreduce’ [-Wimplicit-function-declaration]
   assert(MPI_Allreduce(indata, outdata, DATACOUNT, *static_aggregate_type,
          ^
../../../dumpi/test/manip.c:507:10: warning: implicit declaration of function ‘MPI_Op_free’ [-Wimplicit-function-declaration]
   assert(MPI_Op_free(&sumfunc) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c: In function ‘groups’:
../../../dumpi/test/manip.c:542:10: warning: implicit declaration of function ‘MPI_Comm_dup’ [-Wimplicit-function-declaration]
   assert(MPI_Comm_dup(MPI_COMM_WORLD, &dupworld) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:543:10: warning: implicit declaration of function ‘MPI_Comm_group’ [-Wimplicit-function-declaration]
   assert(MPI_Comm_group(MPI_COMM_WORLD, &worldgroup) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:545:10: warning: implicit declaration of function ‘MPI_Group_compare’ [-Wimplicit-function-declaration]
   assert(MPI_Group_compare(worldgroup, dupworldgroup, &comp) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:560:10: warning: implicit declaration of function ‘MPI_Group_incl’ [-Wimplicit-function-declaration]
   assert(MPI_Group_incl(dupworldgroup,ranksize,ranks,&group_a) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:562:12: warning: implicit declaration of function ‘MPI_Group_size’ [-Wimplicit-function-declaration]
     assert(MPI_Group_size(group_a, &groupsize) == MPI_SUCCESS);
            ^
../../../dumpi/test/manip.c:569:10: warning: implicit declaration of function ‘MPI_Group_excl’ [-Wimplicit-function-declaration]
   assert(MPI_Group_excl(dupworldgroup,ranksize,ranks,&group_b) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:573:12: warning: implicit declaration of function ‘MPI_Group_rank’ [-Wimplicit-function-declaration]
     assert(MPI_Group_rank(group_b, &vv) == MPI_SUCCESS);
            ^
../../../dumpi/test/manip.c:580:10: warning: implicit declaration of function ‘MPI_Group_difference’ [-Wimplicit-function-declaration]
   assert(MPI_Group_difference(worldgroup, group_a, &group_diff) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:589:10: warning: implicit declaration of function ‘MPI_Group_union’ [-Wimplicit-function-declaration]
   assert(MPI_Group_union(group_a, group_b, &group_c) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:602:10: warning: implicit declaration of function ‘MPI_Group_intersection’ [-Wimplicit-function-declaration]
   assert(MPI_Group_intersection(group_a, group_b, &group_d) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:656:12: warning: implicit declaration of function ‘MPI_Group_range_incl’ [-Wimplicit-function-declaration]
     assert(MPI_Group_range_incl(worldgroup, windows, rankrange, &rangegroup_a)
            ^
../../../dumpi/test/manip.c:663:12: warning: implicit declaration of function ‘MPI_Group_range_excl’ [-Wimplicit-function-declaration]
     assert(MPI_Group_range_excl(worldgroup, windows, rankrange, &rangegroup_b)
            ^
../../../dumpi/test/manip.c:675:12: warning: implicit declaration of function ‘MPI_Group_free’ [-Wimplicit-function-declaration]
     assert(MPI_Group_free(&group_a) == MPI_SUCCESS);
            ^
../../../dumpi/test/manip.c: In function ‘comms’:
../../../dumpi/test/manip.c:703:10: warning: implicit declaration of function ‘MPI_Comm_create’ [-Wimplicit-function-declaration]
   assert(MPI_Comm_create(MPI_COMM_WORLD, groupa, &comma) == MPI_SUCCESS);
          ^
../../../dumpi/test/manip.c:714:12: warning: implicit declaration of function ‘MPI_Comm_split’ [-Wimplicit-function-declaration]
     assert(MPI_Comm_split(comma, color, key, &commb) == MPI_SUCCESS);
            ^
../../../dumpi/test/manip.c:721:12: warning: implicit declaration of function ‘MPI_Comm_free’ [-Wimplicit-function-declaration]
     assert(MPI_Comm_free(&comma) == MPI_SUCCESS);
            ^
../../../dumpi/test/testthreads.c: In function ‘main’:
../../../dumpi/test/testthreads.c:94:3: warning: implicit declaration of function ‘MPI_Init_thread’ [-Wimplicit-function-declaration]
   MPI_Init_thread(&argc, &argv, required, &provided);
   ^~~~~~~~~~~~~~~
../../../dumpi/test/testthreads.c:116:3: warning: implicit declaration of function ‘MPI_Finalize’ [-Wimplicit-function-declaration]
   MPI_Finalize();
   ^~~~~~~~~~~~
../../../dumpi/test/testthreads-multiple.c: In function ‘test_thread_multiple’:
../../../dumpi/test/testthreads-multiple.c:75:5: warning: implicit declaration of function ‘MPI_Comm_dup’ [-Wimplicit-function-declaration]
     MPI_Comm_dup(MPI_COMM_WORLD, threadcomm + i);
     ^~~~~~~~~~~~
../../../dumpi/test/testthreads-multiple.c: In function ‘multi_p2p’:
../../../dumpi/test/testthreads-multiple.c:113:3: warning: implicit declaration of function ‘MPI_Comm_size’ [-Wimplicit-function-declaration]
   MPI_Comm_size(data->threadcomm[data->myid], &size);
   ^~~~~~~~~~~~~
../../../dumpi/test/testthreads-multiple.c:114:3: warning: implicit declaration of function ‘MPI_Comm_rank’ [-Wimplicit-function-declaration]
   MPI_Comm_rank(data->threadcomm[data->myid], &rank);
   ^~~~~~~~~~~~~
../../../dumpi/test/testthreads-multiple.c:125:7: warning: implicit declaration of function ‘MPI_Isend’ [-Wimplicit-function-declaration]
       MPI_Isend(&value, 1, MPI_INT, i, data->myid, data->threadcomm[data->myid],
       ^~~~~~~~~
../../../dumpi/test/testthreads-multiple.c:136:11: warning: implicit declaration of function ‘MPI_Irecv’ [-Wimplicit-function-declaration]
           MPI_Irecv(valarr+j, 1, MPI_INT, MPI_ANY_SOURCE, data->myid,
           ^~~~~~~~~
../../../dumpi/test/testthreads-multiple.c:142:5: warning: implicit declaration of function ‘MPI_Waitall’ [-Wimplicit-function-declaration]
     MPI_Waitall(current_request, requests, MPI_STATUSES_IGNORE);
     ^~~~~~~~~~~
../../../dumpi/test/testthreads-multiple.c:150:5: warning: implicit declaration of function ‘MPI_Send’ [-Wimplicit-function-declaration]
     MPI_Send(valarr, 2*size, MPI_INT, rank, rank, data->threadcomm[0]);
     ^~~~~~~~
../../../dumpi/test/testthreads-multiple.c:159:7: warning: implicit declaration of function ‘MPI_Recv’ [-Wimplicit-function-declaration]
       MPI_Recv(valarr, 2*size, MPI_INT, rank, rank,
       ^~~~~~~~
../../../dumpi/test/testthreads-multiple.c: In function ‘multi_coll’:
../../../dumpi/test/testthreads-multiple.c:178:5: warning: implicit declaration of function ‘MPI_Barrier’ [-Wimplicit-function-declaration]
     MPI_Barrier(MPI_COMM_WORLD);
     ^~~~~~~~~~~
../../../dumpi/test/testthreads-multiple.c:181:3: warning: implicit declaration of function ‘MPI_Allreduce’ [-Wimplicit-function-declaration]
   MPI_Allreduce(&value, &rank, 1, MPI_INT, MPI_SUM, data->threadcomm[myid]);
   ^~~~~~~~~~~~~
../../../dumpi/test/testthreads-serialized.c: In function ‘test_thread_serialized’:
../../../dumpi/test/testthreads-serialized.c:80:5: warning: implicit declaration of function ‘MPI_Comm_dup’ [-Wimplicit-function-declaration]
     MPI_Comm_dup(MPI_COMM_WORLD, threadcomm + i);
     ^~~~~~~~~~~~
../../../dumpi/test/testthreads-serialized.c: In function ‘multi_p2p’:
../../../dumpi/test/testthreads-serialized.c:127:3: warning: implicit declaration of function ‘MPI_Comm_size’ [-Wimplicit-function-declaration]
   MPI_Comm_size(data->threadcomm[data->myid], &size);
   ^~~~~~~~~~~~~
../../../dumpi/test/testthreads-serialized.c:128:3: warning: implicit declaration of function ‘MPI_Comm_rank’ [-Wimplicit-function-declaration]
   MPI_Comm_rank(data->threadcomm[data->myid], &rank);
   ^~~~~~~~~~~~~
../../../dumpi/test/testthreads-serialized.c:147:7: warning: implicit declaration of function ‘MPI_Isend’ [-Wimplicit-function-declaration]
       MPI_Isend(&value, 1, MPI_INT, i, data->myid, data->threadcomm[data->myid],
       ^~~~~~~~~
../../../dumpi/test/testthreads-serialized.c:160:11: warning: implicit declaration of function ‘MPI_Irecv’ [-Wimplicit-function-declaration]
           MPI_Irecv(valarr+j, 1, MPI_INT, MPI_ANY_SOURCE, data->myid,
           ^~~~~~~~~
../../../dumpi/test/testthreads-serialized.c:187:7: warning: implicit declaration of function ‘MPI_Waitall’ [-Wimplicit-function-declaration]
       MPI_Waitall(current_request, requests, MPI_STATUSES_IGNORE);
       ^~~~~~~~~~~
../../../dumpi/test/testthreads-serialized.c: In function ‘multi_coll’:
../../../dumpi/test/testthreads-serialized.c:220:5: warning: implicit declaration of function ‘MPI_Barrier’ [-Wimplicit-function-declaration]
     MPI_Barrier(MPI_COMM_WORLD);
     ^~~~~~~~~~~
../../../dumpi/test/testthreads-serialized.c:238:7: warning: implicit declaration of function ‘MPI_Allreduce’ [-Wimplicit-function-declaration]
       MPI_Allreduce(&value, &rank, 1, MPI_INT, MPI_SUM, data->threadcomm[myid]);
       ^~~~~~~~~~~~~
../../../dumpi/test/testthreads-funneled.c: In function ‘message_thread’:
../../../dumpi/test/testthreads-funneled.c:96:3: warning: implicit declaration of function ‘MPI_Comm_size’ [-Wimplicit-function-declaration]
   MPI_Comm_size(MPI_COMM_WORLD, &size);
   ^~~~~~~~~~~~~
../../../dumpi/test/testthreads-funneled.c:97:3: warning: implicit declaration of function ‘MPI_Comm_rank’ [-Wimplicit-function-declaration]
   MPI_Comm_rank(MPI_COMM_WORLD, &rank);
   ^~~~~~~~~~~~~
../../../dumpi/test/testthreads-funneled.c:99:5: warning: implicit declaration of function ‘MPI_Send’ [-Wimplicit-function-declaration]
     MPI_Send(&rank, 1, MPI_INT, 0, 100, MPI_COMM_WORLD);
     ^~~~~~~~
../../../dumpi/test/testthreads-funneled.c:104:7: warning: implicit declaration of function ‘MPI_Recv’ [-Wimplicit-function-declaration]
       MPI_Recv(&j, 1, MPI_INT, MPI_ANY_SOURCE, 100, MPI_COMM_WORLD, &stat);
       ^~~~~~~~
../../../dumpi/test/testthreads-funneled.c:106:3: warning: implicit declaration of function ‘MPI_Barrier’ [-Wimplicit-function-declaration]
   MPI_Barrier(MPI_COMM_WORLD);
   ^~~~~~~~~~~
#+END_EXAMPLE

PS: toutes les manipulations sont nécessaires à cause de la dernière
version d'OTF qui met dans un sous dossier.

***** ROSS
#+begin_src sh :results output :exports both
git clone -b master --single-branch git@github.com:carothersc/ROSS.git
cd ROSS
mkdir build
cd build
ARCH=x86_64 CC=mpicc CXX=mpicxx cmake -DCMAKE_INSTALL_PREFIX=/home/chevamax/logiciels ..
make -j 3
make install
#+end_src

***** CODES
#+begin_src sh :results output :exports both
cd ~/Documents/Stage_LIG_2017/logiciels
wget ftp://ftp.mcs.anl.gov/pub/CODES/releases/codes-0.5.2.tar.gz
tar -xvf codes-0.5.2.tar.gz
mv codes-0.5.2 CODES
cd CODES
./prepare.sh
mkdir build
cd build
../configure --with-dumpi=/home/chevamax/logiciels --prefix=/home/chevamax/logiciels CC=mpicc PKG_CONFIG_PATH=/home/chevamax/logiciels/lib/pkgconfig
make && make install
make tests && make check
#+end_src

Tous les tests doivent être au vert !

***** HPL
Téléchargez le programme [[http://www.netlib.org/benchmark/hpl/hpl-2.2.tar.gz][ici]], ou en ligne de commande
#+begin_src sh 
cd ~/Documents/Stage_LIG_2017/logiciels
wget http://www.netlib.org/benchmark/hpl/hpl-2.2.tar.gz
tar -xvf hpl-2.2.tar.gz
mv hpl-2.2 hpl
cd hpl/setup
sh make_generic
cp Make.UNKNOWN ../Make.linux
cd ..
#+end_src

Puis remplacer le contenu du fichier par celui ci dessous, en
remplacent les lignes avec commentaire #path/to.

#+BEGIN_EXAMPLE
#  
#  -- High Performance Computing Linpack Benchmark (HPL)                
#     HPL - 2.2 - February 24, 2016                          
#     Antoine P. Petitet                                                
#     University of Tennessee, Knoxville                                
#     Innovative Computing Laboratory                                 
#     (C) Copyright 2000-2008 All Rights Reserved                       
#                                                                       
#  -- Copyright notice and Licensing terms:                             
#                                                                       
#  Redistribution  and  use in  source and binary forms, with or without
#  modification, are  permitted provided  that the following  conditions
#  are met:                                                             
#                                                                       
#  1. Redistributions  of  source  code  must retain the above copyright
#  notice, this list of conditions and the following disclaimer.        
#                                                                       
#  2. Redistributions in binary form must reproduce  the above copyright
#  notice, this list of conditions,  and the following disclaimer in the
#  documentation and/or other materials provided with the distribution. 
#                                                                       
#  3. All  advertising  materials  mentioning  features  or  use of this
#  software must display the following acknowledgement:                 
#  This  product  includes  software  developed  at  the  University  of
#  Tennessee, Knoxville, Innovative Computing Laboratory.             
#                                                                       
#  4. The name of the  University,  the name of the  Laboratory,  or the
#  names  of  its  contributors  may  not  be used to endorse or promote
#  products  derived   from   this  software  without  specific  written
#  permission.                                                          
#                                                                       
#  -- Disclaimer:                                                       
#                                                                       
#  THIS  SOFTWARE  IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
#  ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,  INCLUDING,  BUT NOT
#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY
#  OR  CONTRIBUTORS  BE  LIABLE FOR ANY  DIRECT,  INDIRECT,  INCIDENTAL,
#  SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL DAMAGES  (INCLUDING,  BUT NOT
#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
#  DATA OR PROFITS; OR BUSINESS INTERRUPTION)  HOWEVER CAUSED AND ON ANY
#  THEORY OF LIABILITY, WHETHER IN CONTRACT,  STRICT LIABILITY,  OR TORT
#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
# ######################################################################
#  
# ----------------------------------------------------------------------
# - shell --------------------------------------------------------------
# ----------------------------------------------------------------------
#
SHELL        = /bin/sh
#
CD           = cd
CP           = cp
LN_S         = ln -s
MKDIR        = mkdir
RM           = /bin/rm -f
TOUCH        = touch
#
# ----------------------------------------------------------------------
# - Platform identifier ------------------------------------------------
# ----------------------------------------------------------------------
#
ARCH         = linux #ici mettre le même nom que l'extension donnée au fichier
#
# ----------------------------------------------------------------------
# - HPL Directory Structure / HPL library ------------------------------
# ----------------------------------------------------------------------
#
TOPdir       = $(HOME)/Documents/Stage_LIG_2017/logiciels/hpl #path/to/hpl 
INCdir       = $(TOPdir)/include
BINdir       = $(TOPdir)/bin/$(ARCH)
LIBdir       = $(TOPdir)/lib/$(ARCH)
#
HPLlib       = $(LIBdir)/libhpl.a 
#
# ----------------------------------------------------------------------
# - Message Passing library (MPI) --------------------------------------
# ----------------------------------------------------------------------
# MPinc tells the  C  compiler where to find the Message Passing library
# header files,  MPlib  is defined  to be the name of  the library to be 
# used. The variable MPdir is only used for defining MPinc and MPlib.
#
MPdir        = /usr/lib/mpich      #path/to/mpich
MPinc        = -I $(MPdir)/include
MPlib        = -L $(MPdir)/lib -L /home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib -ldumpi #path/to/dumpi
#
# ----------------------------------------------------------------------
# - Linear Algebra library (BLAS or VSIPL) -----------------------------
# ----------------------------------------------------------------------
# LAinc tells the  C  compiler where to find the Linear Algebra  library
# header files,  LAlib  is defined  to be the name of  the library to be 
# used. The variable LAdir is only used for defining LAinc and LAlib.
#
LAdir        = /usr/lib/atlas-base #path/to/lib-atlas
LAinc        = 
LAlib        = $(LAdir)/libf77blas.a $(LAdir)/libatlas.a -lblas
#
# ----------------------------------------------------------------------
# - F77 / C interface --------------------------------------------------
# ----------------------------------------------------------------------
# You can skip this section  if and only if  you are not planning to use
# a  BLAS  library featuring a Fortran 77 interface.  Otherwise,  it  is
# necessary  to  fill out the  F2CDEFS  variable  with  the  appropriate
# options.  **One and only one**  option should be chosen in **each** of
# the 3 following categories:
#
# 1) name space (How C calls a Fortran 77 routine)
#
# -DAdd_              : all lower case and a suffixed underscore  (Suns,
#                       Intel, ...),                           [default]
# -DNoChange          : all lower case (IBM RS6000),
# -DUpCase            : all upper case (Cray),
# -DAdd__             : the FORTRAN compiler in use is f2c.
#
# 2) C and Fortran 77 integer mapping
#
# -DF77_INTEGER=int   : Fortran 77 INTEGER is a C int,         [default]
# -DF77_INTEGER=long  : Fortran 77 INTEGER is a C long,
# -DF77_INTEGER=short : Fortran 77 INTEGER is a C short.
#
# 3) Fortran 77 string handling
#
# -DStringSunStyle    : The string address is passed at the string loca-
#                       tion on the stack, and the string length is then
#                       passed as  an  F77_INTEGER  after  all  explicit
#                       stack arguments,                       [default]
# -DStringStructPtr   : The address  of  a  structure  is  passed  by  a
#                       Fortran 77  string,  and the structure is of the
#                       form: struct {char *cp; F77_INTEGER len;},
# -DStringStructVal   : A structure is passed by value for each  Fortran
#                       77 string,  and  the  structure is  of the form:
#                       struct {char *cp; F77_INTEGER len;},
# -DStringCrayStyle   : Special option for  Cray  machines,  which  uses
#                       Cray  fcd  (fortran  character  descriptor)  for
#                       interoperation.
#
F2CDEFS      = -DAdd_ -DF77_INTEGER=int -DStringSunStyle
#
# ----------------------------------------------------------------------
# - HPL includes / libraries / specifics -------------------------------
# ----------------------------------------------------------------------
#
HPL_INCLUDES = -I$(INCdir) -I$(INCdir)/$(ARCH) $(LAinc) $(MPinc)
HPL_LIBS     = $(HPLlib) $(LAlib) $(MPlib) -lmpl
#
# - Compile time options -----------------------------------------------
#
# -DHPL_COPY_L           force the copy of the panel L before bcast;
# -DHPL_CALL_CBLAS       call the cblas interface;
# -DHPL_CALL_VSIPL       call the vsip  library;
# -DHPL_DETAILED_TIMING  enable detailed timers;
#
# By default HPL will:
#    *) not copy L before broadcast,
#    *) call the BLAS Fortran 77 interface,
#    *) not display detailed timing information.
#
HPL_OPTS     = -DHPL_CALL_CBLAS -DHPL_NO_MPI_DATATYPE
# 
# ----------------------------------------------------------------------
#
HPL_DEFS     = $(F2CDEFS) $(HPL_OPTS) $(HPL_INCLUDES) 
#
# ----------------------------------------------------------------------
# - Compilers / linkers - Optimization flags ---------------------------
# ----------------------------------------------------------------------
#
CC           = /usr/bin/mpicc
CCNOOPT      = $(HPL_DEFS) 
CCFLAGS      = $(HPL_DEFS) -fomit-frame-pointer -O3 -funroll-loops
#
LINKER       = /usr/bin/mpicc
LINKFLAGS    = $(CCFLAGS) -pthread
#
ARCHIVER     = ar
ARFLAGS      = r
RANLIB       = echo
#
# ----------------------------------------------------------------------
#+END_EXAMPLE

#+begin_src sh :session foo :results output :exports both 
make arch=linux
#+end_src

On peut ensuite lancer une execution d'HPL dans =~/hpl/bin/linux=. Il
faut modifier le fichier HPL.dat pour les paramètres de l'execution,
et ensuite lancer *xhpl* via *mpirun* pour avoir des traces.

****** Exemple de fichier HPL.dat
#+BEGIN_EXAMPLE
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any) 
6            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
18048         Ns
1            # of NBs
192           NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
2            Ps
2            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
1            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
#+END_EXAMPLE

puis lancer avec  =mpirun -n 4 ./xhpl=

Après l'avoir lancé, vous devriez avoir des fichiers dumpi :)

**** Mailling list

Hi Misbah,
Thanks for your help, you can find dumpi traces with "UNDEFINED DATA TYPE" and without via the link below. Codes-workload-dump utility is very usefull, thanks for that (I was using dumpistat).

https://1drv.ms/f/s!Ati25f8zqy9lnNFi7EX8u1tmdJ4rfw

Regards,
Maxime

***** from
De: "Misbah Mubarak" <mmubarak@anl.gov>
À: "Maxime Chevalier" <maxime.chevalier@inria.fr>, codes-ross-users@lists.mcs.anl.gov
Envoyé: Vendredi 2 Juin 2017 18:54:13
Objet: Re: [codes-ross-users] Replay HPL's dumpi trace on CODES

Hi Maxime,

There is a codes-workload-dump utility that helps you inspect the traces and provides detailed information on the individual MPI operations such as number of bytes transmitted (which is derived by the data type and count). If you could run the utility with one of the traces and send me the output, I can have a look at whats going on.  Alternatively, if you could share the traces, I can have a look at those.

Using the utility is simple, here is some documentation on how to run it:

https://xgitlab.cels.anl.gov/codes/codes/wikis/codes-dumpi-workload

Thanks,
Misbah

****** from
From: <codes-ross-users-bounces@lists.mcs.anl.gov> on behalf of Maxime Chevalier <maxime.chevalier@inria.fr>
Date: Friday, June 2, 2017 at 8:52 AM
To: "codes-ross-users@lists.mcs.anl.gov" <codes-ross-users@lists.mcs.anl.gov>
Subject: Re: [codes-ross-users] Replay HPL's dumpi trace on CODES

Hi Misbah,
Thanks for your fast response. I was looking for the data type, but I don't really understand. I have figured out how to avoid "UNDEFINED DATA TYPE" errors by compiling HPL whit "HPL_NO_MPI_DATATYPE", but the output is quite the same (see trace below). I don't know if it's a step forward or backward...

Regards,
Maxime

Trace :
Fri Jun 2 09:15:49 2017
ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941
tw_net_start: Found world size to be 1
ROSS Core Configuration: 
Total Nodes 1
Total Processors [Nodes (1) x PE_per_Node (1)] 1
Total KPs [Nodes (1) x KPs (16)] 16
Total LPs 54
Simulation End Time 300000000000.00
LP-to-PE Mapping model defined

ROSS Event Memory Allocation:
Model events 13825
Network events 50000
Total events 63824
 *** START SEQUENTIAL SIMULATION ***
 *** END SIMULATION ***

LP 1 unmatched irecvs 1 unmatched sends 0 Total sends 0 receives 2 collectives 0 delays 8 wait alls 0 waits 0 send time 0.000000 wait 0.000000
LP 3 unmatched irecvs 1 unmatched sends 0 Total sends 1 receives 1 collectives 0 delays 10 wait alls 0 waits 0 send time 3.202149 wait 0.000000
LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 1 collectives 0 delays 7 wait alls 0 waits 0 send time 0.000000 wait 0.000000
LP 7 unmatched irecvs 1 unmatched sends 0 Total sends 1 receives 1 collectives 0 delays 10 wait alls 0 waits 0 send time 3.189207 wait 0.000000
: Running Time = 0.0001 seconds
TW Library Statistics:
Total Events Processed 56
Events Aborted (part of RBs) 0
Events Rolled Back 0
Event Ties Detected in PE Queues 0
Efficiency 100.00 %
Total Remote (shared mem) Events Processed 0
Percent Remote Events 0.00 %
Total Remote (network) Events Processed 0
Percent Remote Events 0.00 %
Total Roll Backs 0
Primary Roll Backs 0
Secondary Roll Backs 0
Fossil Collect Attempts 0
Total GVT Computations 0
Net Events Processed 56
Event Rate (events/sec) 823529.4
Total Events Scheduled Past End Time 0
TW Memory Statistics:
Events Allocated 63825
Memory Allocated 62573
Memory Wasted 683
TW Data Structure sizes in bytes (sizeof):
PE struct 608
KP struct 144
LP struct 128
LP Model struct 760
LP RNGs 80
Total LP 968
Event struct 144
Event struct with Model 928
TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
Priority Queue (enq/deq) 0.0000
AVL Tree (insert/delete) 0.0000
LZ4 (de)compression 0.0000
Buddy system 0.0000
Event Processing 0.0000
Event Cancel 0.0000
Event Abort 0.0000
GVT 0.0000
Fossil Collect 0.0000
Primary Rollbacks 0.0000
Network Read 0.0000
Statistics Computation 0.0000
Statistics Write 0.0000
Total Time (Note: Using Running Time above for Speedup) 0.0002
TW GVT Statistics: MPI AllReduce
GVT Interval 16
GVT Real Time Interval (cycles) 0
GVT Real Time Interval (sec) 0.00000000
Batch Size 16
Forced GVT 0
Total GVT Computations 0
Total All Reduce Calls 0
Average Reduction / GVT -nan
Total bytes sent 8 recvd 20 
max runtime 0.000000 ns avg runtime 0.000000 
max comm time 0.000000 avg comm time -69573.000000 
max send time 3.202149 avg send time 1.597839 
max recv time 45682.609151 avg recv time 11420.652288 
max wait time 0.000000 avg wait time 0.000000

******* from
De: "Misbah Mubarak" <mmubarak@anl.gov>
À: "Maxime Chevalier" <maxime.chevalier@inria.fr>, codes-ross-users@lists.mcs.anl.gov
Envoyé: Mardi 30 Mai 2017 18:12:46
Objet: Re: [codes-ross-users] Replay HPL's dumpi trace on CODES

Hi Maxime,

Thanks for your message. There seems to be a data type that is either not supported by DUMPI or CODES. Are you familiar with what data types are being used by the HPL trace? I will find out if the support for them can be added in the code. 

Regards,
Misbah

******** from 
From: <codes-ross-users-bounces@lists.mcs.anl.gov> on behalf of Maxime Chevalier <maxime.chevalier@inria.fr>
Date: Monday, May 29, 2017 at 3:51 AM
To: "codes-ross-users@lists.mcs.anl.gov" <codes-ross-users@lists.mcs.anl.gov>
Subject: [codes-ross-users] Replay HPL's dumpi trace on CODES

Hi,
I'm trying to replay HPL's DUMPI trace generated on my computer with CODES. Unfortunately, I get a lot of "Undefined data type" errors (see the trace below).
I have already replayed AMG traces (downloaded here) and replayed my own generated AMG traces. It has worked fine.
So I'm wondering if I did something wrong, or if it's HPL fault.

Best regards,
Maxime


Trace :

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941
tw_net_start: Found world size to be 1
ROSS Core Configuration: 
Total Nodes 1
Total Processors [Nodes (1) x PE_per_Node (1)] 1
Total KPs [Nodes (1) x KPs (16)] 16
Total LPs 5
Simulation End Time 300000000000.00
LP-to-PE Mapping model defined

ROSS Event Memory Allocation:
Model events 1281
Network events 50000
Total events 51280

Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type 
Undefined data type *** START SEQUENTIAL SIMULATION ***
 *** END SIMULATION ***

LP 1 unmatched irecvs 1 unmatched sends 0 Total sends 0 receives 1 collectives 0 delays 7 wait alls 0 waits 0 send time 0.000000 wait 0.000000
: Running Time = 0.0000 seconds
TW Library Statistics:
Total Events Processed 8
Events Aborted (part of RBs) 0
Events Rolled Back 0
Event Ties Detected in PE Queues 0
Efficiency 100.00 %
Total Remote (shared mem) Events Processed 0
Percent Remote Events 0.00 %
Total Remote (network) Events Processed 0
Percent Remote Events 0.00 %
Total Roll Backs 0
Primary Roll Backs 0
Secondary Roll Backs 0
Fossil Collect Attempts 0
Total GVT Computations 0
Net Events Processed 8
Event Rate (events/sec) 307692.3
Total Events Scheduled Past End Time 0
TW Memory Statistics:
Events Allocated 51281
Memory Allocated 51168
Memory Wasted 720
TW Data Structure sizes in bytes (sizeof):
PE struct 608
KP struct 144
LP struct 128
LP Model struct 760
LP RNGs 80
Total LP 968
Event struct 144
Event struct with Model 928
TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
Priority Queue (enq/deq) 0.0000
AVL Tree (insert/delete) 0.0000
LZ4 (de)compression 0.0000
Buddy system 0.0000
Event Processing 0.0000
Event Cancel 0.0000
Event Abort 0.0000
GVT 0.0000
Fossil Collect 0.0000
Primary Rollbacks 0.0000
Network Read 0.0000
Statistics Computation 0.0000
Statistics Write 0.0000
Total Time (Note: Using Running Time above for Speedup) 0.0001
TW GVT Statistics: MPI AllReduce
GVT Interval 16
GVT Real Time Interval (cycles) 0
GVT Real Time Interval (sec) 0.00000000
Batch Size 16
Forced GVT 0
Total GVT Computations 0
Total All Reduce Calls 0
Average Reduction / GVT -nan
Total bytes sent 0 recvd 4 
max runtime 0.000000 ns avg runtime 0.000000 
max comm time 0.000000 avg comm time -66232.000000 
max send time 0.000000 avg send time 0.000000 
max recv time 0.000000 avg recv time 0.000000 
max wait time 0.000000 avg wait time 0.000000 
LP-IO: writing output to hpl-trace-25282-1495543803/
LP-IO: data files:
hpl-trace-25282-1495543803/mpi-replay-stats
hpl-trace-25282-1495543803/model-net-category-all

*** 2017-06-06 mardi
**** Nouvel outils pour analyser les traces
https://xgitlab.cels.anl.gov/codes/codes/wikis/codes-dumpi-workload
Grâce aux échanges avec la mailling list, j'ai pu découvir un nouvel
outils pour analyser les traces dumpi. Il sagit de
=codes-workload-dump=. Il s'utilise comme suit :
#+BEGIN_EXAMPLE
 ./bin/codes-workload-dump --type "dumpi-trace-workload" -s --num-ranks n --dumpi-log  dumpi-trace-dir/dumpi-2014.03.06.20.34.26-
#+END_EXAMPLE

***** Documentation
"Individual operations in the DUMPI traces can be inspected by using
the codes-workload-dump utility (in src/workload). This utility
provides detailed information on each of the MPI operations in the
logs e.g. operation type (whether send/recv/wait/collective), tag
number, number of bytes transmitted (derived from the data type and
count of the MPI operation), start and end times of operation.
Note that running this utility on a large number of traces will
generate a lot of output so it is preferred to run it on a small
number of MPI ranks (<4). Using the -s command line option will
display the aggregate statistics."

-Help
#+BEGIN_EXAMPLE
Usage: codes-workload-dump --type TYPE --num-ranks N [OPTION...]
--type: type of workload ("darshan_io_workload", "iolang_workload", dumpi-trace-workload" etc.)
--num-ranks: number of ranks to process (if not set, it is set by the workload)
-s: print final workload stats
DARSHAN OPTIONS (darshan_io_workload)
--d-log: darshan log file
--d-aggregator-cnt: number of aggregators for collective I/O in darshan
IOLANG OPTIONS (iolang_workload)
--i-meta: i/o language kernel meta file path
--i-use-relpath: use i/o kernel path relative meta file path
RECORDER OPTIONS (recorder_io_workload)
--r-trace-dir: directory containing recorder trace files
--r-nprocs: number of ranks in original recorder workload
DUMPI TRACE OPTIONS (dumpi-trace-workload) 
--dumpi-log: dumpi log file 
CHECKPOINT OPTIONS (checkpoint_io_workload)
--chkpoint-size: size of aggregate checkpoint to write
--chkpoint-bw: checkpointing bandwidth
--chkpoint-iters: iteration count for checkpoint workload
--chkpoint-mtti: mean time to interrupt
MOCK IO OPTIONS (iomock_workload)
--iomock-request-type: whether to write or read
--iomock-num-requests: number of writes/reads
--iomock-request-size: size of each request
--iomock-file-id: file id to use for requests
--iomock-use-uniq-file-ids: whether to offset file ids by rank
#+END_EXAMPLE

- Exemple sur HPL
#+begin_example
...
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3057 start:2.80202e+11 end:2.80202e+11
op: app:0 rank:3 type:delay seconds:0.000123
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3058 start:2.80202e+11 end:2.80202e+11
op: app:0 rank:3 type:delay seconds:0.004705
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3061 start:2.80207e+11 end:2.80207e+11
op: app:0 rank:3 type:delay seconds:0.000114
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3062 start:2.80207e+11 end:2.80207e+11
op: app:0 rank:3 type:delay seconds:0.004304
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3065 start:2.80211e+11 end:2.80211e+11
op: app:0 rank:3 type:delay seconds:0.000120
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3066 start:2.80211e+11 end:2.80211e+11
op: app:0 rank:3 type:delay seconds:0.004374
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3069 start:2.80216e+11 end:2.80216e+11
op: app:0 rank:3 type:delay seconds:0.000114
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3070 start:2.80216e+11 end:2.80216e+11
op: app:0 rank:3 type:delay seconds:0.004264
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3073 start:2.80220e+11 end:2.80220e+11
op: app:0 rank:3 type:delay seconds:0.000109
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3074 start:2.80220e+11 end:2.80220e+11
op: app:0 rank:3 type:delay seconds:0.004151
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3077 start:2.80224e+11 end:2.80224e+11
op: app:0 rank:3 type:delay seconds:0.000131
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3078 start:2.80224e+11 end:2.80224e+11
op: app:0 rank:3 type:delay seconds:0.003925
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3081 start:2.80228e+11 end:2.80228e+11
op: app:0 rank:3 type:delay seconds:0.000134
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3082 start:2.80229e+11 end:2.80229e+11
op: app:0 rank:3 type:delay seconds:0.003889
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3085 start:2.80232e+11 end:2.80232e+11
op: app:0 rank:3 type:delay seconds:0.000091
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3086 start:2.80233e+11 end:2.80233e+11
op: app:0 rank:3 type:delay seconds:0.003731
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3089 start:2.80236e+11 end:2.80236e+11
op: app:0 rank:3 type:delay seconds:0.000097
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3090 start:2.80236e+11 end:2.80236e+11
op: app:0 rank:3 type:delay seconds:0.003528
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3093 start:2.80240e+11 end:2.80240e+11
op: app:0 rank:3 type:delay seconds:0.000095
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3094 start:2.80240e+11 end:2.80240e+11
op: app:0 rank:3 type:delay seconds:0.003257
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3097 start:2.80243e+11 end:2.80243e+11
op: app:0 rank:3 type:delay seconds:0.000107
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3098 start:2.80243e+11 end:2.80243e+11
op: app:0 rank:3 type:delay seconds:0.003171
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3101 start:2.80247e+11 end:2.80247e+11
op: app:0 rank:3 type:delay seconds:0.000114
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3102 start:2.80247e+11 end:2.80247e+11
op: app:0 rank:3 type:delay seconds:0.003044
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3105 start:2.80250e+11 end:2.80250e+11
op: app:0 rank:3 type:delay seconds:0.000107
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3106 start:2.80250e+11 end:2.80250e+11
op: app:0 rank:3 type:delay seconds:0.002954
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3109 start:2.80253e+11 end:2.80253e+11
op: app:0 rank:3 type:delay seconds:0.000117
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3110 start:2.80253e+11 end:2.80253e+11
op: app:0 rank:3 type:delay seconds:0.002820
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3113 start:2.80256e+11 end:2.80256e+11
op: app:0 rank:3 type:delay seconds:0.000106
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3114 start:2.80256e+11 end:2.80256e+11
op: app:0 rank:3 type:delay seconds:0.002725
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3117 start:2.80259e+11 end:2.80259e+11
op: app:0 rank:3 type:delay seconds:0.000087
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3118 start:2.80259e+11 end:2.80259e+11
op: app:0 rank:3 type:delay seconds:0.002479
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3121 start:2.80261e+11 end:2.80261e+11
op: app:0 rank:3 type:delay seconds:0.000098
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3122 start:2.80261e+11 end:2.80261e+11
op: app:0 rank:3 type:delay seconds:0.002460
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3125 start:2.80264e+11 end:2.80264e+11
op: app:0 rank:3 type:delay seconds:0.000093
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3126 start:2.80264e+11 end:2.80264e+11
op: app:0 rank:3 type:delay seconds:0.002207
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3129 start:2.80266e+11 end:2.80266e+11
op: app:0 rank:3 type:delay seconds:0.000093
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3130 start:2.80266e+11 end:2.80266e+11
op: app:0 rank:3 type:delay seconds:0.002091
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3133 start:2.80268e+11 end:2.80268e+11
op: app:0 rank:3 type:delay seconds:0.000116
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3134 start:2.80268e+11 end:2.80268e+11
op: app:0 rank:3 type:delay seconds:0.001936
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3137 start:2.80270e+11 end:2.80270e+11
op: app:0 rank:3 type:delay seconds:0.000105
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3138 start:2.80270e+11 end:2.80270e+11
op: app:0 rank:3 type:delay seconds:0.001636
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3141 start:2.80272e+11 end:2.80272e+11
op: app:0 rank:3 type:delay seconds:0.000130
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3142 start:2.80272e+11 end:2.80272e+11
op: app:0 rank:3 type:delay seconds:0.001605
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3145 start:2.80274e+11 end:2.80274e+11
op: app:0 rank:3 type:delay seconds:0.000117
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3146 start:2.80274e+11 end:2.80274e+11
op: app:0 rank:3 type:delay seconds:0.001385
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3149 start:2.80275e+11 end:2.80275e+11
op: app:0 rank:3 type:delay seconds:0.000110
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3150 start:2.80275e+11 end:2.80275e+11
op: app:0 rank:3 type:delay seconds:0.001184
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3153 start:2.80277e+11 end:2.80277e+11
op: app:0 rank:3 type:delay seconds:0.000123
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3154 start:2.80277e+11 end:2.80277e+11
op: app:0 rank:3 type:delay seconds:0.001118
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3157 start:2.80278e+11 end:2.80278e+11
op: app:0 rank:3 type:delay seconds:0.000116
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3158 start:2.80278e+11 end:2.80278e+11
op: app:0 rank:3 type:delay seconds:0.001138
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3161 start:2.80279e+11 end:2.80279e+11
op: app:0 rank:3 type:delay seconds:0.000117
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3162 start:2.80279e+11 end:2.80279e+11
op: app:0 rank:3 type:delay seconds:0.000875
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3165 start:2.80280e+11 end:2.80280e+11
op: app:0 rank:3 type:delay seconds:0.000110
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3166 start:2.80280e+11 end:2.80280e+11
op: app:0 rank:3 type:delay seconds:0.000732
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3169 start:2.80281e+11 end:2.80281e+11
op: app:0 rank:3 type:delay seconds:0.000108
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3170 start:2.80281e+11 end:2.80281e+11
op: app:0 rank:3 type:delay seconds:0.000598
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3173 start:2.80282e+11 end:2.80282e+11
op: app:0 rank:3 type:delay seconds:0.000155
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3174 start:2.80282e+11 end:2.80282e+11
op: app:0 rank:3 type:delay seconds:0.000260
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3177 start:2.80282e+11 end:2.80283e+11
op: app:0 rank:3 type:delay seconds:0.000128
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3178 start:2.80283e+11 end:2.80283e+11
op: app:0 rank:3 type:delay seconds:0.000141
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3181 start:2.80283e+11 end:2.80283e+11
op: app:0 rank:3 type:delay seconds:0.000098
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3182 start:2.80283e+11 end:2.80283e+11
op: app:0 rank:3 type:delay seconds:0.000078
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:1536 type:14 count:192 tag:3185 start:2.80283e+11 end:2.80284e+11
op: app:0 rank:3 type:delay seconds:0.000077
op: app:0 rank:3 type:send src:3 dst:0 bytes:1536 type:14 count:192 tag:3186 start:2.80284e+11 end:2.80284e+11
op: app:0 rank:3 type:delay seconds:0.000005
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:send src:3 dst:2 bytes:8 type:14 count:1 tag:9001 start:2.80284e+11 end:2.80284e+11
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000001
op: app:0 rank:3 type:delay seconds:0.000001
op: app:0 rank:3 type:recv src:2 dst:-1 bytes:8 type:14 count:1 tag:9001 start:2.80284e+11 end:2.80284e+11
op: app:0 rank:3 type:delay seconds:5.071062
op: app:0 rank:3 type:delay seconds:0.000005
op: app:0 rank:3 type:delay seconds:0.000006
op: app:0 rank:3 type:send src:3 dst:0 bytes:72192 type:14 count:9024 tag:9001 start:2.85355e+11 end:2.85355e+11
op: app:0 rank:3 type:delay seconds:0.000005
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:recv src:2 dst:-1 bytes:8 type:14 count:1 tag:9001 start:2.85355e+11 end:2.85410e+11
op: app:0 rank:3 type:delay seconds:0.234955
op: app:0 rank:3 type:delay seconds:0.000007
op: app:0 rank:3 type:delay seconds:0.000010
op: app:0 rank:3 type:send src:3 dst:0 bytes:72192 type:14 count:9024 tag:9001 start:2.85645e+11 end:2.85651e+11
op: app:0 rank:3 type:delay seconds:0.000016
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000003
op: app:0 rank:3 type:recv src:2 dst:-1 bytes:8 type:14 count:1 tag:9001 start:2.85651e+11 end:2.85651e+11
op: app:0 rank:3 type:delay seconds:0.000027
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000003
op: app:0 rank:3 type:send src:3 dst:0 bytes:72192 type:14 count:9024 tag:9001 start:2.85651e+11 end:2.85651e+11
op: app:0 rank:3 type:delay seconds:0.000004
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:recv src:2 dst:-1 bytes:8 type:14 count:1 tag:9001 start:2.85651e+11 end:2.85651e+11
op: app:0 rank:3 type:delay seconds:0.000003
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:recv src:2 dst:-1 bytes:8 type:14 count:1 tag:9001 start:2.85651e+11 end:2.85651e+11
op: app:0 rank:3 type:delay seconds:0.000003
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:recv src:0 dst:-1 bytes:8 type:14 count:1 tag:9001 start:2.85651e+11 end:2.85651e+11
op: app:0 rank:3 type:delay seconds:0.380798
op: app:0 rank:3 type:delay seconds:0.000005
op: app:0 rank:3 type:delay seconds:0.000007
op: app:0 rank:3 type:send src:3 dst:0 bytes:72192 type:14 count:9024 tag:9001 start:2.86032e+11 end:2.86034e+11
op: app:0 rank:3 type:delay seconds:0.000010
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:send src:3 dst:0 bytes:72192 type:14 count:9024 tag:9001 start:2.86034e+11 end:2.86034e+11
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:delay seconds:0.000001
op: app:0 rank:3 type:delay seconds:0.000002
op: app:0 rank:3 type:recv src:2 dst:-1 bytes:8 type:14 count:1 tag:9001 start:2.86034e+11 end:2.86034e+11
op: app:0 rank:3 type:delay seconds:0.080296
op: app:0 rank:3 type:end

 * * * * * FINAL STATS * * * * * *
NUM_OPENS:       0
NUM_CLOSES:      0
NUM_BARRIERS:    0
TOTAL_DELAY:     1111.8083
NUM_READS:       0
READ_SIZE:       0
NUM_WRITES:      0
WRITE_SIZE:      0
NUM_SENDS:       37377
NUM_FREES:       0
SEND_SIZE:       3357378196
NUM_RECVS:       725
RECV_SIZE:       1983852132
NUM_ISENDS:      0
ISEND_SIZE:      0
NUM_IRECVS:      36652
IRECV_SIZE:      1373526064
NUM_BCASTS:      0
BCAST_SIZE:      0
NUM_ALLGATHERS:  0
ALLGATHER_SIZE:  0
NUM_ALLGATHERVS: 0
ALLGATHERV_SIZE: 0
NUM_ALLTOALLS:   0
ALLTOALL_SIZE:   0
NUM_ALLTOALLVS:  0
ALLTOALLV_SIZE:  0
NUM_REDUCES:     0
REDUCE_SIZE:     0
NUM_ALLREDUCE:   0
ALLREDUCE_SIZE:  0
NUM_COLLECTIVE:  0
COLLECTIVE_SIZE: 0
NUM_WAITALLS:    0
NUM_WAITS:       36652
NUM_WAITSOMES:   0
NUM_WAITANYS:    0
NUM_TESTALLS:    0
#+end_example

**** Replay HPL CODES 

***** output
#+BEGIN_EXAMPLE
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=4 --workload_file=dumpi-2017.06.05.18.33.12- --workload_type=dumpi --lp-io-dir=amg-16-par --lp-io-use-suffix=1 -- /home/chevamax/Téléchargements/CODES/src/network-workloads/conf/modelnet-mpi-test.conf
model-net-mpi-replay --sync=3 --num_net_traces=4 --workload_file=dumpi-2017.06.05.18.33.12- --workload_type=dumpi --lp-io-dir=amg-16-par --lp-io-use-suffix=1 -- /home/chevamax/Téléchargements/CODES/src/network-workloads/conf/modelnet-mpi-test.conf 

Tue Jun  6 09:43:39 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 0 receives 2 collectives 0 delays 8 wait alls 0 waits 0 send time 0.000000 wait 0.000000
 LP 3 unmatched irecvs 1 unmatched sends 0 Total sends 1 receives 1 collectives 0 delays 10 wait alls 0 waits 0 send time 3.202149 wait 0.000000
 LP 5 unmatched irecvs 1 unmatched sends 0 Total sends 1 receives 2 collectives 0 delays 11 wait alls 0 waits 0 send time 3.205433 wait 0.000000
 LP 7 unmatched irecvs 1 unmatched sends 0 Total sends 1 receives 1 collectives 0 delays 10 wait alls 0 waits 0 send time 3.189207 wait 0.000000
	: Running Time = 0.0003 seconds

TW Library Statistics:
	Total Events Processed                                      69
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      8
	Total GVT Computations                                       2

	Net Events Processed                                        69
	Event Rate (events/sec)                               205970.1
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                                 0
	Remote recvs                                                 0

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0002
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0008
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.0008

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       2
	Total All Reduce Calls                                       4
	Average Reduction / GVT                                   2.00

 Total bytes sent 12 recvd 24 
 max runtime 0.000000 ns avg runtime 0.000000 
 max comm time 0.000000 avg comm time -952871.750000 
 max send time 3.205433 avg send time 2.399197 
 max recv time 10156.664791 avg recv time 2539.166198 
 max wait time 0.000000 avg wait time 0.000000 
LP-IO: writing output to amg-16-par-6512-1496735019/
LP-IO: data files:
   amg-16-par-6512-1496735019/model-net-category-all
   amg-16-par-6512-1496735019/mpi-replay-stats
   amg-16-par-6512-1496735019/model-net-category-test
#+END_EXAMPLE

Toujours le même genre d'output, mais au moins ca marche.
Je vais faire avec AMG du coup.

**** AMG

***** Installation
[[https://portal.nersc.gov/project/CAL/doe-miniapps-srcs/amg20130624.tgz][archive]] 

Verifiez la non présence d'openmpi !

****** Compilation AMG
Changement du makefile.include pour ajouter le lien
vers la librairie dumpi + quelques autres options :
#+BEGIN_EXAMPLE
INCLUDE_CFLAGS = -O2 -DTIMER_USE_MPI -HYPRE_LONG_LONG -L<path to dumpi lib> -ldumpi
INCLUDE_LFLAGS = -lm
#+END_EXAMPLE

Le =-L= est utile seulement si vous n'avez pas configuré votre =LD_LIBRARY_PATH=

****** Lancement d'AMG
#+begin_src sh :results output :exports both
mpirun -np 8 ./amg2013 -laplace -n 40 40 40 -P 2 2 2
#+end_src

Sortie : 
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Tue Jun  6 15:45:48 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 8187913.974337 wait 865246321.758128
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 7287976.135521 wait 560542941.964223
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 7536684.249505 wait 571749913.716578
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 7750926.621984 wait 933839790.970317
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 8636987.439616 wait 898909385.460124
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 8203584.988557 wait 825732294.253115
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 8281196.008655 wait 915988399.306430
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 8076730.606683 wait 838540429.296298
	: Running Time = 172.2083 seconds

TW Library Statistics:
	Total Events Processed                                 2429875
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                   2429875
	Event Rate (events/sec)                                14110.1
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        479532
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    736

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)    412.3424

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1874219246.601267 ns avg runtime 1863889122.194420 
 max comm time 933925326.488712 avg comm time 801401414.069420 
 max send time 8636987.439616 avg send time 7995250.003107 
 max recv time 2527977615.376651 avg recv time 1678766128.851202 
 max wait time 933839790.970317 avg wait time 801318684.590652 
 Average number of hops traversed 1.365391 average chunk latency 4.554955 us maximum chunk latency 84.417238 us avg message size 2408.470215 bytes finished messages 26162 finished chunks 261364 

 ADAPTIVE ROUTING STATS: 261364 chunks routed minimally 0 chunks routed non-minimally completed packets 261364 

 Total packets generated 140603 finished 140603 
#+END_EXAMPLE
****** Deprecated
******* Compilation AMG
Changement du makefile dans le sous dossier test pour ajouter le lien
vers la librairie dumpi
=-L/home/chevamax/Documents/Stage_LIG_2017/logiciels/sst-dumpi/install/lib
-ldumpi\= dans LFLAGS.

Puis faire make dans le dossier principal

******* Lancement d'AMG
#+begin_src sh :results output :exports both
mpirun -np 8 ./amg2013 -laplace -n 40 40 40 -P 2 2 2
#+end_src

******* Replay
traces téléchargées.

#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=216 --workload_file=../../df_AMG_n216_dumpi/dumpi-2014.03.03.14.55.23- --workload_type=dumpi --lp-io-dir=amg-8-trace --lp-io-use-suffix=1 -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf 

Tue Jun  6 10:40:13 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 478 receives 478 collectives 0 delays 1218 wait alls 56 waits 0 send time 612826.428503 wait 14669328.061635
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 832 receives 832 collectives 0 delays 1930 wait alls 60 waits 0 send time 1018385.000731 wait 64370657.327652
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 566 receives 566 collectives 0 delays 1398 wait alls 60 waits 0 send time 750754.503860 wait 72563845.789648
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 840 receives 840 collectives 0 delays 1946 wait alls 60 waits 0 send time 1047815.519626 wait 157051216.933560
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 464 receives 464 collectives 0 delays 1184 wait alls 50 waits 0 send time 647397.328881 wait 66705324.503492
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 386 receives 386 collectives 0 delays 1034 wait alls 56 waits 0 send time 503242.249048 wait 66246441.900921
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 654 receives 654 collectives 0 delays 1574 wait alls 60 waits 0 send time 846491.070120 wait 117358929.038023
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 790 receives 790 collectives 0 delays 1846 wait alls 60 waits 0 send time 1041571.325534 wait 70288613.070390
 LP 18 unmatched irecvs 0 unmatched sends 0 Total sends 1194 receives 1194 collectives 0 delays 2654 wait alls 60 waits 0 send time 1470155.471660 wait 68216556.238328
 LP 19 unmatched irecvs 0 unmatched sends 0 Total sends 1010 receives 1010 collectives 0 delays 2286 wait alls 60 waits 0 send time 1264102.027885 wait 66450400.852029
 LP 20 unmatched irecvs 0 unmatched sends 0 Total sends 956 receives 956 collectives 0 delays 2178 wait alls 60 waits 0 send time 1205111.164923 wait 59633424.426049
 LP 21 unmatched irecvs 0 unmatched sends 0 Total sends 528 receives 528 collectives 0 delays 1318 wait alls 56 waits 0 send time 723458.275000 wait 117875229.796452
 LP 27 unmatched irecvs 0 unmatched sends 0 Total sends 1026 receives 1026 collectives 0 delays 2324 wait alls 66 waits 0 send time 1235379.351462 wait 74247344.022784
 LP 28 unmatched irecvs 0 unmatched sends 0 Total sends 1004 receives 1004 collectives 0 delays 2274 wait alls 60 waits 0 send time 1255522.120947 wait 118504878.250013
 LP 29 unmatched irecvs 0 unmatched sends 0 Total sends 1066 receives 1066 collectives 0 delays 2398 wait alls 60 waits 0 send time 1336751.450998 wait 63630925.617630
 LP 30 unmatched irecvs 0 unmatched sends 0 Total sends 1042 receives 1042 collectives 0 delays 2356 wait alls 66 waits 0 send time 1303271.469921 wait 116979946.393221
 LP 36 unmatched irecvs 0 unmatched sends 0 Total sends 978 receives 978 collectives 0 delays 2222 wait alls 60 waits 0 send time 1264630.386888 wait 60750187.906107
 LP 37 unmatched irecvs 0 unmatched sends 0 Total sends 628 receives 628 collectives 0 delays 1522 wait alls 60 waits 0 send time 816591.158841 wait 159049402.123190
 LP 38 unmatched irecvs 0 unmatched sends 0 Total sends 756 receives 756 collectives 0 delays 1778 wait alls 60 waits 0 send time 962886.542565 wait 62828381.096530
 LP 39 unmatched irecvs 0 unmatched sends 0 Total sends 1224 receives 1224 collectives 0 delays 2714 wait alls 60 waits 0 send time 1516243.201509 wait 117957528.505235
 LP 45 unmatched irecvs 0 unmatched sends 0 Total sends 1058 receives 1058 collectives 0 delays 2382 wait alls 60 waits 0 send time 1320910.147259 wait 60146490.080305
 LP 46 unmatched irecvs 0 unmatched sends 0 Total sends 708 receives 708 collectives 0 delays 1672 wait alls 50 waits 0 send time 967492.464439 wait 158523458.934212
 LP 47 unmatched irecvs 0 unmatched sends 0 Total sends 1132 receives 1132 collectives 0 delays 2530 wait alls 60 waits 0 send time 1397302.027431 wait 74069915.010258
 LP 48 unmatched irecvs 0 unmatched sends 0 Total sends 740 receives 740 collectives 0 delays 1746 wait alls 60 waits 0 send time 928590.332869 wait 65232701.431432
 LP 54 unmatched irecvs 0 unmatched sends 0 Total sends 664 receives 664 collectives 0 delays 1594 wait alls 60 waits 0 send time 848973.949047 wait 36345958.118133
 LP 55 unmatched irecvs 0 unmatched sends 0 Total sends 674 receives 674 collectives 0 delays 1604 wait alls 50 waits 0 send time 938229.478640 wait 125516971.862382
 LP 56 unmatched irecvs 0 unmatched sends 0 Total sends 1028 receives 1028 collectives 0 delays 2328 wait alls 66 waits 0 send time 1267561.482412 wait 117911981.356183
 LP 57 unmatched irecvs 0 unmatched sends 0 Total sends 1236 receives 1236 collectives 0 delays 2744 wait alls 66 waits 0 send time 1504637.069302 wait 50347396.693650
 LP 63 unmatched irecvs 0 unmatched sends 0 Total sends 878 receives 878 collectives 0 delays 2022 wait alls 60 waits 0 send time 1146823.708011 wait 123363217.866624
 LP 64 unmatched irecvs 0 unmatched sends 0 Total sends 690 receives 690 collectives 0 delays 1646 wait alls 60 waits 0 send time 879885.796478 wait 227372017.567977
 LP 65 unmatched irecvs 0 unmatched sends 0 Total sends 646 receives 646 collectives 0 delays 1558 wait alls 60 waits 0 send time 774507.757742 wait 71956122.051773
 LP 66 unmatched irecvs 0 unmatched sends 0 Total sends 768 receives 768 collectives 0 delays 1798 wait alls 56 waits 0 send time 950385.223462 wait 158583089.307583
 LP 72 unmatched irecvs 0 unmatched sends 0 Total sends 670 receives 670 collectives 0 delays 1606 wait alls 60 waits 0 send time 849275.876803 wait 121573968.425720
 LP 73 unmatched irecvs 0 unmatched sends 0 Total sends 794 receives 794 collectives 0 delays 1854 wait alls 60 waits 0 send time 967002.342169 wait 226746422.904084
 LP 74 unmatched irecvs 0 unmatched sends 0 Total sends 676 receives 676 collectives 0 delays 1618 wait alls 60 waits 0 send time 853497.663715 wait 73897714.895026
 LP 75 unmatched irecvs 0 unmatched sends 0 Total sends 516 receives 516 collectives 0 delays 1294 wait alls 56 waits 0 send time 641329.482212 wait 184735895.207342
 LP 81 unmatched irecvs 0 unmatched sends 0 Total sends 650 receives 650 collectives 0 delays 1562 wait alls 56 waits 0 send time 858544.351669 wait 223511975.760137
 LP 82 unmatched irecvs 0 unmatched sends 0 Total sends 1008 receives 1008 collectives 0 delays 2282 wait alls 60 waits 0 send time 1287474.151241 wait 74911528.900648
 LP 83 unmatched irecvs 0 unmatched sends 0 Total sends 886 receives 886 collectives 0 delays 2038 wait alls 60 waits 0 send time 1173980.655245 wait 186291322.476267
 LP 84 unmatched irecvs 0 unmatched sends 0 Total sends 726 receives 726 collectives 0 delays 1708 wait alls 50 waits 0 send time 1006582.941256 wait 180308323.805768
 LP 90 unmatched irecvs 0 unmatched sends 0 Total sends 1080 receives 1080 collectives 0 delays 2432 wait alls 66 waits 0 send time 1358841.625623 wait 114358120.575814
 LP 91 unmatched irecvs 0 unmatched sends 0 Total sends 804 receives 804 collectives 0 delays 1874 wait alls 60 waits 0 send time 1009827.313515 wait 225479612.621467
 LP 92 unmatched irecvs 0 unmatched sends 0 Total sends 996 receives 996 collectives 0 delays 2258 wait alls 60 waits 0 send time 1292993.186195 wait 159080981.001880
 LP 93 unmatched irecvs 0 unmatched sends 0 Total sends 1044 receives 1044 collectives 0 delays 2344 wait alls 50 waits 0 send time 1409568.727912 wait 123803484.427334
 LP 99 unmatched irecvs 0 unmatched sends 0 Total sends 1010 receives 1010 collectives 0 delays 2276 wait alls 50 waits 0 send time 1392107.978241 wait 72580169.316250
 LP 100 unmatched irecvs 0 unmatched sends 0 Total sends 1952 receives 1952 collectives 0 delays 4182 wait alls 72 waits 0 send time 2346398.054515 wait 223514663.875979
 LP 101 unmatched irecvs 0 unmatched sends 0 Total sends 998 receives 998 collectives 0 delays 2252 wait alls 50 waits 0 send time 1362987.743583 wait 178904814.966390
 LP 102 unmatched irecvs 0 unmatched sends 0 Total sends 1016 receives 1016 collectives 0 delays 2298 wait alls 60 waits 0 send time 1296163.307307 wait 159821204.649910
 LP 108 unmatched irecvs 0 unmatched sends 0 Total sends 858 receives 858 collectives 0 delays 1982 wait alls 60 waits 0 send time 1098585.436320 wait 61204099.545769
 LP 109 unmatched irecvs 0 unmatched sends 0 Total sends 1852 receives 1852 collectives 0 delays 3980 wait alls 70 waits 0 send time 2231130.888345 wait 239282149.488297
 LP 110 unmatched irecvs 0 unmatched sends 0 Total sends 1036 receives 1036 collectives 0 delays 2328 wait alls 50 waits 0 send time 1417221.694278 wait 231406965.247936
 LP 111 unmatched irecvs 0 unmatched sends 0 Total sends 1096 receives 1096 collectives 0 delays 2448 wait alls 50 waits 0 send time 1469905.820924 wait 242108030.842040
 LP 117 unmatched irecvs 0 unmatched sends 0 Total sends 1508 receives 1508 collectives 0 delays 3282 wait alls 60 waits 0 send time 1893400.232530 wait 227244561.724871
 LP 118 unmatched irecvs 0 unmatched sends 0 Total sends 704 receives 704 collectives 0 delays 1664 wait alls 50 waits 0 send time 987026.752725 wait 182374233.768544
 LP 119 unmatched irecvs 0 unmatched sends 0 Total sends 708 receives 708 collectives 0 delays 1672 wait alls 50 waits 0 send time 980978.812301 wait 13967711.361794
 LP 120 unmatched irecvs 0 unmatched sends 0 Total sends 1344 receives 1344 collectives 0 delays 2954 wait alls 60 waits 0 send time 1720179.012817 wait 35015531.040038
 LP 126 unmatched irecvs 0 unmatched sends 0 Total sends 1614 receives 1614 collectives 0 delays 3494 wait alls 60 waits 0 send time 2023971.208713 wait 182946647.373766
 LP 127 unmatched irecvs 0 unmatched sends 0 Total sends 1642 receives 1642 collectives 0 delays 3550 wait alls 60 waits 0 send time 2015590.811251 wait 184289237.361690
 LP 128 unmatched irecvs 0 unmatched sends 0 Total sends 1346 receives 1346 collectives 0 delays 2958 wait alls 60 waits 0 send time 1700357.502209 wait 243228804.840325
 LP 129 unmatched irecvs 0 unmatched sends 0 Total sends 1214 receives 1214 collectives 0 delays 2694 wait alls 60 waits 0 send time 1518689.918331 wait 179953424.617609
 LP 135 unmatched irecvs 0 unmatched sends 0 Total sends 926 receives 926 collectives 0 delays 2118 wait alls 60 waits 0 send time 1246349.675652 wait 237034450.909029
 LP 136 unmatched irecvs 0 unmatched sends 0 Total sends 954 receives 954 collectives 0 delays 2164 wait alls 50 waits 0 send time 1352503.447149 wait 235505287.971080
 LP 137 unmatched irecvs 0 unmatched sends 0 Total sends 1460 receives 1460 collectives 0 delays 3196 wait alls 70 waits 0 send time 1852326.853503 wait 236632839.250267
 LP 138 unmatched irecvs 0 unmatched sends 0 Total sends 1066 receives 1066 collectives 0 delays 2388 wait alls 50 waits 0 send time 1409283.182707 wait 235555608.684319
 LP 144 unmatched irecvs 0 unmatched sends 0 Total sends 1752 receives 1752 collectives 0 delays 3776 wait alls 66 waits 0 send time 2130807.570266 wait 225930646.267494
 LP 145 unmatched irecvs 0 unmatched sends 0 Total sends 1130 receives 1130 collectives 0 delays 2532 wait alls 66 waits 0 send time 1388729.942234 wait 242848402.532029
 LP 146 unmatched irecvs 0 unmatched sends 0 Total sends 718 receives 718 collectives 0 delays 1698 wait alls 56 waits 0 send time 930406.156660 wait 229788144.347419
 LP 147 unmatched irecvs 0 unmatched sends 0 Total sends 670 receives 670 collectives 0 delays 1596 wait alls 50 waits 0 send time 945853.904861 wait 234202698.028066
 LP 153 unmatched irecvs 0 unmatched sends 0 Total sends 1562 receives 1562 collectives 0 delays 3396 wait alls 66 waits 0 send time 1893068.247829 wait 44977957.049517
 LP 154 unmatched irecvs 0 unmatched sends 0 Total sends 780 receives 780 collectives 0 delays 1826 wait alls 60 waits 0 send time 1038846.624604 wait 228435021.601487
 LP 155 unmatched irecvs 0 unmatched sends 0 Total sends 884 receives 884 collectives 0 delays 2034 wait alls 60 waits 0 send time 1153261.695121 wait 242093904.982951
 LP 156 unmatched irecvs 0 unmatched sends 0 Total sends 704 receives 704 collectives 0 delays 1674 wait alls 60 waits 0 send time 883647.081994 wait 226714793.456252
 LP 162 unmatched irecvs 0 unmatched sends 0 Total sends 848 receives 848 collectives 0 delays 1968 wait alls 66 waits 0 send time 1056655.404335 wait 157175170.273683
 LP 163 unmatched irecvs 0 unmatched sends 0 Total sends 1212 receives 1212 collectives 0 delays 2696 wait alls 66 waits 0 send time 1473940.846421 wait 28336559.684429
 LP 164 unmatched irecvs 0 unmatched sends 0 Total sends 1194 receives 1194 collectives 0 delays 2660 wait alls 66 waits 0 send time 1440356.469666 wait 151749253.578270
 LP 165 unmatched irecvs 0 unmatched sends 0 Total sends 682 receives 682 collectives 0 delays 1620 wait alls 50 waits 0 send time 955091.139227 wait 161779088.065750
 LP 171 unmatched irecvs 0 unmatched sends 0 Total sends 944 receives 944 collectives 0 delays 2154 wait alls 60 waits 0 send time 1221491.539740 wait 43063209.056568
 LP 172 unmatched irecvs 0 unmatched sends 0 Total sends 602 receives 602 collectives 0 delays 1470 wait alls 60 waits 0 send time 784025.269904 wait 233804447.033664
 LP 173 unmatched irecvs 0 unmatched sends 0 Total sends 942 receives 942 collectives 0 delays 2150 wait alls 60 waits 0 send time 1229456.029328 wait 50172193.310056
 LP 174 unmatched irecvs 0 unmatched sends 0 Total sends 968 receives 968 collectives 0 delays 2192 wait alls 50 waits 0 send time 1319939.372680 wait 227463054.419840
 LP 180 unmatched irecvs 0 unmatched sends 0 Total sends 1024 receives 1024 collectives 0 delays 2304 wait alls 50 waits 0 send time 1400669.257240 wait 55674515.977710
 LP 181 unmatched irecvs 0 unmatched sends 0 Total sends 1454 receives 1454 collectives 0 delays 3174 wait alls 60 waits 0 send time 1816148.078225 wait 181507834.768630
 LP 182 unmatched irecvs 0 unmatched sends 0 Total sends 1544 receives 1544 collectives 0 delays 3354 wait alls 60 waits 0 send time 1911634.255754 wait 224604363.142339
 LP 183 unmatched irecvs 0 unmatched sends 0 Total sends 1078 receives 1078 collectives 0 delays 2422 wait alls 60 waits 0 send time 1309799.522605 wait 230384066.827760
 LP 189 unmatched irecvs 0 unmatched sends 0 Total sends 1054 receives 1054 collectives 0 delays 2374 wait alls 60 waits 0 send time 1362222.775894 wait 189425317.992194
 LP 190 unmatched irecvs 0 unmatched sends 0 Total sends 1702 receives 1702 collectives 0 delays 3670 wait alls 60 waits 0 send time 2070140.487704 wait 117181727.419880
 LP 191 unmatched irecvs 0 unmatched sends 0 Total sends 1096 receives 1096 collectives 0 delays 2448 wait alls 50 waits 0 send time 1452532.883894 wait 224114053.684710
 LP 192 unmatched irecvs 0 unmatched sends 0 Total sends 1548 receives 1548 collectives 0 delays 3362 wait alls 60 waits 0 send time 1914415.247092 wait 227210177.590556
 LP 198 unmatched irecvs 0 unmatched sends 0 Total sends 1014 receives 1014 collectives 0 delays 2284 wait alls 50 waits 0 send time 1393252.678999 wait 220299486.476730
 LP 199 unmatched irecvs 0 unmatched sends 0 Total sends 1390 receives 1390 collectives 0 delays 3056 wait alls 70 waits 0 send time 1669083.321960 wait 241003019.512698
 LP 200 unmatched irecvs 0 unmatched sends 0 Total sends 1628 receives 1628 collectives 0 delays 3534 wait alls 72 waits 0 send time 1969004.464915 wait 185460021.241456
 LP 201 unmatched irecvs 0 unmatched sends 0 Total sends 1666 receives 1666 collectives 0 delays 3598 wait alls 60 waits 0 send time 2080485.562014 wait 227932490.141174
 LP 207 unmatched irecvs 0 unmatched sends 0 Total sends 1722 receives 1722 collectives 0 delays 3710 wait alls 60 waits 0 send time 2139705.407085 wait 176512745.363789
 LP 208 unmatched irecvs 0 unmatched sends 0 Total sends 1458 receives 1458 collectives 0 delays 3182 wait alls 60 waits 0 send time 1843616.685628 wait 189486438.128715
 LP 209 unmatched irecvs 0 unmatched sends 0 Total sends 1538 receives 1538 collectives 0 delays 3342 wait alls 60 waits 0 send time 1918725.054495 wait 239318145.981069
 LP 210 unmatched irecvs 0 unmatched sends 0 Total sends 740 receives 740 collectives 0 delays 1736 wait alls 50 waits 0 send time 1016436.312454 wait 231850048.656625
 LP 216 unmatched irecvs 0 unmatched sends 0 Total sends 992 receives 992 collectives 0 delays 2250 wait alls 60 waits 0 send time 1267154.230789 wait 177495309.095000
 LP 217 unmatched irecvs 0 unmatched sends 0 Total sends 1456 receives 1456 collectives 0 delays 3178 wait alls 60 waits 0 send time 1821569.537089 wait 116792071.752879
 LP 218 unmatched irecvs 0 unmatched sends 0 Total sends 1050 receives 1050 collectives 0 delays 2356 wait alls 50 waits 0 send time 1436108.782684 wait 37291272.684969
 LP 219 unmatched irecvs 0 unmatched sends 0 Total sends 1584 receives 1584 collectives 0 delays 3434 wait alls 60 waits 0 send time 1973944.999668 wait 49904082.837897
 LP 225 unmatched irecvs 0 unmatched sends 0 Total sends 1012 receives 1012 collectives 0 delays 2280 wait alls 50 waits 0 send time 1388698.228261 wait 126694818.180600
 LP 226 unmatched irecvs 0 unmatched sends 0 Total sends 1028 receives 1028 collectives 0 delays 2322 wait alls 60 waits 0 send time 1320794.752760 wait 118546753.671267
 LP 227 unmatched irecvs 0 unmatched sends 0 Total sends 822 receives 822 collectives 0 delays 1910 wait alls 60 waits 0 send time 1043424.735314 wait 163767703.897466
 LP 228 unmatched irecvs 0 unmatched sends 0 Total sends 724 receives 724 collectives 0 delays 1704 wait alls 50 waits 0 send time 1019108.346654 wait 120293854.572289
 LP 234 unmatched irecvs 0 unmatched sends 0 Total sends 1170 receives 1170 collectives 0 delays 2606 wait alls 60 waits 0 send time 1477524.275769 wait 163838276.502195
 LP 235 unmatched irecvs 0 unmatched sends 0 Total sends 1080 receives 1080 collectives 0 delays 2426 wait alls 60 waits 0 send time 1362518.229356 wait 72469903.266244
 LP 236 unmatched irecvs 0 unmatched sends 0 Total sends 1034 receives 1034 collectives 0 delays 2340 wait alls 66 waits 0 send time 1305745.383152 wait 181265556.555207
 LP 237 unmatched irecvs 0 unmatched sends 0 Total sends 736 receives 736 collectives 0 delays 1738 wait alls 60 waits 0 send time 930078.205976 wait 124599280.836448
 LP 243 unmatched irecvs 0 unmatched sends 0 Total sends 748 receives 748 collectives 0 delays 1762 wait alls 60 waits 0 send time 944942.837639 wait 226684255.961120
 LP 244 unmatched irecvs 0 unmatched sends 0 Total sends 1266 receives 1266 collectives 0 delays 2798 wait alls 60 waits 0 send time 1559772.421147 wait 224068310.261899
 LP 245 unmatched irecvs 0 unmatched sends 0 Total sends 882 receives 882 collectives 0 delays 2030 wait alls 60 waits 0 send time 1171132.628482 wait 59646838.696376
 LP 246 unmatched irecvs 0 unmatched sends 0 Total sends 884 receives 884 collectives 0 delays 2034 wait alls 60 waits 0 send time 1169541.450742 wait 230349262.080623
 LP 252 unmatched irecvs 0 unmatched sends 0 Total sends 1132 receives 1132 collectives 0 delays 2530 wait alls 60 waits 0 send time 1419571.729277 wait 117445327.827391
 LP 253 unmatched irecvs 0 unmatched sends 0 Total sends 496 receives 496 collectives 0 delays 1248 wait alls 50 waits 0 send time 697422.312509 wait 51882411.210824
 LP 254 unmatched irecvs 0 unmatched sends 0 Total sends 668 receives 668 collectives 0 delays 1592 wait alls 50 waits 0 send time 897448.488961 wait 220915271.187779
 LP 255 unmatched irecvs 0 unmatched sends 0 Total sends 1482 receives 1482 collectives 0 delays 3230 wait alls 60 waits 0 send time 1843559.665634 wait 125682220.057065
 LP 261 unmatched irecvs 0 unmatched sends 0 Total sends 1732 receives 1732 collectives 0 delays 3730 wait alls 60 waits 0 send time 2055488.948681 wait 171337339.221986
 LP 262 unmatched irecvs 0 unmatched sends 0 Total sends 2020 receives 2020 collectives 0 delays 4316 wait alls 70 waits 0 send time 2390286.865081 wait 179397186.992181
 LP 263 unmatched irecvs 0 unmatched sends 0 Total sends 1038 receives 1038 collectives 0 delays 2332 wait alls 50 waits 0 send time 1400244.520539 wait 78380897.412120
 LP 264 unmatched irecvs 0 unmatched sends 0 Total sends 854 receives 854 collectives 0 delays 1974 wait alls 60 waits 0 send time 1130986.389140 wait 185598053.656403
 LP 270 unmatched irecvs 0 unmatched sends 0 Total sends 1318 receives 1318 collectives 0 delays 2912 wait alls 70 waits 0 send time 1605993.188225 wait 183441365.822489
 LP 271 unmatched irecvs 0 unmatched sends 0 Total sends 1494 receives 1494 collectives 0 delays 3254 wait alls 60 waits 0 send time 1853253.940284 wait 237731108.339606
 LP 272 unmatched irecvs 0 unmatched sends 0 Total sends 1114 receives 1114 collectives 0 delays 2484 wait alls 50 waits 0 send time 1506073.121325 wait 187989091.856423
 LP 273 unmatched irecvs 0 unmatched sends 0 Total sends 1504 receives 1504 collectives 0 delays 3274 wait alls 60 waits 0 send time 1925987.933597 wait 186961890.194399
 LP 279 unmatched irecvs 0 unmatched sends 0 Total sends 1284 receives 1284 collectives 0 delays 2834 wait alls 60 waits 0 send time 1699702.868855 wait 41587294.618191
 LP 280 unmatched irecvs 0 unmatched sends 0 Total sends 962 receives 962 collectives 0 delays 2190 wait alls 60 waits 0 send time 1268313.802547 wait 56179800.029834
 LP 281 unmatched irecvs 0 unmatched sends 0 Total sends 772 receives 772 collectives 0 delays 1800 wait alls 50 waits 0 send time 1045713.702199 wait 242366453.174915
 LP 282 unmatched irecvs 0 unmatched sends 0 Total sends 1444 receives 1444 collectives 0 delays 3154 wait alls 60 waits 0 send time 1827816.130192 wait 239838693.289420
 LP 288 unmatched irecvs 0 unmatched sends 0 Total sends 1086 receives 1086 collectives 0 delays 2428 wait alls 50 waits 0 send time 1495648.511026 wait 242109938.601502
 LP 289 unmatched irecvs 0 unmatched sends 0 Total sends 1474 receives 1474 collectives 0 delays 3214 wait alls 60 waits 0 send time 1866654.582048 wait 239148605.136481
 LP 290 unmatched irecvs 0 unmatched sends 0 Total sends 1666 receives 1666 collectives 0 delays 3598 wait alls 60 waits 0 send time 2074126.473026 wait 231683272.233514
 LP 291 unmatched irecvs 0 unmatched sends 0 Total sends 1270 receives 1270 collectives 0 delays 2806 wait alls 60 waits 0 send time 1570096.533302 wait 229041214.090651
 LP 297 unmatched irecvs 0 unmatched sends 0 Total sends 1168 receives 1168 collectives 0 delays 2602 wait alls 60 waits 0 send time 1478174.260760 wait 228006413.133874
 LP 298 unmatched irecvs 0 unmatched sends 0 Total sends 1854 receives 1854 collectives 0 delays 3984 wait alls 70 waits 0 send time 2235385.680993 wait 54407837.495879
 LP 299 unmatched irecvs 0 unmatched sends 0 Total sends 1530 receives 1530 collectives 0 delays 3326 wait alls 60 waits 0 send time 1918623.175208 wait 43435109.217533
 LP 300 unmatched irecvs 0 unmatched sends 0 Total sends 1688 receives 1688 collectives 0 delays 3642 wait alls 60 waits 0 send time 2073585.683253 wait 180443386.817762
 LP 306 unmatched irecvs 0 unmatched sends 0 Total sends 1006 receives 1006 collectives 0 delays 2268 wait alls 50 waits 0 send time 1369678.066230 wait 234827300.894513
 LP 307 unmatched irecvs 0 unmatched sends 0 Total sends 1182 receives 1182 collectives 0 delays 2630 wait alls 60 waits 0 send time 1453872.587879 wait 225128096.688900
 LP 308 unmatched irecvs 0 unmatched sends 0 Total sends 774 receives 774 collectives 0 delays 1814 wait alls 60 waits 0 send time 986412.539371 wait 238588346.901442
 LP 309 unmatched irecvs 0 unmatched sends 0 Total sends 1064 receives 1064 collectives 0 delays 2394 wait alls 60 waits 0 send time 1368119.135462 wait 230947991.556343
 LP 315 unmatched irecvs 0 unmatched sends 0 Total sends 676 receives 676 collectives 0 delays 1608 wait alls 50 waits 0 send time 925767.213918 wait 229314849.631709
 LP 316 unmatched irecvs 0 unmatched sends 0 Total sends 1082 receives 1082 collectives 0 delays 2430 wait alls 60 waits 0 send time 1359575.569396 wait 241533502.538067
 LP 317 unmatched irecvs 0 unmatched sends 0 Total sends 958 receives 958 collectives 0 delays 2182 wait alls 60 waits 0 send time 1216238.557433 wait 235232023.804240
 LP 318 unmatched irecvs 0 unmatched sends 0 Total sends 544 receives 544 collectives 0 delays 1350 wait alls 56 waits 0 send time 728295.820487 wait 241818960.780068
 LP 324 unmatched irecvs 0 unmatched sends 0 Total sends 942 receives 942 collectives 0 delays 2156 wait alls 66 waits 0 send time 1140182.089447 wait 47864710.306853
 LP 325 unmatched irecvs 0 unmatched sends 0 Total sends 754 receives 754 collectives 0 delays 1774 wait alls 60 waits 0 send time 1023713.128732 wait 239851624.351898
 LP 326 unmatched irecvs 0 unmatched sends 0 Total sends 1178 receives 1178 collectives 0 delays 2622 wait alls 60 waits 0 send time 1446818.706926 wait 227632766.612120
 LP 327 unmatched irecvs 0 unmatched sends 0 Total sends 682 receives 682 collectives 0 delays 1620 wait alls 50 waits 0 send time 948953.171935 wait 238425191.492895
 LP 333 unmatched irecvs 0 unmatched sends 0 Total sends 930 receives 930 collectives 0 delays 2126 wait alls 60 waits 0 send time 1197751.448529 wait 235173176.706286
 LP 334 unmatched irecvs 0 unmatched sends 0 Total sends 748 receives 748 collectives 0 delays 1762 wait alls 60 waits 0 send time 921654.263274 wait 227369786.169390
 LP 335 unmatched irecvs 0 unmatched sends 0 Total sends 904 receives 904 collectives 0 delays 2074 wait alls 60 waits 0 send time 1175455.369354 wait 239444069.426787
 LP 336 unmatched irecvs 0 unmatched sends 0 Total sends 1574 receives 1574 collectives 0 delays 3420 wait alls 66 waits 0 send time 1909375.763095 wait 240876821.702868
 LP 342 unmatched irecvs 0 unmatched sends 0 Total sends 1064 receives 1064 collectives 0 delays 2384 wait alls 50 waits 0 send time 1431078.085745 wait 239582472.113191
 LP 343 unmatched irecvs 0 unmatched sends 0 Total sends 1022 receives 1022 collectives 0 delays 2300 wait alls 50 waits 0 send time 1392639.599693 wait 54306997.881879
 LP 344 unmatched irecvs 0 unmatched sends 0 Total sends 1392 receives 1392 collectives 0 delays 3050 wait alls 60 waits 0 send time 1762868.547964 wait 237377876.573738
 LP 345 unmatched irecvs 0 unmatched sends 0 Total sends 732 receives 732 collectives 0 delays 1720 wait alls 50 waits 0 send time 1009808.307314 wait 229509575.923357
 LP 351 unmatched irecvs 0 unmatched sends 0 Total sends 1096 receives 1096 collectives 0 delays 2458 wait alls 60 waits 0 send time 1395358.000737 wait 239002588.009873
 LP 352 unmatched irecvs 0 unmatched sends 0 Total sends 1082 receives 1082 collectives 0 delays 2420 wait alls 50 waits 0 send time 1468959.213742 wait 238612525.266457
 LP 353 unmatched irecvs 0 unmatched sends 0 Total sends 1540 receives 1540 collectives 0 delays 3346 wait alls 60 waits 0 send time 1910908.953035 wait 236151256.616066
 LP 354 unmatched irecvs 0 unmatched sends 0 Total sends 1070 receives 1070 collectives 0 delays 2396 wait alls 50 waits 0 send time 1460741.955959 wait 244232512.426837
 LP 360 unmatched irecvs 0 unmatched sends 0 Total sends 1070 receives 1070 collectives 0 delays 2396 wait alls 50 waits 0 send time 1439992.987388 wait 232491468.639140
 LP 361 unmatched irecvs 0 unmatched sends 0 Total sends 1002 receives 1002 collectives 0 delays 2270 wait alls 60 waits 0 send time 1268899.691438 wait 234563596.485585
 LP 362 unmatched irecvs 0 unmatched sends 0 Total sends 736 receives 736 collectives 0 delays 1728 wait alls 50 waits 0 send time 1047246.540606 wait 226312200.080464
 LP 363 unmatched irecvs 0 unmatched sends 0 Total sends 972 receives 972 collectives 0 delays 2200 wait alls 50 waits 0 send time 1346636.251382 wait 234003752.808560
 LP 369 unmatched irecvs 0 unmatched sends 0 Total sends 1762 receives 1762 collectives 0 delays 3790 wait alls 60 waits 0 send time 2158949.638434 wait 240479668.726626
 LP 370 unmatched irecvs 0 unmatched sends 0 Total sends 1684 receives 1684 collectives 0 delays 3634 wait alls 60 waits 0 send time 2045615.450660 wait 233456691.814360
 LP 371 unmatched irecvs 0 unmatched sends 0 Total sends 1410 receives 1410 collectives 0 delays 3086 wait alls 60 waits 0 send time 1793352.499286 wait 239029658.233294
 LP 372 unmatched irecvs 0 unmatched sends 0 Total sends 948 receives 948 collectives 0 delays 2162 wait alls 60 waits 0 send time 1232901.018381 wait 236571751.867198
 LP 378 unmatched irecvs 0 unmatched sends 0 Total sends 954 receives 954 collectives 0 delays 2174 wait alls 60 waits 0 send time 1252010.787490 wait 28872020.499920
 LP 379 unmatched irecvs 0 unmatched sends 0 Total sends 1270 receives 1270 collectives 0 delays 2806 wait alls 60 waits 0 send time 1627886.750878 wait 223550083.236020
 LP 380 unmatched irecvs 0 unmatched sends 0 Total sends 1506 receives 1506 collectives 0 delays 3278 wait alls 60 waits 0 send time 1877065.936016 wait 44484845.249219
 LP 381 unmatched irecvs 0 unmatched sends 0 Total sends 980 receives 980 collectives 0 delays 2216 wait alls 50 waits 0 send time 1347203.572137 wait 37974722.570828
 LP 387 unmatched irecvs 0 unmatched sends 0 Total sends 1702 receives 1702 collectives 0 delays 3676 wait alls 66 waits 0 send time 2071769.522280 wait 120930600.863587
 LP 388 unmatched irecvs 0 unmatched sends 0 Total sends 846 receives 846 collectives 0 delays 1958 wait alls 60 waits 0 send time 1121676.269522 wait 236390395.372274
 LP 389 unmatched irecvs 0 unmatched sends 0 Total sends 644 receives 644 collectives 0 delays 1554 wait alls 60 waits 0 send time 849074.614689 wait 239583967.466475
 LP 390 unmatched irecvs 0 unmatched sends 0 Total sends 898 receives 898 collectives 0 delays 2062 wait alls 60 waits 0 send time 1194248.256042 wait 77551255.308577
 LP 396 unmatched irecvs 0 unmatched sends 0 Total sends 1394 receives 1394 collectives 0 delays 3066 wait alls 72 waits 0 send time 1680447.482774 wait 181954755.003045
 LP 397 unmatched irecvs 0 unmatched sends 0 Total sends 1448 receives 1448 collectives 0 delays 3168 wait alls 66 waits 0 send time 1730700.253292 wait 179452043.558964
 LP 398 unmatched irecvs 0 unmatched sends 0 Total sends 1090 receives 1090 collectives 0 delays 2446 wait alls 60 waits 0 send time 1362520.825888 wait 184339756.060921
 LP 399 unmatched irecvs 0 unmatched sends 0 Total sends 964 receives 964 collectives 0 delays 2194 wait alls 60 waits 0 send time 1163915.966380 wait 226976312.408118
 LP 405 unmatched irecvs 0 unmatched sends 0 Total sends 654 receives 654 collectives 0 delays 1574 wait alls 60 waits 0 send time 785996.187805 wait 235819906.841401
 LP 406 unmatched irecvs 0 unmatched sends 0 Total sends 654 receives 654 collectives 0 delays 1570 wait alls 56 waits 0 send time 842502.145936 wait 182925092.226000
 LP 407 unmatched irecvs 0 unmatched sends 0 Total sends 734 receives 734 collectives 0 delays 1730 wait alls 56 waits 0 send time 924714.183380 wait 188442299.926165
 LP 408 unmatched irecvs 0 unmatched sends 0 Total sends 1032 receives 1032 collectives 0 delays 2340 wait alls 70 waits 0 send time 1235125.968845 wait 161082373.801387
 LP 414 unmatched irecvs 0 unmatched sends 0 Total sends 778 receives 778 collectives 0 delays 1828 wait alls 66 waits 0 send time 972096.350793 wait 231361833.270194
 LP 415 unmatched irecvs 0 unmatched sends 0 Total sends 726 receives 726 collectives 0 delays 1724 wait alls 66 waits 0 send time 840037.942789 wait 161424164.007532
 LP 416 unmatched irecvs 0 unmatched sends 0 Total sends 784 receives 784 collectives 0 delays 1836 wait alls 62 waits 0 send time 991389.414077 wait 182640364.070923
 LP 417 unmatched irecvs 0 unmatched sends 0 Total sends 1380 receives 1380 collectives 0 delays 3036 wait alls 70 waits 0 send time 1648899.118545 wait 233117396.568241
 LP 423 unmatched irecvs 0 unmatched sends 0 Total sends 690 receives 690 collectives 0 delays 1636 wait alls 50 waits 0 send time 973248.559611 wait 162699136.436757
 LP 424 unmatched irecvs 0 unmatched sends 0 Total sends 682 receives 682 collectives 0 delays 1620 wait alls 50 waits 0 send time 955682.437545 wait 230925787.404967
 LP 425 unmatched irecvs 0 unmatched sends 0 Total sends 1206 receives 1206 collectives 0 delays 2678 wait alls 60 waits 0 send time 1487774.469349 wait 228363345.677702
 LP 426 unmatched irecvs 0 unmatched sends 0 Total sends 728 receives 728 collectives 0 delays 1722 wait alls 60 waits 0 send time 912367.151058 wait 229756244.240830
 LP 432 unmatched irecvs 0 unmatched sends 0 Total sends 722 receives 722 collectives 0 delays 1710 wait alls 60 waits 0 send time 911450.421544 wait 40130943.189459
 LP 433 unmatched irecvs 0 unmatched sends 0 Total sends 1064 receives 1064 collectives 0 delays 2394 wait alls 60 waits 0 send time 1361151.609776 wait 74152714.250620
 LP 434 unmatched irecvs 0 unmatched sends 0 Total sends 972 receives 972 collectives 0 delays 2210 wait alls 60 waits 0 send time 1252078.728616 wait 227835027.875529
 LP 435 unmatched irecvs 0 unmatched sends 0 Total sends 1156 receives 1156 collectives 0 delays 2578 wait alls 60 waits 0 send time 1447239.062585 wait 228657241.661385
 LP 441 unmatched irecvs 0 unmatched sends 0 Total sends 788 receives 788 collectives 0 delays 1842 wait alls 60 waits 0 send time 1062313.289631 wait 237410160.582239
 LP 442 unmatched irecvs 0 unmatched sends 0 Total sends 884 receives 884 collectives 0 delays 2034 wait alls 60 waits 0 send time 1089451.411746 wait 121658383.355642
 LP 443 unmatched irecvs 0 unmatched sends 0 Total sends 950 receives 950 collectives 0 delays 2166 wait alls 60 waits 0 send time 1155609.993471 wait 164773488.395418
 LP 444 unmatched irecvs 0 unmatched sends 0 Total sends 1086 receives 1086 collectives 0 delays 2438 wait alls 60 waits 0 send time 1362374.449758 wait 161352193.949322
 LP 450 unmatched irecvs 0 unmatched sends 0 Total sends 1188 receives 1188 collectives 0 delays 2652 wait alls 70 waits 0 send time 1477398.179498 wait 120814719.110398
 LP 451 unmatched irecvs 0 unmatched sends 0 Total sends 1102 receives 1102 collectives 0 delays 2470 wait alls 60 waits 0 send time 1351933.540047 wait 120456427.367757
 LP 452 unmatched irecvs 0 unmatched sends 0 Total sends 1612 receives 1612 collectives 0 delays 3502 wait alls 72 waits 0 send time 1880752.859434 wait 226689385.494537
 LP 453 unmatched irecvs 0 unmatched sends 0 Total sends 694 receives 694 collectives 0 delays 1654 wait alls 60 waits 0 send time 878190.189927 wait 119678148.347902
 LP 459 unmatched irecvs 0 unmatched sends 0 Total sends 934 receives 934 collectives 0 delays 2140 wait alls 66 waits 0 send time 1122283.617665 wait 124098407.652423
 LP 460 unmatched irecvs 0 unmatched sends 0 Total sends 896 receives 896 collectives 0 delays 2058 wait alls 60 waits 0 send time 1150567.877600 wait 187384767.985001
 LP 461 unmatched irecvs 0 unmatched sends 0 Total sends 1174 receives 1174 collectives 0 delays 2614 wait alls 60 waits 0 send time 1423410.518199 wait 183216359.475160
 LP 462 unmatched irecvs 0 unmatched sends 0 Total sends 804 receives 804 collectives 0 delays 1874 wait alls 60 waits 0 send time 1074091.749002 wait 77431811.628409
 LP 468 unmatched irecvs 0 unmatched sends 0 Total sends 970 receives 970 collectives 0 delays 2206 wait alls 60 waits 0 send time 1211009.781027 wait 117800903.942812
 LP 469 unmatched irecvs 0 unmatched sends 0 Total sends 606 receives 606 collectives 0 delays 1478 wait alls 60 waits 0 send time 809266.469347 wait 231986628.903106
 LP 470 unmatched irecvs 0 unmatched sends 0 Total sends 464 receives 464 collectives 0 delays 1190 wait alls 56 waits 0 send time 584690.327968 wait 162310616.991503
 LP 471 unmatched irecvs 0 unmatched sends 0 Total sends 736 receives 736 collectives 0 delays 1744 wait alls 66 waits 0 send time 940218.685630 wait 186896575.091691
 LP 477 unmatched irecvs 0 unmatched sends 0 Total sends 866 receives 866 collectives 0 delays 1998 wait alls 60 waits 0 send time 1009355.984707 wait 48347050.325136
 LP 478 unmatched irecvs 0 unmatched sends 0 Total sends 776 receives 776 collectives 0 delays 1818 wait alls 60 waits 0 send time 934262.453814 wait 237456164.552619
 LP 479 unmatched irecvs 0 unmatched sends 0 Total sends 576 receives 576 collectives 0 delays 1418 wait alls 60 waits 0 send time 751647.854004 wait 235302036.020718
 LP 480 unmatched irecvs 0 unmatched sends 0 Total sends 468 receives 468 collectives 0 delays 1198 wait alls 56 waits 0 send time 572385.313953 wait 226989333.384371
	: Running Time = 11.0548 seconds

TW Library Statistics:
	Total Events Processed                                11540841
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                  11540841
	Event Rate (events/sec)                              1043967.8
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        479532
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    736

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)     26.4695

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 136925280 recvd 136925280 
 max runtime 832428174.181527 ns avg runtime 831592158.607247 
 max comm time 244256041.446027 avg comm time 165598826.375765 
 max send time 2390286.865081 avg send time 1321096.933746 
 max recv time 1242807411.964691 avg recv time 491966904.633888 
 max wait time 244232512.426837 avg wait time 165575909.276246 
LP-IO: writing output to amg-8-trace-11460-1496738413/
LP-IO: data files:
   amg-8-trace-11460-1496738413/dragonfly-router-traffic
   amg-8-trace-11460-1496738413/dragonfly-router-stats
   amg-8-trace-11460-1496738413/dragonfly-msg-stats
   amg-8-trace-11460-1496738413/model-net-category-all
   amg-8-trace-11460-1496738413/model-net-category-test
   amg-8-trace-11460-1496738413/mpi-replay-stats
 Average number of hops traversed 3.033524 average chunk latency 1.896739 us maximum chunk latency 17.016149 us avg message size 610.249207 bytes finished messages 224376 finished chunks 702636 

 ADAPTIVE ROUTING STATS: 574306 chunks routed minimally 128330 chunks routed non-minimally completed packets 702636 

 Total packets generated 449894 finished 449894 
#+end_src

Traces générées :
#+BEGIN_EXAMPLE
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.10.01.19- --workload_type=dumpi --lp-io-dir=amg-8-trace --lp-io-use-suffix=1 -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf 

Tue Jun  6 10:43:57 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 0 unmatched irecvs 11 unmatched sends 1 Total sends 12 receives 16 collectives 4 delays 8221 wait alls 2 waits 0 send time 25265.591539 wait 0.000000
 LP 1 unmatched irecvs 6 unmatched sends 1 Total sends 10 receives 11 collectives 4 delays 5814 wait alls 2 waits 1 send time 24109.662548 wait 0.000000
 LP 2 unmatched irecvs 3 unmatched sends 4 Total sends 3 receives 5 collectives 4 delays 52 wait alls 1 waits 0 send time 2409.319522 wait 0.000000
 LP 3 unmatched irecvs 1 unmatched sends 4 Total sends 3 receives 4 collectives 4 delays 51 wait alls 1 waits 0 send time 2408.773640 wait 0.000000
 LP 9 unmatched irecvs 4 unmatched sends 4 Total sends 3 receives 6 collectives 4 delays 53 wait alls 1 waits 0 send time 2409.227179 wait 0.000000
 LP 10 unmatched irecvs 1 unmatched sends 4 Total sends 3 receives 4 collectives 4 delays 51 wait alls 1 waits 0 send time 2408.965475 wait 0.000000
 LP 11 unmatched irecvs 2 unmatched sends 3 Total sends 3 receives 5 collectives 4 delays 52 wait alls 1 waits 0 send time 2408.101657 wait 0.000000
 LP 12 unmatched irecvs 8 unmatched sends 0 Total sends 10 receives 11 collectives 4 delays 10012 wait alls 2 waits 1 send time 23571.601525 wait 0.000000
	: Running Time = 0.0101 seconds

TW Library Statistics:
	Total Events Processed                                   28635
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                     28635
	Event Rate (events/sec)                              2823686.0
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        479532
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    736

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.0243

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 116088 recvd 40040 
 max runtime 0.000000 ns avg runtime 0.000000 
 max comm time 0.000000 avg comm time -67382407.000000 
 max send time 25265.591539 avg send time 10623.905386 
 max recv time 97865523.416018 avg recv time 37668845.067461 
 max wait time 0.000000 avg wait time 0.000000 
LP-IO: writing output to amg-8-trace-11620-1496738637/
LP-IO: data files:
   amg-8-trace-11620-1496738637/dragonfly-router-traffic
   amg-8-trace-11620-1496738637/dragonfly-router-stats
   amg-8-trace-11620-1496738637/dragonfly-msg-stats
   amg-8-trace-11620-1496738637/model-net-category-all
   amg-8-trace-11620-1496738637/model-net-category-test
   amg-8-trace-11620-1496738637/mpi-replay-stats
 Average number of hops traversed 1.331967 average chunk latency 1.783403 us maximum chunk latency 3.837023 us avg message size 2469.957520 bytes finished messages 47 finished chunks 488 

 ADAPTIVE ROUTING STATS: 488 chunks routed minimally 0 chunks routed non-minimally completed packets 488 

 Total packets generated 263 finished 263 
#+END_EXAMPLE

J'ai du unmatch et ca c'est pas bon...
En regénérant des traces, j'obtient la même chose

Je vais voir en rejouant mes anciennes traces générées :
#+BEGIN_EXAMPLE
model-net-mpi-replay --sync=1 --num_net_traces=16 --workload_file=./anciennesTraces/dumpi-2017.05.29.15.00.43- --workload_type=dumpi --lp-io-dir=amg-8-trace --lp-io-use-suffix=1 -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test-dfly-amg-216.conf 

Tue Jun  6 10:53:22 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

 Total nodes 1056 routers 264 groups 33 radix 16 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                 2376
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                            608257
	Network events                                           50000
	Total events                                            658256

 *** START SEQUENTIAL SIMULATION ***

GVT #0: simulation 1% complete, max event queue size 136 (GVT = 3000418690.7835).
AVL tree size: 0
GVT #0: simulation 2% complete, max event queue size 402 (GVT = 6035527191.2941).
AVL tree size: 0
GVT #0: simulation 3% complete, max event queue size 688 (GVT = 11483113110.0620).
AVL tree size: 0
GVT #0: simulation 4% complete, max event queue size 688 (GVT = 12009052862.3804).
AVL tree size: 0
GVT #0: simulation 5% complete, max event queue size 755 (GVT = 15041415977.3348).
AVL tree size: 0
GVT #0: simulation 6% complete, max event queue size 755 (GVT = 18002513250.3600).
AVL tree size: 0
GVT #0: simulation 7% complete, max event queue size 755 (GVT = 21000392946.9969).
AVL tree size: 0
GVT #0: simulation 8% complete, max event queue size 755 (GVT = 24001429862.3904).
AVL tree size: 0
GVT #0: simulation 9% complete, max event queue size 755 (GVT = 27016181539.3666).
AVL tree size: 0
GVT #0: simulation 10% complete, max event queue size 755 (GVT = 30000421301.8214).
AVL tree size: 0
GVT #0: simulation 11% complete, max event queue size 755 (GVT = 33000388626.2790).
AVL tree size: 0
GVT #0: simulation 12% complete, max event queue size 755 (GVT = 36000490201.6426).
AVL tree size: 0
 *** END SIMULATION ***


 LP 0 unmatched irecvs 0 unmatched sends 0 Total sends 5195 receives 5196 collectives 165 delays 13765 wait alls 818 waits 0 send time 31049590.884462 wait 6896811063.124624
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 5320 receives 5320 collectives 165 delays 14014 wait alls 818 waits 0 send time 27406535.422290 wait 4902476557.148384
 LP 2 unmatched irecvs 0 unmatched sends 0 Total sends 7528 receives 7578 collectives 165 delays 18495 wait alls 833 waits 0 send time 48394690.382042 wait 7307331091.361316
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 7513 receives 7497 collectives 165 delays 18384 wait alls 818 waits 0 send time 52232192.354479 wait 4480098136.370174
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 7725 receives 7733 collectives 165 delays 18832 wait alls 818 waits 0 send time 55760528.873564 wait 4495196328.271902
 LP 10 unmatched irecvs 0 unmatched sends 0 Total sends 7224 receives 7206 collectives 165 delays 17804 wait alls 818 waits 0 send time 55449250.169176 wait 6989604606.631558
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 5134 receives 5124 collectives 165 delays 13593 wait alls 786 waits 0 send time 25451387.954232 wait 8660401684.884769
 LP 12 unmatched irecvs 0 unmatched sends 0 Total sends 5011 receives 4998 collectives 165 delays 13383 wait alls 818 waits 0 send time 23487805.313354 wait 7571367496.128121
 LP 18 unmatched irecvs 0 unmatched sends 0 Total sends 4971 receives 4971 collectives 165 delays 13277 wait alls 786 waits 0 send time 27980908.753104 wait 5488817203.298100
 LP 19 unmatched irecvs 0 unmatched sends 0 Total sends 5156 receives 5146 collectives 165 delays 13676 wait alls 818 waits 0 send time 24128321.802325 wait 7623737540.569324
 LP 20 unmatched irecvs 0 unmatched sends 0 Total sends 6995 receives 6951 collectives 165 delays 17281 wait alls 786 waits 0 send time 46167576.805130 wait 4851373601.992508
 LP 21 unmatched irecvs 0 unmatched sends 0 Total sends 7635 receives 7685 collectives 165 delays 18709 wait alls 833 waits 0 send time 44505846.432115 wait 4975400608.077621
 LP 27 unmatched irecvs 0 unmatched sends 0 Total sends 7405 receives 7377 collectives 165 delays 18156 wait alls 818 waits 0 send time 47102605.814613 wait 4562468563.792100
 LP 28 unmatched irecvs 0 unmatched sends 0 Total sends 7655 receives 7700 collectives 165 delays 18744 wait alls 833 waits 0 send time 43722919.167827 wait 4438655118.330465
 LP 29 unmatched irecvs 0 unmatched sends 0 Total sends 5338 receives 5335 collectives 165 delays 14047 wait alls 818 waits 0 send time 28659629.223436 wait 7415602451.839598
 LP 30 unmatched irecvs 0 unmatched sends 0 Total sends 4415 receives 4403 collectives 165 delays 12090 wait alls 723 waits 0 send time 24836982.610907 wait 8429789123.096380
	: Running Time = 11.2695 seconds

TW Library Statistics:
	Total Events Processed                                20237447
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             6
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                  20237447
	Event Rate (events/sec)                              1795778.2
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                        658257
	Memory Allocated                                        479532
	Memory Wasted                                               28

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                             64
	LP RNGs                                                     80
	Total LP                                                   272
	Event struct                                               144
	Event struct with Model                                    736

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)     26.9842

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 564713492 recvd 564713492 
 max runtime 38171957179.288765 ns avg runtime 38076505282.498756 
 max comm time 8660526284.320320 avg comm time 6193220647.686256 
 max send time 55760528.873564 avg send time 37896048.247691 
 max recv time 36342589195.592293 avg recv time 21138164904.627800 
 max wait time 8660401684.884769 avg wait time 6193070698.432308 
LP-IO: writing output to amg-8-trace-12096-1496739202/
LP-IO: data files:
   amg-8-trace-12096-1496739202/dragonfly-router-traffic
   amg-8-trace-12096-1496739202/dragonfly-router-stats
   amg-8-trace-12096-1496739202/dragonfly-msg-stats
   amg-8-trace-12096-1496739202/model-net-category-all
   amg-8-trace-12096-1496739202/model-net-category-test
   amg-8-trace-12096-1496739202/mpi-replay-stats
 Average number of hops traversed 1.447529 average chunk latency 8.990396 us maximum chunk latency 129.941465 us avg message size 5634.738281 bytes finished messages 100220 finished chunks 2265893 

 ADAPTIVE ROUTING STATS: 2265893 chunks routed minimally 0 chunks routed non-minimally completed packets 2265893 

 Total packets generated 1168810 finished 1168810 
#+END_EXAMPLE

Ca se passe bien. J'ai donc un soucis dans mes traces DUMPI générés...

******* Réinstallation de DUMPI
Peut être que dumpi s'est mal installé à cause de openmpi. 
En l'ayant désinstallé j'ai toujours le problème. Surement le fait de
l'utilisation d'une autre librairie OTF.

Via une machine virtuelle j'ai installé dumpi et AMG. Je crois que le
problème vient de ma configuration d'AMG. Je vais donc réinstaller AMG
dans un premier temps en mettant à jours directement le wiki.


**** Webinar
Participation au Webinar épisode 9 sur les Testbeds. Il faudra que je
relise le récap quand il sera dispo car c'est pile dans le sujet du
stage (avec présentation de Grid'5000)
*** 2017-06-07 mercredi
**** AMG replay
Maintenant que j'ai pu rejouer une trace AMG sur mon pc, je vais
pouvoir la rejouer un certain nombre de fois pour voir si le temps
reste constant.
***** Parallele
****** Resultat 1
#+begin_src sh :results output :exports both
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 08:40:54 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #7911: simulation 100% complete, max event queue size 603 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***

 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801

 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
	: Running Time = 3.4431 seconds

TW Library Statistics:
	Total Events Processed                                 2258477
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     1602174
	Event Ties Detected in PE Queues                             0
	Efficiency                                             -144.12 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                  41840
	Percent Remote Events                                     6.38 %

	Total Roll Backs                                        167444
	Primary Roll Backs                                       26815
	Secondary Roll Backs                                    140629
	Fossil Collect Attempts                                  31648
	Total GVT Computations                                    7912

	Net Events Processed                                    656303
	Event Rate (events/sec)                               190615.5
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                            292188
	Remote recvs                                            292188

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.1970
	AVL Tree (insert/delete)                                0.1210
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        3.9324
	Event Cancel                                            1.7647
	Event Abort                                             0.0000

	GVT                                                     8.2201
	Fossil Collect                                          0.1008
	Primary Rollbacks                                       0.3989
	Network Read                                            0.8247
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      8.2442

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                    7912
	Total All Reduce Calls                                   23090
	Average Reduction / GVT                                   2.92

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 2
#+begin_src sh :results output :exports both
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 08:42:32 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #8000: simulation 100% complete, max event queue size 603 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***


 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
	: Running Time = 3.3772 seconds

TW Library Statistics:
	Total Events Processed                                 2276815
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     1620512
	Event Ties Detected in PE Queues                             0
	Efficiency                                             -146.92 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                  41840
	Percent Remote Events                                     6.38 %

	Total Roll Backs                                        169474
	Primary Roll Backs                                       27253
	Secondary Roll Backs                                    142221
	Fossil Collect Attempts                                  32004
	Total GVT Computations                                    8001

	Net Events Processed                                    656303
	Event Rate (events/sec)                               194331.6
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                            294372
	Remote recvs                                            294372

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.2015
	AVL Tree (insert/delete)                                0.1208
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        3.8104
	Event Cancel                                            1.7647
	Event Abort                                             0.0000

	GVT                                                     8.0637
	Fossil Collect                                          0.1009
	Primary Rollbacks                                       0.4035
	Network Read                                            0.8015
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      8.0866

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                    8001
	Total All Reduce Calls                                   23345
	Average Reduction / GVT                                   2.92

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 3
#+begin_src sh :results output :exports both
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 08:45:20 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #7946: simulation 100% complete, max event queue size 603 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***


 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
	: Running Time = 3.3574 seconds

TW Library Statistics:
	Total Events Processed                                 2259837
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     1603534
	Event Ties Detected in PE Queues                             0
	Efficiency                                             -144.33 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                  41840
	Percent Remote Events                                     6.38 %

	Total Roll Backs                                        167331
	Primary Roll Backs                                       26713
	Secondary Roll Backs                                    140618
	Fossil Collect Attempts                                  31788
	Total GVT Computations                                    7947

	Net Events Processed                                    656303
	Event Rate (events/sec)                               195476.9
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                            291912
	Remote recvs                                            291912

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.1972
	AVL Tree (insert/delete)                                0.1197
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        3.8881
	Event Cancel                                            1.7873
	Event Abort                                             0.0000

	GVT                                                     8.0155
	Fossil Collect                                          0.1018
	Primary Rollbacks                                       0.3973
	Network Read                                            0.8025
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      8.0392

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                    7947
	Total All Reduce Calls                                   23192
	Average Reduction / GVT                                   2.92

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 4
#+begin_src sh :results output :exports both
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 08:45:45 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #7861: simulation 100% complete, max event queue size 603 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***


 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
	: Running Time = 3.3531 seconds

TW Library Statistics:
	Total Events Processed                                 2243949
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     1587646
	Event Ties Detected in PE Queues                             0
	Efficiency                                             -141.91 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                  41840
	Percent Remote Events                                     6.38 %

	Total Roll Backs                                        166403
	Primary Roll Backs                                       26871
	Secondary Roll Backs                                    139532
	Fossil Collect Attempts                                  31448
	Total GVT Computations                                    7862

	Net Events Processed                                    656303
	Event Rate (events/sec)                               195728.1
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                            290228
	Remote recvs                                            290228

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.1951
	AVL Tree (insert/delete)                                0.1178
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        3.7959
	Event Cancel                                            1.7294
	Event Abort                                             0.0000

	GVT                                                     8.0053
	Fossil Collect                                          0.1018
	Primary Rollbacks                                       0.4001
	Network Read                                            0.7963
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      8.0289

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                    7862
	Total All Reduce Calls                                   22955
	Average Reduction / GVT                                   2.92

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 

#+END_EXAMPLE
****** Resultat 5
#+begin_src sh :results output :exports both
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 08:46:46 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #8216: simulation 100% complete, max event queue size 603 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
	: Running Time = 3.6221 seconds

TW Library Statistics:
	Total Events Processed                                 2354922
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     1698619
	Event Ties Detected in PE Queues                             0
	Efficiency                                             -158.82 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                  41840
	Percent Remote Events                                     6.38 %

	Total Roll Backs                                        172397
	Primary Roll Backs                                       27471
	Secondary Roll Backs                                    144926
	Fossil Collect Attempts                                  32868
	Total GVT Computations                                    8217

	Net Events Processed                                    656303
	Event Rate (events/sec)                               181193.7
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                            329220
	Remote recvs                                            329220

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.2067
	AVL Tree (insert/delete)                                0.1380
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        4.0489
	Event Cancel                                            1.9717
	Event Abort                                             0.0000

	GVT                                                     8.6499
	Fossil Collect                                          0.1016
	Primary Rollbacks                                       0.4383
	Network Read                                            0.8995
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      8.6729

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                    8217
	Total All Reduce Calls                                   24029
	Average Reduction / GVT                                   2.92

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 

#+END_EXAMPLE
****** Resultat 6
#+begin_src sh :results output :exports both
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 08:47:13 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #7956: simulation 100% complete, max event queue size 603 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***

 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801

 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
	: Running Time = 3.3595 seconds

TW Library Statistics:
	Total Events Processed                                 2270330
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     1614027
	Event Ties Detected in PE Queues                             0
	Efficiency                                             -145.93 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                  41840
	Percent Remote Events                                     6.38 %

	Total Roll Backs                                        169192
	Primary Roll Backs                                       27229
	Secondary Roll Backs                                    141963
	Fossil Collect Attempts                                  31828
	Total GVT Computations                                    7957

	Net Events Processed                                    656303
	Event Rate (events/sec)                               195357.5
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                            293432
	Remote recvs                                            293432

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.1991
	AVL Tree (insert/delete)                                0.1254
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        3.8695
	Event Cancel                                            1.7786
	Event Abort                                             0.0000

	GVT                                                     8.0209
	Fossil Collect                                          0.1031
	Primary Rollbacks                                       0.4074
	Network Read                                            0.7982
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      8.0441

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                    7957
	Total All Reduce Calls                                   23229
	Average Reduction / GVT                                   2.92

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 7
#+begin_src sh :results output :exports both
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 08:47:32 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #7937: simulation 100% complete, max event queue size 603 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***

 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801

 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
	: Running Time = 3.6399 seconds

TW Library Statistics:
	Total Events Processed                                 2262965
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     1606662
	Event Ties Detected in PE Queues                             0
	Efficiency                                             -144.80 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                  41840
	Percent Remote Events                                     6.38 %

	Total Roll Backs                                        167920
	Primary Roll Backs                                       26884
	Secondary Roll Backs                                    141036
	Fossil Collect Attempts                                  31752
	Total GVT Computations                                    7938

	Net Events Processed                                    656303
	Event Rate (events/sec)                               180308.4
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                            291976
	Remote recvs                                            291976

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.2063
	AVL Tree (insert/delete)                                0.1197
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        3.8670
	Event Cancel                                            1.8060
	Event Abort                                             0.0000

	GVT                                                     8.6905
	Fossil Collect                                          0.1055
	Primary Rollbacks                                       0.4131
	Network Read                                            0.7923
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      8.7155

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                    7938
	Total All Reduce Calls                                   23151
	Average Reduction / GVT                                   2.92

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 

#+END_EXAMPLE
****** Resultat 8
#+begin_src sh :results output :exports both
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 08:47:53 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #8004: simulation 100% complete, max event queue size 603 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***


 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
	: Running Time = 3.4843 seconds

TW Library Statistics:
	Total Events Processed                                 2279212
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     1622909
	Event Ties Detected in PE Queues                             0
	Efficiency                                             -147.28 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                  41840
	Percent Remote Events                                     6.38 %

	Total Roll Backs                                        169365
	Primary Roll Backs                                       27358
	Secondary Roll Backs                                    142007
	Fossil Collect Attempts                                  32020
	Total GVT Computations                                    8005

	Net Events Processed                                    656303
	Event Rate (events/sec)                               188360.3
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                            293524
	Remote recvs                                            293524

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.2076
	AVL Tree (insert/delete)                                0.1193
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        3.9233
	Event Cancel                                            1.8276
	Event Abort                                             0.0000

	GVT                                                     8.3182
	Fossil Collect                                          0.1031
	Primary Rollbacks                                       0.4122
	Network Read                                            0.8021
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      8.3429

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                    8005
	Total All Reduce Calls                                   23366
	Average Reduction / GVT                                   2.92

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 

#+END_EXAMPLE
****** Resultat 9
#+begin_src sh :results output :exports both
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 08:48:16 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #8131: simulation 100% complete, max event queue size 603 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***

 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801

 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
	: Running Time = 3.5705 seconds

TW Library Statistics:
	Total Events Processed                                 2316113
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     1659810
	Event Ties Detected in PE Queues                             0
	Efficiency                                             -152.90 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                  41840
	Percent Remote Events                                     6.38 %

	Total Roll Backs                                        171347
	Primary Roll Backs                                       27411
	Secondary Roll Backs                                    143936
	Fossil Collect Attempts                                  32528
	Total GVT Computations                                    8132

	Net Events Processed                                    656303
	Event Rate (events/sec)                               183810.3
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                            306948
	Remote recvs                                            306948

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.2101
	AVL Tree (insert/delete)                                0.1305
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        4.0191
	Event Cancel                                            1.8989
	Event Abort                                             0.0000

	GVT                                                     8.5252
	Fossil Collect                                          0.1127
	Primary Rollbacks                                       0.4155
	Network Read                                            0.8252
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      8.5495

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                    8132
	Total All Reduce Calls                                   23766
	Average Reduction / GVT                                   2.92

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 

#+END_EXAMPLE
****** Resultat 10
#+begin_src sh :results output :exports both
mpirun -n 4 model-net-mpi-replay --sync=3 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 08:48:32 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 4 

ROSS Core Configuration: 
	Total Nodes                                                  4
	Total Processors                                   [Nodes (4) x PE_per_Node (1)] 4
	Total KPs                                          [Nodes (4) x KPs (16)] 64
	Total LPs                                                   56
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                              3585
	Network events                                           50000
	Total events                                             53584

 *** START PARALLEL OPTIMISTIC SIMULATION WITH SUSPEND LP FEATURE ***

GVT #7757: simulation 100% complete, max event queue size 603 (GVT = MAX).
AVL tree size: 0
 *** END SIMULATION ***


 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
	: Running Time = 4.0470 seconds

TW Library Statistics:
	Total Events Processed                                 2211432
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                     1555129
	Event Ties Detected in PE Queues                             0
	Efficiency                                             -136.95 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                  41840
	Percent Remote Events                                     6.38 %

	Total Roll Backs                                        164780
	Primary Roll Backs                                       25999
	Secondary Roll Backs                                    138781
	Fossil Collect Attempts                                  31032
	Total GVT Computations                                    7758

	Net Events Processed                                    656303
	Event Rate (events/sec)                               162170.2
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         53585
	Memory Allocated                                         71502
	Memory Wasted                                              906

TW Network Statistics:
	Remote sends                                            281468
	Remote recvs                                            281468

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.1987
	AVL Tree (insert/delete)                                0.1220
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        5.0381
	Event Cancel                                            1.7588
	Event Abort                                             0.0000

	GVT                                                     9.6701
	Fossil Collect                                          0.1144
	Primary Rollbacks                                       0.3929
	Network Read                                            0.7637
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      9.6903

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                    7758
	Total All Reduce Calls                                   22589
	Average Reduction / GVT                                   2.91

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
***** Sequentiel 
****** Resultat 1
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 13:39:09 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
	: Running Time = 0.2727 seconds

TW Library Statistics:
	Total Events Processed                                  656303
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    656303
	Event Rate (events/sec)                              2406420.3
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.6530

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 2
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 13:42:50 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
	: Running Time = 0.2694 seconds

TW Library Statistics:
	Total Events Processed                                  656303
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    656303
	Event Rate (events/sec)                              2435776.8
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.6452

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 3
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 13:43:08 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
	: Running Time = 0.2676 seconds

TW Library Statistics:
	Total Events Processed                                  656303
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    656303
	Event Rate (events/sec)                              2452919.0
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.6407

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 4
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 13:43:35 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
	: Running Time = 0.2725 seconds

TW Library Statistics:
	Total Events Processed                                  656303
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    656303
	Event Rate (events/sec)                              2408566.3
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.6525

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 5
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 13:43:48 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
	: Running Time = 0.2688 seconds

TW Library Statistics:
	Total Events Processed                                  656303
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    656303
	Event Rate (events/sec)                              2441785.1
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.6436

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 6
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 13:44:02 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
	: Running Time = 0.2694 seconds

TW Library Statistics:
	Total Events Processed                                  656303
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    656303
	Event Rate (events/sec)                              2436048.0
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.6451

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 7
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 13:44:11 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
	: Running Time = 0.2743 seconds

TW Library Statistics:
	Total Events Processed                                  656303
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    656303
	Event Rate (events/sec)                              2392917.2
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.6567

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 8
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 13:44:27 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
	: Running Time = 0.2688 seconds

TW Library Statistics:
	Total Events Processed                                  656303
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    656303
	Event Rate (events/sec)                              2441539.8
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.6436

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 9
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 13:44:37 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
	: Running Time = 0.2688 seconds

TW Library Statistics:
	Total Events Processed                                  656303
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    656303
	Event Rate (events/sec)                              2441558.0
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.6436

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
****** Resultat 10
#+begin_src sh :results output :exports both
model-net-mpi-replay --sync=1 --num_net_traces=8 --workload_file=dumpi-2017.06.06.15.44.54- --workload_type=dumpi  -- ../../CODES/src/network-workloads/conf/modelnet-mpi-test.conf
#+end_src
#+RESULTS:
#+BEGIN_EXAMPLE
Wed Jun  7 13:44:47 2017

ROSS Revision: 4c6a7d8eb9c784797d900edfc76725d62ec25941

tw_net_start: Found world size to be 1 

ROSS Core Configuration: 
	Total Nodes                                                  1
	Total Processors                                   [Nodes (1) x PE_per_Node (1)] 1
	Total KPs                                          [Nodes (1) x KPs (16)] 16
	Total LPs                                                   54
	Simulation End Time                                300000000000.00
	LP-to-PE Mapping                                   model defined


ROSS Event Memory Allocation:
	Model events                                             13825
	Network events                                           50000
	Total events                                             63824

 *** START SEQUENTIAL SIMULATION ***

 *** END SIMULATION ***


 LP 1 unmatched irecvs 0 unmatched sends 0 Total sends 3316 receives 3338 collectives 140 delays 9083 wait alls 584 waits 0 send time 564673.438014 wait 864289399.804539
 LP 3 unmatched irecvs 0 unmatched sends 0 Total sends 3259 receives 3265 collectives 140 delays 8953 wait alls 584 waits 0 send time 516319.092984 wait 559585821.219168
 LP 5 unmatched irecvs 0 unmatched sends 0 Total sends 3189 receives 3177 collectives 140 delays 8795 wait alls 584 waits 0 send time 504658.277001 wait 570800048.399966
 LP 7 unmatched irecvs 0 unmatched sends 0 Total sends 3409 receives 3410 collectives 140 delays 9248 wait alls 584 waits 0 send time 520109.736998 wait 932883767.906023
 LP 9 unmatched irecvs 0 unmatched sends 0 Total sends 3196 receives 3176 collectives 140 delays 8784 wait alls 567 waits 0 send time 504530.145300 wait 897957292.014923
 LP 11 unmatched irecvs 0 unmatched sends 0 Total sends 3255 receives 3276 collectives 140 delays 8960 wait alls 584 waits 0 send time 509138.727596 wait 824776328.759475
 LP 13 unmatched irecvs 0 unmatched sends 0 Total sends 3262 receives 3238 collectives 140 delays 8929 wait alls 584 waits 0 send time 510002.463940 wait 915030709.793312
 LP 15 unmatched irecvs 0 unmatched sends 0 Total sends 3276 receives 3282 collectives 140 delays 8987 wait alls 584 waits 0 send time 487936.698410 wait 837590373.616801
	: Running Time = 0.2709 seconds

TW Library Statistics:
	Total Events Processed                                  656303
	Events Aborted (part of RBs)                                 0
	Events Rolled Back                                           0
	Event Ties Detected in PE Queues                             0
	Efficiency                                              100.00 %
	Total Remote (shared mem) Events Processed                   0
	Percent Remote Events                                     0.00 %
	Total Remote (network) Events Processed                      0
	Percent Remote Events                                     0.00 %

	Total Roll Backs                                             0
	Primary Roll Backs                                           0
	Secondary Roll Backs                                         0
	Fossil Collect Attempts                                      0
	Total GVT Computations                                       0

	Net Events Processed                                    656303
	Event Rate (events/sec)                              2422640.5
	Total Events Scheduled Past End Time                         0

TW Memory Statistics:
	Events Allocated                                         63825
	Memory Allocated                                         62573
	Memory Wasted                                              683

TW Data Structure sizes in bytes (sizeof):
	PE struct                                                  608
	KP struct                                                  144
	LP struct                                                  128
	LP Model struct                                            760
	LP RNGs                                                     80
	Total LP                                                   968
	Event struct                                               144
	Event struct with Model                                    928

TW Clock Cycle Statistics (MAX values in secs at 1.0000 GHz):
	Priority Queue (enq/deq)                                0.0000
	AVL Tree (insert/delete)                                0.0000
	LZ4 (de)compression                                     0.0000
	Buddy system                                            0.0000
	Event Processing                                        0.0000
	Event Cancel                                            0.0000
	Event Abort                                             0.0000

	GVT                                                     0.0000
	Fossil Collect                                          0.0000
	Primary Rollbacks                                       0.0000
	Network Read                                            0.0000
	Statistics Computation                                  0.0000
	Statistics Write                                        0.0000
	Total Time (Note: Using Running Time above for Speedup)      0.6487

TW GVT Statistics: MPI AllReduce
	GVT Interval                                                16
	GVT Real Time Interval (cycles)                    0
	GVT Real Time Interval (sec)                        0.00000000
	Batch Size                                                  16

	Forced GVT                                                   0
	Total GVT Computations                                       0
	Total All Reduce Calls                                       0
	Average Reduction / GVT                                   -nan

 Total bytes sent 63010396 recvd 63010396 
 max runtime 1873261977.339076 ns avg runtime 1862934388.822206 
 max comm time 932969051.538982 avg comm time 800446680.697205 
 max send time 564673.438014 avg send time 514671.072530 
 max recv time 2523106611.125298 avg recv time 1673622329.972924 
 max wait time 932883767.906023 avg wait time 800364217.689276 
#+END_EXAMPLE
***** Commentaires
****** Running Time seq                                              :R:
#+begin_src R :file plotSeqRunningTime.png :results output graphics :session *R*
library(ggplot2)
sequentiel = c(0.2727,0.2694,0.2676,0.2725,0.2688,0.2694,0.2743,0.2688,0.2688,0.2709)
parallel = c(3.4431,3.3772,3.3574,3.3531,3.6221,3.3595,3.6399,3.4843,3.5702,4.0470)
rollBack = c(167444,169474,167331,166403,172397,169192,167920,169365,171347,164780)
print(qplot(seq(1,10),sequentiel) + scale_x_continuous(breaks=seq(0,10,1)) + ylab("Running Time") + xlab("Numéro simulation"))
#+end_src

#+RESULTS:
[[file:plotSeqRunningTime.png]]
****** Running Time par                                              :R:
#+begin_src R :file plotParRunningTime.png :results output graphics :session *R* :exports both
print(qplot(seq(1,10),parallel) + scale_x_continuous(breaks=seq(0,10,1)) + ylab("Running Time") + xlab("Numéro simulation"))
#+end_src

#+RESULTS:
[[file:plotParRunningTime.png]]

#+begin_src R :file plotParRollBack.png :results output graphics :session *R* :exports both
qplot(seq(1,10),rollBack) + scale_x_continuous(breaks=seq(0,10,1)) + ylab("Total rollback") + xlab("Numéro simulation")
#+end_src

#+RESULTS:
[[file:plotParRollBack.png]]

On remarque pas de corrélation entre le nombre de rollback et le
running time. Ca vient donc d'autre part et il faut que je le face
avec des traces plus importantes (ie. Grid5000).
**** Réunion
Sait dumper trace
Sait convertir
Sait rejouer sur CODEs (mais avec info bizarres)
Toujours d'info sur replay plus vite en simulation // que seq
AMG : Matrice calcul en fonction des voisin (for(i)for()... send recv
Scalatrace : compresse les traces en comprenant le schemas d'echange
entre les processus. Ca permet également d'annonymiser les traces.
HPL lui a une zone de calcul/comm qui change en fonction du
temps. Scalatrace fait donc des analyses avec les regressions
linéaire. Il y a un outils qui compare directement des traces faites
sur des nombre != de proco, et fait des statistiques dessus. Une foi
les variables identifié, il est possible de faire à partir de
plusieurs petites traces, de l'extrapolation.
HPL : Le code et la gestion MPI sont entrelacé et le nombre de prob
MPI change en fonction de chaque execution. Celui à qui on envoit
dépend de qui on a recu, donc l'exection n'est jamais la même. C'est
complexe à rejouer. Scalatrace gère une forme de non determinisme mais
pas la variabilité du à MPI.

DUMPI-CORTEX : gere les appels mpi collectifs. MPI à différents algo
en collectif en fonction de la taille et l'algo de diffusion. Cet
outil transforme les BCAST en send/rcv point à point.

CODES c'est voir l'interaction entre les topologies, les differentes
applications MPI qui tournent (voir si ils se gènent) et le scheduler
qui positionne ces applications sur le réseau.

Bonne simulation : modèle de machine résonnable, modèle workload
résonable, la gestion des collectives, capture de l'application (si on
lit avec une bibli de trace y'a un biai de capture), calibration de la
platforme (latence, bande passante, ...) -> Instanciation du modèl. Il
faut également prendre en compte les modèles physiques et logiques qui
sont différents et impactent donc les résultats. Les compilateurs,
techno, dérive des horloges sont à prendre en compte.
ex: Sacha Hunald, =MPI_BROADCAST= à un temps d'exection très variable
pour les mêmes expériences/conditions. Il faut donc répéter ne
nombreuses fois l'expérience pour s'approcher d'un résultat plausible.

Il faut comprendre les paramètres dans les réglages de CODES, car
c'est peut être seulement ce qu'on donne à CODES qui fait que ca ne
marche pas.
**** Obj
- [X] Finir expériences
- [X] VITE
- [ ] GRID'5000
- [ ] Définir si on garde AMG (peut être s'intéréssé au NAS de la NASA?)
- [ ] Faire une bibliographie
**** VITE

#+BEGIN_EXAMPLE
sudo apt-get install vite -y
sudo update-alternatives --set mpi /usr/include/mpich
#+END_EXAMPLE

Une foi qu'on transforme en otf, on peut ouvrir avec VITE. Avec un
zoom 4848331% on obtient ce genre de siualisation :
[[file:testViteExport.png]]
**** Grid 5000

Ressources : 
- https://www.grid5000.fr/mediawiki/index.php/Getting_Started
- https://www.grid5000.fr/mediawiki/index.php/SSH_and_Grid%275000
- https://www.grid5000.fr/mediawiki/index.php/HPC_and_HTC_tutorial
- https://www.grid5000.fr/mediawiki/index.php/Run_MPI_On_Grid%275000
- https://www.grid5000.fr/mediawiki/index.php/Advanced_OAR
- https://www.grid5000.fr/mediawiki/index.php/Advanced_Kadeploy
- https://www.grid5000.fr/mediawiki/index.php/Grid5000:UsagePolicy
*** 2017-06-08 jeudi
**** Mailling list CODES-ROSS
***** Content
Hi Maxime,

I ran the HPL traces with no MPI data type on the simulation and here are some observations. I disabled any synchronizations (wait, wait-alls) in the simulation so that it only matches the MPI sends with the receives and does nothing else. 

- Rank 0 expects 192 messages from Rank 1 but it instead receives 192 messages from Rank 2.
- Rank 1 receives 192 messages from rank 0 but there are no corresponding receives posted so the messages remain unmatched. -
- Rank 2 is expecting 192 messages from Rank 0 but they don’t arrive (probably because they arrived at Rank 1).

Is it possible that having no MPI data type resulted in missing messages that introduced these discrepancies? Or maybe the application is terminating earlier than usual? 

I will try the version with MPI data types and let you know if the results are different.   

Thanks,
Misbah

***** Commentaires
On sait mainteneant pourquoi CODES n'est pas content. Quelque chose
d'étrange se passe au niveau d'HPL. Est-ce que ca vient d'HPL, de
DUMPI, de MPICH3 ou encore de ma machine ?

***** Réponse
#+BEGIN_EXAMPLE
Hi Misbah!
Thanks again for your time. What you have observed is weird. Can it come from my DUMPI installation? I used MPICH3 and in the doc you warn about it. As I generate DUMPI traces too, I had compiled DUMPI as follows : 

../configure --enable-libdumpi --enable-test --prefix=/home/chevamax/logiciels CC=mpicc CXX=mpiCC CFLAGS="-DMPICH_SUPPRESS_PROTOTYPES=1 -DHAVE_PRAGMA_HP_SEC_DEF=1 -pthread -I/home/chevamax/logiciels/include/open-trace-format -L/home/chevamax/logiciels/lib"

I try to use dumpi2otf (I know it's not finished) to visualize dumpi trace (as I found nothing graphical), so that's why I link libotf. Perhaps the CFLAGS's options break something. But I doubt that's come from here as I generate AMG traces and replay them without errors.

When I execute HPL, it run smoothly with a correct terminaison (end tests pass and no warning/error messages), so I don't think it comes from an early terminaison of HPL.

Maxime
#+END_EXAMPLE
**** Mail Arnaud
***** Content

Salut,

  désolé pour cette réunion écourtée. Bon, au final, c'est cool Maxime,
tu avances bien. Tu as pris CODES en main, tu sais générer et rejouer
des traces DUMPI. Je pense qu'il doit y avoir moyen que CODES génère des
traces de l'application à son tour (qu'on peut alors visualiser pour
vérifier que ça correspond bien à notre intuition et que le simulateur
ne fait pas n'importe quoi) et qu'on récupère autre chose en sortie que
des stats internes sur ROSS. Si tu peux regarder ça, ça serait bien.

Pour avancer dans notre compréhension de CODES et de ses performance, il
faudrait des traces plus grandes. Quand on n'a pas de grosse machine
sous la mains, ça n'est pas forcément évident... C'est pour ça que je
mentionnais scalaExtrap, sous projet de scalatrace. Tu peux essayer mais
j'ai peur que ça soit moins propre que CODES.

Une autre approche serait d'utiliser SimGrid mais ça veut dire arriver à
générer des traces DUMPI. C'est ce que disait Lucas après tout, on ne
sait jamais, sur un coup de chance, ça pourrait marcher puisque SMPI
fourni l'interface PMPI. ;)

Je propose que tu utilises les deux jours qui restent de cette semaine
pour explorer ces deux pistes (commence par SMPI, c'est moins risqué imho).

Sinon, petite info complémentaire, Tom a fini ce qu'il avait à faire sur
la préparation d'HPL pour SimGrid et ça marche plutôt bien puisqu'il a
réussi à simuler l'ordre de grandeur qu'on souhaitait atteindre. La
simulation est séquentielle mais il arrive à émuler HPL avec une matrice
de rang 4.10^6 avec 4096 processus MPI. Ça prend 47 heures et nécessite
16 Go de RAM. Je pense qu'il n'y a plus grand chose à gagner (à moins
d'optimisant SimGrid lui même... il n'est pas exclu depuis la dernière
fois qu'on a tout optimisé comme des fous que des régressions de
performances aient été introduites car c'est assez difficile à
tester/évaluer...). Sincèrement 2 jours pour cette échelle, c'est déjà
acceptable pour plein de gens.

Pour que ça prenne moins de deux jours, il faut sortir le time
parallèle. Je sais que c'était l'objectif du stage mais il ne faut pas
trop t'inquiéter là dessus. Évaluer l'existant avant de se lancer était
essentiel, ne serait-ce que pour avoir la certitude que ça ne marche
pas. Donc, je propose qu'on insiste encore sur CODES maintenant que tu
as bien investi là dessus pendant au moins une semaine et en parallèle,
on va pouvoir commencer à réfléchir à comment on pourrait bien lancer
HPL/SimGrid en time parallèle.

A+
    Arnaud
***** Commentaires
Ok donc si je résume, mes objectifs pour la semaine sont :
- Regarder si CODES peut générer des traces de son execution interne.
- Faire de plus grandes traces avec CODES soit via G5k (comme on l'a
  évoqué plusieurs fois) soit via ScalaExtrap.
- Faire génrerer des traces DUMPI à SimGrid ? Je pense pas comprendre
  ce point là.
**** DUMPI
Article intéréssant :
https://pdfs.semanticscholar.org/95d2/b96a0b525fdade8aaa0e17bd213377b55530.pdf
Notamment le passage suivant :
#+BEGIN_EXAMPLE
To evaluate this approach we have prototyped the idea using existing performance
tools. We used the DUMPI MPI tracing library that was developed by Sandia as part
of their SST/macro project [8]. The traces are recorded in a binary format with utilities
to convert the results to plain text. The library records both the input arguments and
return values1
for the MPI functions. The library can also support tracing individual
functions, and can be configured to record performance counter data using PAPI [8].
There are utilities for converting DUMPI trace files to Open Trace Format (OTF) files,
which could be used with existing performance visualization tools like Vampir.
#+END_EXAMPLE

Qui a l'air de dire que dumpi2otf fonctionne bien.
***** User manual
On peut configurer plus finement dumpi via un fichier .conf dans le
répertoir courant.
#+BEGIN_EXAMPLE
\section config Runtime configuration of the DUMPI trace library
The DUMPI trace collect library, libdumpi, will look for a file called <tt>dumpi.conf</tt> in the current working directory to pick up configuration information. These can be used to initially configure what dumpi collects, how it names its files, etc. Below are a list of options followed by the default value.
<ul>
<li> <tt>fileroot dumpi-</tt> This allows you to specify the prefix, including the filesystem directory where to write the generated DUMPI trace files.
<li> <tt>timestamp full</tt> Can specify <tt>none, cpu, wall, </tt> or <tt>full</tt>.
<li> <tt>MPI_Default enable</tt> Can specify <tt>disable, success</tt> or <tt>enable</tt>. These allow you to specify how much profiling you want for MPI calls by default. This can be disabled or enabled for profiling (call count statistics will still be collected).  Additionally, probing calls (Iprobe, Test*, ...) can be conditionally profiled iff they succeed (for non-probing calls, success is equivalent to enable).
<li> Individual MPI calls (e.g. <tt>MPI_Init, MPI_Iprobe, MPI_Testany,</tt>) can be specified using the same arguments above.
<li><tt>statuses success</tt> Also takes the same arguments as above. Can be used to disable the collection of XXX to reduce the size of the trace files. More information can be found in the \ref traceformat documentation.
<li><tt>PAPI </tt>\<counter\> (e.g. <tt>PAPI PAPI PAPI_TOT_CYC</tt>) If DUMPI is so configured, you can collect PAPI information on each call to the DUMPI library (e.g. on entry and exit to <tt>MPI_Send</tt>) This greatly increases the file size, and the names of the counters supported is system dependent. Also, the number of counters that can be collected, etc. is also system dependent.
</ul>

A typical file that we use is:

# Concept configuration file for dumpi (refactored).

# file root defaults to "dumpi-"
fileroot     dumpi-

# You can define what sort of timestamp information you want output.
# timestamp (none|cpu|wall|full)  # defaults to full
timestamp    full

# Specify how much profiling you want for individual MPI calls.
# All calls can be disabled or enabled for profiling (call count statistics
# will still be collected).  Additionally, probing calls (Iprobe, Test*, ...)
# can be conditionally profiled iff they succeed (for non-probing calls,
# success is equivalent to enable).
#
# MPI_Default (disable|success|enable)  # all MPI calls default to enable
MPI_Default enable
# Individual MPI calls can override the default setting, e.g.:
MPI_Iprobe   success
MPI_Test     success
MPI_Testsome success
MPI_Testany  success
MPI_Testall  success
#
# Output can also be selectively suppressed for status information (it is
# big and clumsy, and you may not want to use it).  
# success is synonymous with enable.
# statuses (disable|success|enable)   # defaults to enable
statuses     enable

#
# There is a whole set of other calls for PAPI profiling support.
# By default, all PAPI calls are disabled unless explictly turned on.
# To turn on a PAPI call, just specify its name (not commented), e.g.
PAPI PAPI_TOT_CYC   # total cycles
PAPI PAPI_LST_INS   # total load store instructions
PAPI PAPI_BR_INS    # total branch instructions executed
# Note that you can also specify non-native PAPI calls this way 
# -- use papi_avail or papi_native_avail to figure out the name of
# the performance counter you want to use

#
# You can enable or disable debugging from this file.
# The valid flags are:
#     debug off      Disable all debugging
#     debug all      Enable all debugging
#     debug traceio  Get info about io functions
#     debug mpicalls Get info about when MPI functions are entered or exited
#     debug libdumpi Get debug info about internal workings of libdumpi
#
# All debug flags except "off" are cumulative, so you can use
#     debug traceio
#     debug mpicalls
# to get info about both traceio and mpicalls.
debug        off
#+END_EXAMPLE
***** AMG
Je viens de voir sur le site des traces AMG que :

"Note that, unlike the IPM data, the dumpi traces only track the
communications for one V-cycle of the multigrid sequence. This was
accomplished by turning off profiling for most of the code using the
dumpi API and turning it on only in the single routine in the file
parcsr_ls/par_cycle.c. Within this file, the variable maxit_prec was set
equal to 1."

C'est peut être pour ca que je vois pas grand chose en OTF avec VITE.

En visualisant les traces trouvées sur internet avec VITE, j'ai
remarqué qu'elles sont plus pauvres que celle que je génère, ca me
rassure.

Toutes les traces que je trouve en fait donne le même genre de
visualisation sur VITE, toute réduite à la fin comme celle que je
génère. Mais les exemples de traces OTF sur le site de Vampir donne
une visualisation "attendu" qui rempli toute la période.
**** Mail Arnaud
***** Content
#+BEGIN_EXAMPLE
Salut,

Le 08/06/2017 09:17, Maxime Chevalier a écrit :
>> Pour avancer dans notre compréhension de CODES et de ses performance, il
>> faudrait des traces plus grandes. Quand on n'a pas de grosse machine
>> sous la mains, ça n'est pas forcément évident... C'est pour ça que je
>> mentionnais scalaExtrap, sous projet de scalatrace. Tu peux essayer mais
>> j'ai peur que ça soit moins propre que CODES.
> 
> Du coup j'abandonne Grid'5000 pour faire de plus grandes traces, en
> tentant de remplacer DUMPI par scalatrace, et en les extrapolant via
> scalaExtrap ?

Toutes les approches présentent des difficultés. J'ai peur que tu ne
puisses pas aller à très grande échelle sur Grid5000 (quelques
centaines de processus, ce qui est déjà pas mal, mais si on a besoin
de plusieurs milliers pour donner assez de travail à CODES on risque
d'être un peu coincé...) mais si tu as commencé et/ou que c'est
quelque chose qui te tente, il faut te lancer! :) Ça ne sera pas perdu
de toutes façons.

> Une autre approche serait d'utiliser SimGrid mais ça veut dire arriver à
> générer des traces DUMPI. C'est ce que disait Lucas après tout, on ne
> sait jamais, sur un coup de chance, ça pourrait marcher puisque SMPI
> fourni l'interface PMPI. ;)
> 
> J'ai pas trop compris ce point, il faut que j'exécute HPL sur SimGrid et
> celui-ci me fournisse des traces DUMPI ?

Ça serait l'idée. Exécuter HPL sur SimGrid, actuellement, c'est
facile, ça se fait sur un portable et ça va relativement vite. Par
contre, SimGrid fournit des traces paje, pas DUMPI... :( Mais en
linkant avec libDUMPI, ça pourrait marcher. Ceci dit tu vas être
ennuyé car même si tu génères ainsi une grosse trace DUMPI de HPL, ça
ne résoudra pas le problème que CODES a l'air de planter à l'heure
actuelle avec une telle trace. Et si c'est une trace de AMG qu'on
souhaite obtenir, SimGrid devrait fonctionner mais si l'application
n'est pas préparée (c'est ce que Tom a fait pendant son stage), ça
sera très lent...

À bientôt,

    Arnaud
#+END_EXAMPLE
**** ROSS
Un article qui permet de comprendre un peu mieux comment fonctionne
ROSS :
https://pdfs.semanticscholar.org/8cf9/e252c8314e26f20b619acb6392d52abac647.pdf
Notament avec une explication des LP/KP/PE et une évaluation de
performance sur ROSS.

Il y a un outil qui permet de faire des traces de la simulation :
[[http://carothersc.github.io/ROSS/feature/instrumentation.html][link]].
#+BEGIN_EXAMPLE
Event Tracing
There are two ways to collect the event trace. One is to collect data only about events that are causing rollbacks. When an event that should have been processed in the past is received, data about this event is collected (described below). The other method of event tracing is to collect data for all events. For event tracing, ROSS can directly access the source and destination LP IDs for each event, as well as the sent and received virtual timestamps. It will also record the real time that the event is computed at. To turn on the event tracing with either --event-trace=1 for full event trace or --event-trace=2 for tracing only events that cause rollbacks.
Because event types are determined by the model developer, ROSS cannot directly access this. The user can create a callback function that ROSS will use to collect the event type, and ROSS will then handle storing the data in the buffer and the I/O. The benefit to this is that the user can choose to collect other data about the event, and ROSS will handle this as well. It’s important to remember that this data collection will result in a lot of output, since it’s happening per event. The details on using event tracing with model-level data is described in the next section on Model-level sampling
#+END_EXAMPLE

Mais il faut modifier des choses dans le code/model.

Il y a un merge request sur le git de CODES, qui ajoute l'event
tracing aux modèles.On pourrait partir de là ? [[https://xgitlab.cels.anl.gov/codes/codes/merge_requests/28/diffs][lien]]

Un autre merge request vient juste d'être posté ajoutant "codes-vis
model-level instrumentation WIP" et d'autres choses [[https://xgitlab.cels.anl.gov/codes/codes/merge_requests/30][lien]] . Ce dernier
merge request à l'air d'être très complet en reprenant le premier
merge et en changeant énormémant de choses au niveau de MPI, des
messages d'erreurs, des changements de types de données,...

Donc je pense que la gestion des traces dans CODES arrive sous
peu. Cependant dans la doc ROSS c'est un format à eux qui est utilisé,
je pense pas qu'on puisse linker DUMPI avec.
