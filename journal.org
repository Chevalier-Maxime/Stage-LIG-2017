# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:       Journal de bord
#+AUTHOR:      Maxime Chevalier 
#+LANGUAGE:    fr
#+TAGS: LIG(L) SimGrid(S) PSI3(P) CODES(C) ROSS(O) Space(A) Time(T)
#+TAGS: R(R) OrgMode(M) Deprecated(D) DUMPI(U)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* 2016
** 2016-03 March
*** 2016-03-07 Monday
**** [[https://github.com/alegrand/RR_webinars/blob/master/1_replicable_article_laboratory_notebook/index.org][First webinar on reproducible research: litterate programming]]
***** Emacs shortcuts
Here are a few convenient emacs shortcuts for those that have never
used emacs. In all of the emacs shortcuts, =C=Ctrl=, =M=Alt/Esc= and
=S=Shift=.  Note that you may want to use two hours to follow the emacs
tutorial (=C-h t=). In the configuration file CUA keys have been
activated and allow you to use classical copy/paste (=C-c/C-v=)
shortcuts. This can be changed from the Options menu.
  - =C-x C-c= exit
  - =C-x C-s= save buffer
  - =C-g= panic mode ;) type this whenever you want to exit an awful
    series of shortcuts
  - =C-Space= start selection marker although selection with shift and
    arrows should work as well
  - =C-l= reposition the screen
  - =C-_= (or =C-z= if CUA keys have been activated)
  - =C-s= search
  - =M-%= replace
  - =C-x C-h= get the list of emacs shortcuts
  - =C-c C-h= get the list of emacs shortcuts considering the mode you are
    currently using (e.g., C, Lisp, org, ...)
  There are a bunch of cheatsheets also available out there (e.g.,
  [[http://www.shortcutworld.com/en/linux/Emacs_23.2.1.html][this one for emacs]] and [[http://orgmode.org/orgcard.txt][this one for org-mode]] or this [[http://sachachua.com/blog/wp-content/uploads/2013/05/How-to-Learn-Emacs-v2-Large.png][graphical one]]).
***** Org-mode                                                  :OrgMode:
  Many emacs shortcuts start by =C-x=. Org-mode's shortcuts generaly
  start with =C-c=.
  - =Tab= fold/unfold
  - =C-c c= capture (finish capturing with =C-c C-c=, this is explained on
    the top of the buffer that just opened)
  - =C-c C-c= do something useful here (tag, execute, ...)
  - =C-c C-o= open link
  - =C-c C-t= switch todo
  - =C-c C-e= export
  - =M-Enter= new item/section
  - =C-c a= agenda (try the =L= option)
  - =C-c C-a= attach files
  - =C-c C-d= set a deadl1ine (use =S-arrows= to navigate in the dates)
  - =A-arrows= move subtree (add shift for the whole subtree)
***** Org-mode Babel (for literate programming)                 :OrgMode:
  - =<s + tab= template for source bloc. You can easily adapt it to get this:
      : #+BEGIN_SRC sh
      : ls
      : #+END_SRC
    Now if you =C-c C-c=, it will execute the block.
    #+BEGIN_EXAMPLE
  #+RESULTS:
  | #journal.org# |
  | journal.html  |
  | journal.org   |
  | journal.org~  |
    #+END_EXAMPLE
  
  - Source blocks have many options (formatting, arguments, names,
    sessions,...), which is why I have my own shortcuts =<b + tab= bash
    block (or =B= for sessions).
    #+BEGIN_EXAMPLE 
  #+begin_src sh :results output :exports both
  ls /tmp/*201*.pdf
  #+end_src

  #+RESULTS:
  : /tmp/2015_02_bordeaux_otl_tutorial.pdf
  : /tmp/2015-ASPLOS.pdf
  : /tmp/2015-Europar-Threadmap.pdf
  : /tmp/europar2016-1.pdf
  : /tmp/europar2016.pdf
  : /tmp/M2-PDES-planning-examens-janvier2016.pdf
    #+END_EXAMPLE
  - I have defined many such templates in my configuration. You can
    give a try to =<r=, =<R=, =<RR=, =<g=, =<p=, =<P=, =<m= ...
  - Some of these templates are not specific to babel: e.g., =<h=, =<l=,
    =<L=, =<c=, =<e=, ...
***** In case you want to play with ipython on a recent debian   :Python:
Here is what you should install:
#+begin_src sh :results output :exports both
sudo apt-get install python3-pip ipython3 ipython3-notebook python3-numpy python3-matplotlib
#+end_src

The ipython notebook can then be run with the following command:
#+begin_src sh :results output :exports both
ipython3 notebook
#+end_src

The latest version of this notebook is called [[http://jupyter.org/][Jupyter]] and is polyglot
like babel. Playing with it is easy as it's deployed on the cloud but
as I'm not a python expert I'm not sure to know how to deploy it locally.
***** In case you want to play with R/knitR/rstudio:                  :R:
Here is what you should install on debian:
#+BEGIN_SRC sh
sudo apt-get install r-base r-cran-ggplot2
#+END_SRC

Rstudio and knitr are unfortunately not packaged within debian so the
easiest is to download the corresponding debian package on the [[http://www.rstudio.com/ide/download/desktop][Rstudio
webpage]] and then to install it manually (depending on when you do
this, you can obviously change the version number).
#+BEGIN_SRC sh
wget https://download1.rstudio.org/rstudio-0.99.887-amd64.deb
sudo dpkg -i rstudio-0.99.887-amd64.deb
sudo apt-get -f install # to fix possibly missing dependencies
#+END_SRC
You will also need to install knitr. To this end, you should simply
run R (or Rstudio) and use the following command.
#+BEGIN_SRC R
install.packages("knitr")
#+END_SRC
If =r-cran-ggplot2= could not be installed for some reason, you can also
install it through R by doing:
#+BEGIN_SRC R
install.packages("ggplot2")
#+END_SRC

As you will experience, knitr is polyglot but not Rstudio, which
makes its use not as fluid when using other languages than R.
* 2017
** 2017-05 mai
*** 2017-05-09 mardi
**** DONE Lecture papiers [3/3]                                   :ATTACH:
:PROPERTIES:
:Attachments: PDS_fujimoto2015.pdf
:ID:       a3acf95b-21d5-4621-a955-41bab99b38f6
:END:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-05-11 jeu. 15:12]
- State "TODO"       from "TODO"       [2017-05-11 jeu. 14:56]
- State "TODO"       from "TODO"       [2017-05-11 jeu. 11:23]
- State "TODO"       from "TODO"       [2017-05-11 jeu. 11:03]
- State "TODO"       from "TODO"       [2017-05-11 jeu. 09:53]
- State "TODO"       from "TODO"       [2017-05-10 mer. 17:23]
- State "TODO"       from "TODO"       [2017-05-10 mer. 16:06]
- State "TODO"       from "TODO"       [2017-05-10 mer. 16:04]
- State "TODO"       from "TODO"       [2017-05-10 mer. 16:04]
- State "TODO"       from "TODO"       [2017-05-10 mer. 16:04]
- State "TODO"       from "TODO"       [2017-05-10 mer. 16:03]
- State "TODO"       from "TODO"       [2017-05-10 mer. 15:56]
- State "TODO"       from "TODO"       [2017-05-10 mer. 15:32]
- State "TODO"       from "TODO"       [2017-05-10 mer. 15:27]
- State "TODO"       from "TODO"       [2017-05-10 mer. 15:15]
- State "TODO"       from "TODO"       [2017-05-10 mer. 15:11]
- State "TODO"       from "TODO"       [2017-05-10 mer. 15:11]
- State "TODO"       from "TODO"       [2017-05-10 mer. 15:11]
- State "TODO"       from "TODO"       [2017-05-10 mer. 15:11]
- State "TODO"       from "TODO"       [2017-05-10 mer. 14:39]
- State "TODO"       from "TODO"       [2017-05-10 mer. 13:56]
- State "TODO"       from "TODO"       [2017-05-10 mer. 13:48]
- State "TODO"       from "TODO"       [2017-05-10 mer. 13:48]
- State "TODO"       from "TODO"       [2017-05-10 mer. 13:48]
- State "TODO"       from "TODO"       [2017-05-10 mer. 13:13]
- State "TODO"       from "TODO"       [2017-05-10 mer. 11:06]
- State "TODO"       from "TODO"       [2017-05-10 mer. 11:06]
- State "TODO"       from "TODO"       [2017-05-10 mer. 10:40]
- State "TODO"       from "TODO"       [2017-05-10 mer. 10:38]
- State "TODO"       from "TODO"       [2017-05-10 mer. 10:35]
- State "TODO"       from "TODO"       [2017-05-10 mer. 10:35]
- State "TODO"       from "TODO"       [2017-05-10 mer. 10:35]
- State "TODO"       from "TODO"       [2017-05-10 mer. 10:33]
- State "TODO"       from "TODO"       [2017-05-10 mer. 10:18]
- State "TODO"       from "TODO"       [2017-05-10 mer. 09:40]
- State "TODO"       from "TODO"       [2017-05-09 mar. 16:49]
- State "TODO"       from "TODO"       [2017-05-09 mar. 16:49]
- State "TODO"       from "TODO"       [2017-05-09 mar. 16:49]
- State "TODO"       from "TODO"       [2017-05-09 mar. 16:30]
- State "TODO"       from "TODO"       [2017-05-09 mar. 16:27]
- State "TODO"       from "TODO"       [2017-05-09 mar. 16:15]
- State "TODO"       from "TODO"       [2017-05-09 mar. 16:15]
- State "TODO"       from "TODO"       [2017-05-09 mar. 16:15]
- State "TODO"       from "TODO"       [2017-05-09 mar. 15:54]
- State "TODO"       from "TODO"       [2017-05-09 mar. 15:54]
- State "TODO"       from "TODO"       [2017-05-09 mar. 15:36]
- State "TODO"       from "DONE"       [2017-05-09 mar. 15:36]
- State "DONE"       from "TODO"       [2017-05-09 mar. 15:36]
- State "TODO"       from "TODO"       [2017-05-09 mar. 15:36]
- State "TODO"       from "TODO"       [2017-05-09 mar. 15:36]
- State "TODO"       from "TODO"       [2017-05-09 mar. 15:36]
- State "TODO"       from              [2017-05-09 mar. 15:12]
:END:
- [X] simulation «[[file:Papiers/Timeparallelsimulation.pdf][temps parallèle]]» (extrait du livre de
      Fujimoto, 2000)
  - *Space parallel approch* : C'est la decomposition horizontale d'un
    programme, où chaque process maintient ses variables d'état tout au long de la simulation.
    - Plus flexible, et plus applicable (dans la plupart des cas).
  - *Time parallel approch* :
    - Propriétés : Parallélisme massif, indépendance des processeurs
      logiques (moins de synchronisation)
    - Approches : comment déterminer l'état initial des process (sauf le premier process), il faut corriger la trajectoire.
      - *Fix-up computation* : Etat initial random, qui ne match presque
        jamais, donc il faut faire une correction avec une seconde
        simulation (et ca fonctionne grâce à la contraction des
        ensembles de trajectoires) prenant en état initial l'état
        final du process précédant. Et quand ca match, c'est gagné!
        Fonction le mieux lorsque l'état final ne dépend pas de l'état
        initial (2 itérations et c'est fini). Lorsque ce n'est pas le
        cas, (pire cas) il faut pour N intervals, N itérations, soit
        du sequentiel avec des ressources pour du parallèle
        (bad!). *Exemple ou ca fonctionne* cache LRU.
      - *Precomputation of state at specific time division points* : Il
        se peut que dans certains programmes on puisse déterminer des
        points spécifiques (débordement de buffer par exemple) qui
        vont arriver et donc en faire des états de départ. *Exemple* :
        buffer avec N entrées de capacité 1 et une sortie de capacité C.
        - Soit il y a overflow
        - Soit il y a underflow
        - Du coup chaque processus détermine si il part d'un
          over/under et effectue sa simulation pour X services. Quand
          il a fini, il envoie le reste de sa file d'attente au
          processus d'après qui se charge de simuler l'interval entre
          l'état d'arrêt du processus précédent, et son postulat de départ.
        - À la fin, on peut calculer le nombre de service et de perte
          en additionnant les résultats de chaque processus.
        - Cependant ca pet échouer car les processus peuvent ne pas
          atteindre l'objectif (overflow souvent car c'est très
          rare). Il est donc très difficile d'identifier les points de division.
      - *Parallel prefix computations* :
        - Utilisation de récurence car il existe de très bon
          algorithmes pour faire du calcul prefix sur des systèmes parallèles.
        - Il faut faire attention au ratio nbCalculs/nbProcesseurs. En
          effet, si ce ratio tend vers 1, il y a des pertes à cause du
          temps de communication entre les processeurs (qui est du
          temps de calcul perdu).
        - /!\ Je n'ai pas compris l'exemple de cette partie (page 9 et 10).
    - Plutôt utilisé pour développer des algo de simulation parallèle
      spécifique.
    - Permet de parralléliser des cas où le space parralélisme ne
      fonctionne pas (ou peu), comme dans les exemples.
  - Dans certain cas, il est possible de mélanger les deux approches.

- [X] Et un [[file:Papiers/PDS_fujimoto2015.pdf][autre]] Fujimoto (2015) pour avoir une vision d’ensemble sur la
      simulation parallèle (sauf sur la technique «  temps-parallèle »  qui
      n’est pas mentionnée dans cet article)
  - Lecture :
    - *PDES* : Parallel Discrete Event Simulation. But : accelerer
      l'execution des simulation. Les "pas" de simulations sont
      irrégulier. Peut être vu comme une collection d'evenements
      sequentiels discret, qui interagissent par des messages
      datés. Un PDES doit fournir les mêmes résutats que le programme
      en séquentiel (ou parfois aproximativement les mêmes). Le
      principal problèlme c'est la synchrinisation entre les
      differents processus.

    - *Simulation distribué* : concerne l'execution de simulation sur
      des machines séparé "géographiquement", où l'agrégation de
      simulation pour un environnement de simulation (*exemple*
      simulateur de vol,...). Synchronisation = time management. De
      gros efforts ont été fait pour developer des standards
      d'interconnection de simulation (DIS,HLA) qui, entre autre
      réduisent le nombre de message envoyé.

    - *Synchronisation conservative* : Permet d'avoir le même résultat
      qu'une exécution séquentielle sur un processeur monocœur. Cette
      propriété peut être montré en s'assurant que chaque processus
      logique traite les messages par leurs dates.

      - Première génération : Pour qu'il y ait du parallèlisme, il
        faut cependant que certains LP (processus logique) puissent
        prendre de l'avance. Pour cela il faut s'assurer que l'on ne
        recevera pas d'event avec un horadatage plus petit que celui
        qu'on va executer (*ALGO CMB). *Contraintes* : réseau FIFO. Tant qu'il y a
        des messages dans les buffers le LP à l'information. Si le
        buffer est vide, il doit se bloquer /!\ il peut y avoir des
        interblocages. Pour les éviter les LPs envoient des messages
        nulls qui garantissent les horodatages. Les LP ont aussi un
        *lookahead* qui defini le temps min avant l'envoie d'un autre
        message (l'horadatage du prochain evenement doit être bien
        plus grand que le temps actuel). Cependant, si il est trop petit, de nombreux messages
        inutiles peuvent être envoyé si il y a blocage (cf exemple
        fin page 4).

      - Seconde génération : Dans un premiers temps, les nouveaux algo
        ont tenté de supprimer le problème de lookahead en utilisant
        le plus petit horadatage d'event dans le système, pour passer
        directement à la suite. D'autres approche complétement
        différentes de CMB ont été tenté, avec par exemple YAWNS qui
        utilise des points globaux de synchronisation (des
        barrières). *Schéma lookahead* question : Je suis pas sûr de
        comprendre comment ca marche... *Réponse* : SimGrid ne
        fonctionne pas sur ce modèle. C'est juste dire aux autres LPs
        que le LP ne va pas envoyer de message avant T+L, et qu'ils
        peuvent donc process jusque là.

    - *Synchronisation optimiste* : Les events ne sont plus éxécutés
      strictement dans l'ordre.(*TIME WARP*)contrairement aux algorithmes
      précédents, les erreurs sont possible, mais un mécanisme de
      rollback permet de retourner dans un état sain grâce à deu
      mécanismes :

      - *Mécanisme de control local* : Si un LP recoit un event daté de
        50 et qu'il en est à 100, il va faire un rollback
        jusqu'a 50. Cependant, si il y a eu modification des variables
        d'état, ou envoie de message ca ne peut pas être annulé. Pour
        les variables d'état il est possible de les sauvegarder avant
        modification (*copy state saving*) qui a un *cout* en temps de
        copie et en mémoire) ou faire de *l'incremental state saving*
        qui sauvegarder seulement les infos qu'on a modifié mais qui
        nécessite d'avoir les adresses des variables (à
        modifier/backup). Une autre technique est le *reverse
        computation* (si un event fait +1, on fait -1) qui est la plus
        avantageuse des 3, mais ca peut de
        pas être réalisable. Pour les messages, un système anti
        message récursif a été mis au point. On r'envoie le message
        identique avec un marqueur, si il a pasété traité, il est
        supprimé de la file, sinon il y a rollback pour le LP qui
        recoit l'anti-message (et ainsi de suite, pouvant conduire à
        des rollbacks en cascade /!\)

      - *Mécanisme de contrôle global* : GVT : Pas compris. C'est un
        état global, mais j'ai pas compris comment il est utilisé/ce
        qui est sauvegardé. Mécanisme de coupe comme vu en algo
        distribué (RICM4). Ok donc GVT c'est une barrière dans le
        temps avant laquelle on ne peut pas revenir lors d'un
        rollback. Du coup, il est possible de libérer les ressources
        utilisé pour sauvegarder les états avant cette barrière.
        https://www.acm-sigsim-mskr.org/Courseware/Fujimoto/Slides/FujimotoSlides-12-ComputingGlobalVirtualTime.pdf

        - D'autres pistes sont envisagées, comme limiter dans une
          fenêtre de temps les LPs (ce qui empêche certains LP d'être
          trop en avance et de faire beaucoup de rollback). Une autre
          technique et de retarder les envoies de messages jusqu'a ce
          que ce soit garantie que l'envoie ne va pas être rollback
          plus tard.

- [X] Les outils : 
  - [X] SimGrid un simulateur d’applications parallèles 
    http://simgrid.gforge.inria.fr (présentation générale http://simgrid.gforge.inria.fr/tutorials/simgrid-101.pdf)

    - SimDag : Framework pour application parralèles. Les tâches avec
      dépendances limitent la parralèlisme. On fourni à SimDag le
      graphe des tâches et ça retourne des informations sur les
      machines qui ont fait tels taches.

    - MPI : couche d'abstracton adapté à la programation parallèle
      (partie communication).

    - SMPI : Simule des application // avec MPI. Réalise le calcul
      pour de vrais en simulant l'architecture, mais en séquentiel. LE
      but c'est pas forcément le résultat, mais plutôt de voir si
      l'algorithme fonctionne bien sur l'architecture choisie.

    - MSG : Visualisation de données

    - Fonctionne comme en RO pour simuler les architechtures.

    - Permet plusieurs approches :

      - On-line : simulation sur sa propre machine. Mais /!\ à la
        topologie logique (grille par ex) qui est ensuite mappé sur
        une topologie physique différente. SimGrid peut également
        simuler cette différence.

      - Off-line : Execution du programme sur un super-calculateur
        puis récupération des traces. Avec ces trâces, rejouer avec
        des topologies différentes. Cependant cette approche nécéssite
        d'avoir accès à un super-calculateur, mais est limité pas la
        taille initiale de la topologie (on peut difficilement
        extrapoler). De plus, pour pouvoir jouer avec les trâce sur
        des configurations différentes, il faut que le code soit
        déterministe, hors il ne l'est pas nécessairement.

  - [X] PSi3 : Perfect simulator. S'arrête à la districbution
    stationnaire /!\ coût élevé
    - [X] un simulateur [développé par Inria] contenant une version
      parallèle « à la Fujimoto/Nicol » 
      https://gforge.inria.fr/projects/psi/ dédié aux files d'attentes
      réseaux (en temps discret). Le logiciel fournis des traces
      d'éxecution qui sont ensuite utilisable pour analyse (avec R par
      exemple). Ressemble beaucoup aux TD/DM de EP (les exemples).
    - [X]  [[file:Papiers/PDS_fujimoto2015.pdf][slides de présentation ci-joints (Briot/Vincent)]] Simulation
      de chaine de Markov, et utilisation des processus de vie et de
      mort. Le nombre d'etat doit être inferieur à 10⁷ pour les
      calculs numériques. Pour les simulations le cout est
      linéaire. Le but est de générer beaucoup d'exemples grâce un
      script de simulation pour ensuite les analyser. Psi3 permet de
      faire des statistiques sur les réseaux à file finie sur des
      évènements rare : rejet, blocage, ... avec la garantie
      d'indépendance entre les traces. La simulation utilises
      plusieurs coeurs (framework OpenMP cf RICM3). Il y a plusieurs
      approches pour la simulation :
      - Space //, Time // et les deux

      - Backward simulation : chaque coeur fait une trace.

      - Forward simulation *?? question*

      - Peret la //isation des entrées sorties.
  - [X] CODES : un simulateur [développé par Argonne] un simulateur
                parallèle d’applications parallèles
                https://press3.mcs.anl.gov/codes/ basé sur le
    framework ROSS. Ce simulateur a pour but d'augmenter la
    parallélisation sur du stackage de masse et pour les applications
    qui utilisent beaucoups de données. Dans le but de tester des
    architectures Exascale qui n'existe pas encore. Ca permet de tester des
    algorithmes avec une simulation de réseau faite par ROSS et des
    simulations d'I/O modélisé par CODE. Avec ce simulateur on peut
    mettre en évidence des goulots, évaluer la tolérance aux fautes,
    la mise à l'échelle et les techniques de
    "recovery". L'architechture TORUS (architecture réseau à plusieurs
    dimmentions) a été directement ajouté dans CODES et non dans
    ROSS. On peut donc ajouter nos propres modèles. Voici en
    attachement 2 articles présentant CODES
    -[X] [[file:Papiers/P1884.pdf][1]] 
    - 4.2 - Figure 3 : pas compris les résultats de l'expérience. 
    -[X] [[file:Papiers/LiuCarothers2012.pdf][2]] Possibilité de s'inspirer de ce papier pour la démarche sur
    des études
  - [X] ROSS :  simulateur parallèle, lui-même basé sur le
                mécanisme Time Warp 
                http://carothersc.github.io/ROSS/about.html . La
    simulation est basé sur des LPs, chacun modélisant un composant du
    système. Pour communiquer, les LPs utilisent des message horodaté.
                https://github.com/carothersc/ROSS. 
                
  - [X] Time Warp : Pour Time Warp il y a l’[[file:Papiers/Jefferson87.pdf][article]] de Jefferson et
                    al. qui résume à peu près. Utilisation d'un temps
    virtuel pour la synchronisation (Jefferson, cf Cours AD
    RICM4). Principe du rollback cité plus haut. Time Warp est à la
    base un OS, car le faire au dessus d'un autre OS n'ayant pas la
    même logique doublé tous les éléments (scheduling, I/O, ...). Time
    Warp est destiné aux applications utilisant un temps logique ou un
    temps de simulation dans leur modèle. La condition FIFO n'est plus
    nécessaire même si elle est préférable. Il y a régulierement des
    sauvegarde globales du système (de type GVT je suppose).
    - Anti-message : Comme vu plus haut, mais plus détailé. Chaque
      message envoyé engendre un anti-message dans le "buffer"
      d'emission qui n'est pas envoyé. Cependant, si il y a un
      rollback, le LP reprend son exécution et à chaque envoie de
      message, il compare avec les anti-message : si il y a un même
      message anti + nouveau, les deux sont supprimé (on a déjà envoyé
      le message), si il n'y a pas correspondance, un anti-message du
      nouveau est conservé. Tous les anti-message non représenté
      durant la réexecution doivent être annulé chez les autres
      LPs. Ensuite c'est l'algo en cascade.

    - La communication entre les processus ne se fait pas via des
      pipe,... il n'est donc pas nécéssaire de déclarer quel process
      communique avec quel process. Il suffi de faire un
      send. Question : != entre QueryMessage et EventMessage

    - Structure d'un process : cf page 10

    - Les processus s'executent qu'a la réception de message et font
      le traitement, ils ne font pas de calcul en dehors. Ils peuvent
      cependant s'envoyer eux-même des messages. Les processus doivent
      être deterministe (l'execution d'une même entrée doit donner la
      même sortie pour eviter les rollback en cascade. Pour les nombre
      aléa jouer avec la seed).

    - Utilisation de beaucoup de mémoire pour le GVT et pour la
      sauvegarde des anti-messages. Si il y a plus de place, le plus
      grand message (T dans le système, le plus loin dans le futur) est désenvoyé ce qui cause un rollback quelque
      part et donc libère de la place. Le méssage sera réenvoyé plus tard.

  - [X] Slide d'Arnaud sur les infrastructures de calcul :
   http://mescal.imag.fr/membres/arnaud.legrand/blog/2015/03/25/intervention_isn-gz.pdf
    - Tentative de mélanger les calculs et les communication pour
      réduire les barrières (MUMPS)

    - Explication simple d'un calcul de pivot sur une matrice
      diagonale.



**** Réunion Maxime Chevalier, simulation parallèle
- Lu chapitre simulation time // de Fujimoto
- A lu papier sur SG, PSI, CODES/ROSS (torus)
- A commencé à intégrer les notions de simulation time //, space //,
  simulation parfaite, simulation de Monte Carlo, à évènements
  discrets (pas forcément de chaine de Markov).
- À faire:
  - Journal:
    - Créer un repos github privé et nous donner les droits
      (alegrand, fperronnin).
    - On vérifie la config org-mode
  - Simulation:
    - Installer CODES/ROSS
    - Récupérer une trace de HPL (générée "en vrai" ou avec SimGrid)
    - Rejouer cette trace au dessus de CODES en séquentiel
    - Rejouer cette trace au dessus de SimGrid (en séquentiel donc)
    - Rejouer cette trace au dessus de CODES en multi-core
    - Rejouer cette trace au dessus de CODES en distribué (MPI)
    - Puis on avise pour le time //
**** Premiers pas avec org-mode
#+begin_src sh :results output :exports both
ls /tmp/
#+end_src

#+RESULTS:
: babel-58401Ra
: babel-8460yua
: emacs8460hfp
: systemd-private-309cd01bea0a4d7cbb3d3706e339b36a-colord.service-6SHoQ5
: systemd-private-309cd01bea0a4d7cbb3d3706e339b36a-rtkit-daemon.service-DraqAW
: tracker-extract-files.1000

#+begin_src python :results output :exports both
print("Hello")
#+end_src

#+RESULTS:
: Hello

#+begin_src R :results output :session *R* :exports both
summary(cars)
#+end_src

#+BEGIN_EXAMPLE
This seminar took place on March 7, 2016. The link to the video is below. Do not forget to check the section on software installation if you want to test yourself the demoed tools.

Table of Contents
#+END_EXAMPLE

**** Réunion avec Florence et Arnaud
***** DONE Mettre en place mon cahier de laboratoire [3/3]
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-05-09 mar. 15:05]
- State "TODO"       from "TODO"       [2017-05-09 mar. 15:05]
- State "TODO"       from "TODO"       [2017-05-09 mar. 13:54]
- State "TODO"       from "STARTED"    [2017-05-09 mar. 13:54]
- State "STARTED"    from "TODO"       [2017-05-09 mar. 13:52]
- State "TODO"       from              [2017-05-09 mar. 13:52]
:END:

- [X] github
- [X] org-mode
- [X] intégrer le mail

Entered on [2017-05-09 mar. 13:45]
******
*** 2017-05-11 jeudi
**** Fin des lectures
**** Installation CODES/ROSS sur ma machine
***** DEFERRED ROSS                                                :ROSS:
:LOGBOOK:
- State "DEFERRED"   from              [2017-05-15 lun. 13:35]
- State "DEFERRED"   from "CANCELLED"  [2017-05-15 lun. 13:35]
- State "CANCELLED"  from "DONE"       [2017-05-15 lun. 13:35]
- State "DONE"       from "APPT"       [2017-05-15 lun. 13:35]
- State "APPT"       from "WAITING"    [2017-05-15 lun. 13:35]
- State "STARTED"    from "TODO"       [2017-05-11 jeu. 17:51]
- State "TODO"       from              [2017-05-11 jeu. 17:51]
:END:
Lien Git : https://github.com/carothersc/ROSS
Requirement  : 
- C compiler (C11 prefered but not required)
- CMake : https://cmake.org (2.8 min) -> 3.6.2
#+begin_src sh
sudo yum install cmake
#+end_src
- Implémentation MPI : MPICH recommandé -> 3.2
#+begin_src sh
sudo yum install mpich
#+end_src
Machine : Fedora 25, gcc version 6.3.1, intel core i5, 8Go de RAM
****** Tentative installation 1
Ca ne fonctionne pas en suivant les étapes du git, je supprime le
dossier et je passe par un autre tuto.
****** Tentative d'installation 2
https://github.com/carothersc/ROSS/wiki/Installation
En fait j'ai une erreur dans la configuration de cmake...
#+BEGIN_EXAMPLE
CMake Error at /usr/share/cmake/Modules/CMakeDetermineCCompiler.cmake:57 (message):
  Could not find compiler set in environment variable CC:

  mpicc.
Call Stack (most recent call first):
  CMakeLists.txt:1 (PROJECT)


CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage
-- Configuring incomplete, errors occurred!
See also "/home/chevamax/Documents/ross-build/CMakeFiles/CMakeOutput.log".
#+END_EXAMPLE

(lundi 15 mai)
En fait mpich n'était pas dans mon PATH...
On retante.
Ca ne marche toujours pas, mais je viens de
trouver. https://ask.fedoraproject.org/en/question/32864/mpirun-command-not-found/
Il faut charger le module pour pouvoir l'utiliser. Du coup il faut
ajouter cette ligne au fichier de conf : 

#+BEGIN_EXAMPLE
module load mpi/mpich-x86_64 
#+END_EXAMPLE

#+begin_src
cd
gedit .bash_profile
#+end_src

J'y ai cru mais en fait non...

*** 2017-05-15 lundi
**** DONE ROSS                                                      :ROSS:
:LOGBOOK:
- State "DONE"       from ""       [2017-05-15 lun. 14:35]
:END:
***** intallation                                            :Deprecated:
CF CODES pour l'instalation
Reprise de l'installation, sur un nouvel environnement, Ubuntu cette
foi-ci (17.04)
On repart du début.
https://github.com/carothersc/ROSS
Version gcc : 6.3.0 20170406
Version cmake : 3.7.2-1
Version MPICH : 3.2-7
Version Doxygen 1.8.13

C'est bon tout fonctionne !
#+BEGIN_EXAMPLE
git clone -b master --single-branch git@github.com:carothersc/ROSS.git
mkdir ROSS-build
cd ROSS-build
export ARCH=x86_64
export CC=mpicc
cmake -DROSS_BUILD_MODELS=ON ../ROSS
make
make test
#+END_EXAMPLE

Avec ces commandes, on peut tester si tout fonctionne bien (car on a
des modèles et des tests). Si tout passe c'est OK. Merci ubuntu.

**** DONE CODES                                                    :CODES:
:LOGBOOK:
- State "DONE"       from "APPT"       [2017-05-15 lun. 17:17]
- State "APPT"       from "WAITING"    [2017-05-15 lun. 17:17]
- State "STARTED"    from "TODO"       [2017-05-15 lun. 17:17]
- State "TODO"       from              [2017-05-11 jeu. 17:51]
:END:
Sources : https://press3.mcs.anl.gov/codes/downloads/
Version : 0.5.2
Entered on [2017-05-15 lun. 09:34]
Mainenant que ROSS a l'air de fonctionner on passe à CODES.

En fait pour installer les deux il suffi de suivre les consignes dans
le fichier d'installation de CODES.
/!\ insallation dumpi
**** Traces HPL
Maintenant que tout est installé et que je commence à comprendre, il
faut se diriger vers le premier objectif.
Je lis donc la doc GETTING STARTED, pour comprendre comment se compose
le logiciel, et avoir un point d'entrée.
***** Meeting avec Florence
CODES utilise DUMPI pour les traces. Il faut donc que je génère des
trâces avec mon PC grace à un logiciel HPL en MPI, que je pourrais
réinjecter. On tente de voir Arnaud demain.
*** 2017-05-16 mardi
**** DONE DUMPI
http://sst.sandia.gov/about_dumpi.html
J'ai l'impression qu'il faut installer tout le simulateur(SST) pour pouvoir
profiter de DUMPI. Cependant ils parlent le package, alors c'est peut
être un standalone 
profiter de DUMPI. 
**** Meeting avec Florence et Arnaud
- Les sources de DUMPI sont en fait sur [[https://github.com/sstsimulator/sst-dumpi][github]].
- D'après la [[https://github.com/sstsimulator/sst-dumpi/blob/master/docs/traceformat.dox][documentation]], c'est un format binaire et je ne suis pas
  sûr qu'il y ait moyen de générer de telles traces sans passer par
  leur bibliothèque. Si on veut de grosses traces (pour donner à
  manger à CODES), ça va être compliqué. Il aurait été pratique de les
  obtenir avec SimGrid (et le travail de Tom) mais il faudrait que
  SimGrid puisse générer du DUMPI.
- Après discussion rapide avec Lucas Schnorr, il est possible que
  DUMPI fonctionne avec SMPI puisque SMPI implémente PMPI sur lequel
  DUMPI se branche. Lucas avait réussi avec akypuera
  (http://github.com/schnorr/akypuera). On verra plus tard de toutes
  façons, il faut commencer par une petite trace.
- On n'a aucune idée de comment visualiser ou analyser les traces
  DUMPI. Je ne sais pas comment les convertir vers un autre
  format. Une possibilité serait de passer par [[https://github.com/hkustliqi/DUMPI_parser][ce parseur]] mais ça
  reste un pari.
- Il existe deux types de trâces, profiling (info de bases) ou une
  trâce complète (avec des outils comme DUMPI, EXTRAE, TAU/SCALASCA,
  OTF2 (format vers lequel tout le monde tend mais encore incomplet),
  POTI, ...
- DUMPI fait parti du projet SST, qui rassemble plusieurs projets
  - SST-MACRO : ressemble à SimGrid mais fonctionne en C++, et a des
    contraintes différentes. Il est encore en activité.
  - SST-Micro : pour simuler des tous petits systèmes de manière très
    détaillée.

À faire (sur ta machine pour commencer):
- Lancer HPL avec 12 processus (par exemple) et une matrice de taille
  20,000. Ne pas hésiter à aller voir Tom ou Christian si tu veux un
  fichier d'entrée pour HPL qui correspond à ceci.
- Suivre les informations de traçage de DUMPI (probablement à base de
  =LD_PRELOAD= et de =-ldumpi=) pour tracer HPL.
- Essayer de la rejouer sur ROSS.
**** Instalation DUMPI                                        :Deprecated:
Après le clonage du repo, il faut s'assurer d'avoir les bons outils :
- libtool
- m4
- automake
- autoconf
Une foi dans le dossier, il faut faire les commandes suivantes :
#+begin_src sh
./bootstrap.sh
mkdir build
cd build
../configure --enable-libdumpi --enable-test --prefix=$HOME/dumpi_inst CC=mpicc CXX=mpiCC
make
make install
make doc
#+end_src
J'ai cependant un problème, de nombreuses erreurs surviennent. En
effet, les signatures entre les .h de mpi et l'implémentation de DUMPI
diffèrent. Je vais donc voir si je dois prendre une version moins
récente de MPICH.
*** 2017-05-17 mercredi
**** DONE Installation de DUMPI/CODES                              :CODES:
En reregardant la doc de CODES, il y a justement un avertissement sur
DUMPI. 
"To enable network tracing with dumpi
    (http://sst.sandia.gov/about_dumpi.html), use the option
    --with-dumpi=/path/to/dumpi/install with configure.

    NOTE: we only require libundumpi for trace processing. Hence, if building
    dumpi from source you may configure with --disable-libdumpi and
    --enable-libundumpi (this is especially useful if you have mpich3, which
    breaks libdumpi's function wrappers through const'ifying the MPI
    interface)."

Du coup pour l'installation ca donne ca :
#+begin_src sh
./bootstrap.sh
mkdir build
cd build
../configure --disable-libdumpi --enable-libundumpi --prefix=/home/chevamax/Documents/Stage_LIG_2017/sst-dumpi/dumpi_inst CC=mpicc CXX=mpiCC
make
make install
make doc
#+end_src
EDIT : il faut ensuite ajouter le chemin vers ~/dumpi-inst/lib dans le
fichier /etc/ld.so.conf et faire un ldconfig

Du coup il faut que je réinstalle CODES avec les le chemin vers dumpi.

#+begin_src sh
./prepare.sh
mkdir build
cd build
apt-get install flex
apt-get install bison
apt-get install pkg-config
../configure --with-dumpi=/home/chevamax/Documents/Stage_LIG_2017/sst-dumpi/dumpi_inst --prefix=/home/chevamax/Documents/Stage_LIG_2017/CODES/install CC=mpicc PKG_CONFIG_PATH=../../ROSS/install/lib/pkgconfig
make && make install
make tests && make check
#+end_src

Maintenant les tests ne passent plus ...
#+BEGIN_EXAMPLE
tests/workload/codes-workload-test: error while loading shared libraries: libundumpi.so.3: cannot open shared object file: No such file or directory
FAIL tests/workload/codes-workload-test.sh (exit status: 127)
#+END_EXAMPLE

J'ai du louper une étape.
En cherchant sur internet j'ai vu qu'on pouvait ajouter des librairies
dans le fichier /etc/ld.so.conf En ajoutant le chemin vers la
librairie dumpi ca fonctionne.

**** Installation de HPL
lien : http://www.netlib.org/benchmark/hpl/software.html
inspiration de : [[https://www.howtoforge.com/tutorial/hpl-high-performance-linpack-benchmark-raspberry-pi/][installation raspberry pi]] [[http://www.crc.nd.edu/~rich/CRC_Summer_Scholars_2014/HPL-HowTo.pdf][How to sur cluster]]

#+begin_src sh
sudo apt-get install libatlas-base-dev libmpich2-dev gfortran
#+end_src

le paquet libmpich2-dev n'existe plus dans les repos, j'ai donc pris
libmpich-dev qui la remplace (il y a cependant une version :i386 qui
existe, je sais pas si elle optimise certain processeurs).

MPdir = /usr/lib/mpich

En suivant le tuto pour raspberry pi, j'ai un problème (surement de
configuration). Voici la trace de l'erreur pour la partie 4 du tuto :

#+BEGIN_EXAMPLE
make[2] : on entre dans le répertoire « /home/chevamax/Documents/Stage_LIG_2017/hpl-2.2/testing/ptest/linux »
mpif77  -o /home/chevamax/Documents/Stage_LIG_2017/hpl-2.2/bin/linux/xhpl HPL_pddriver.o         HPL_pdinfo.o           HPL_pdtest.o /home/chevamax/Documents/Stage_LIG_2017/hpl-2.2/lib/linux/libhpl.a  /usr/lib/atlas-base/libf77blas.a /usr/lib/atlas-base/libatlas.a -lblas /usr/lib/mpich/lib/libmpich.a
/usr/lib/mpich/lib/libmpich.a(lib_libmpich_la-initthread.o) : Dans la fonction « MPIR_Init_thread » :
(.text+0xb8) : référence indéfinie vers « pthread_mutexattr_init »
/usr/lib/mpich/lib/libmpich.a(lib_libmpich_la-initthread.o) : Dans la
fonction « MPIR_Init_thread » :
#+END_EXAMPLE

En ajoutant -pthread dans les flag on arrive à compiler.

Maintenant nouveau problème : 
#+Begin_Example
mpif77 -pthread -o /home/chevamax/Documents/Stage_LIG_2017/hpl-2.2/bin/linux/xhpl HPL_pddriver.o         HPL_pdinfo.o           HPL_pdtest.o /home/chevamax/Documents/Stage_LIG_2017/hpl-2.2/lib/linux/libhpl.a  /usr/lib/atlas-base/libf77blas.a /usr/lib/atlas-base/libatlas.a -lblas /usr/lib/mpich/lib/libmpich.a
/usr/lib/mpich/lib/libmpich.a(lib_libmpich_la-mpid_nem_ckpt.o) : Dans la fonction « ckpt_cb » :
(.text+0x342) : référence indéfinie vers « cr_checkpoint »
/usr/lib/mpich/lib/libmpich.a(lib_libmpich_la-mpid_nem_ckpt.o) : Dans la fonction « ckpt_cb » :
(.text+0x3e3) : référence indéfinie vers « cr_get_restart_info »
/usr/lib/mpich/lib/libmpich.a(lib_libmpich_la-mpid_nem_ckpt.o) : Dans la fonction « MPIDI_nem_ckpt_init » :
(.text+0xca5) : référence indéfinie vers « cr_init »
#+END_EXAMPLE

Je sais pas du tout qu'est ce qui manque. Est-ce que ça a un lien avec
BLCR ? Si oui il va falloir que je me lance dans l'installation.

*** 2017-05-18 jeudi

**** Mail Arnaud

1. Quand tu indiques "J'ai cependant un problème, de nombreuses
   erreurs surviennent. En effet, les signatures entre les .h de mpi
   et l'implémentation de DUMPI diffèrent. Je vais donc voir si je
   dois prendre une version moins récente de MPICH." Ça vaut le coup
   que tu indiques les erreurs (au moins les premières) dans une
   balise #+BEGIN_EXAMPLE   #+END_EXAMPLE
2. À propos de
   #+BEGIN_EXAMPLE
   tests/workload/codes-workload-test: error while loading shared
libraries: libundumpi.so.3:
   cannot open shared object file: No such file or directory FAIL
tests/workload/codes-workload-test.sh (exit status: 127)
   #+END_EXAMPLE
   Mettons que tu ais installé DUMPI dans /home/maxime/toto/lib (i.e.,
   le libundumpi.so est dans ce répertoire là).
   #+BEGIN_EXAMPLE
   export LD_LIBRARY_PATH=/home/maxime/toto/lib:$LD_LIBRARY_PATH
   #+END_EXAMPLE
   Ça devrait permettre au linker de trouver la bibliothèque. Modifier
   /etc/ld/so.conf me parait une mauvaise idée...
3. "Est-ce que ça a un lien avec BLCR ?" Ouh là, non, tu n'as pas
   besoin de ça. C'est une librairie de checkpoint qui permet de
   redémarer le code même quand on perd un noeud. Je ne sais pas d'où
   ça sort c'est anormal, tu ne devrais pas en avoir besoin.

**** Reprise installation HPL
En regardant sur internet, je vois que les fonctions font parties d'un
package blcr-util. Je tente donc de l'installer pour voir.
Effectivement ca ne résoud pas le soucis...
Et mpiexec prend en charge bclr c'est étrange.

***** Tentative d'installation sur une machine virtuelle
Je vais tenter d'installer HPL sur un linux vierge, pour voir si il
n'y a pas conflit avec autre chose de déjà installé.
J'ai exactement la même erreure...

Après plusieurs tests de configurations trouvé sur plusieurs tuto,
j'en ai finalement trouvé un qui fonctionne ! (sur la machine
virtuelle pour le moment). [[http://jahanzebnotes.blogspot.fr/2013/06/how-to-run-hpl-benchmark-with-atlas.html][lien]]

*** 2017-05-19 vendredi

**** Changement des paramètres sur ma machine
#+BEGIN_EXAMPLE
#  
#  -- High Performance Computing Linpack Benchmark (HPL)                
#     HPL - 2.2 - February 24, 2016                          
#     Antoine P. Petitet                                                
#     University of Tennessee, Knoxville                                
#     Innovative Computing Laboratory                                 
#     (C) Copyright 2000-2008 All Rights Reserved                       
#                                                                       
#  -- Copyright notice and Licensing terms:                             
#                                                                       
#  Redistribution  and  use in  source and binary forms, with or without
#  modification, are  permitted provided  that the following  conditions
#  are met:                                                             
#                                                                       
#  1. Redistributions  of  source  code  must retain the above copyright
#  notice, this list of conditions and the following disclaimer.        
#                                                                       
#  2. Redistributions in binary form must reproduce  the above copyright
#  notice, this list of conditions,  and the following disclaimer in the
#  documentation and/or other materials provided with the distribution. 
#                                                                       
#  3. All  advertising  materials  mentioning  features  or  use of this
#  software must display the following acknowledgement:                 
#  This  product  includes  software  developed  at  the  University  of
#  Tennessee, Knoxville, Innovative Computing Laboratory.             
#                                                                       
#  4. The name of the  University,  the name of the  Laboratory,  or the
#  names  of  its  contributors  may  not  be used to endorse or promote
#  products  derived   from   this  software  without  specific  written
#  permission.                                                          
#                                                                       
#  -- Disclaimer:                                                       
#                                                                       
#  THIS  SOFTWARE  IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
#  ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,  INCLUDING,  BUT NOT
#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY
#  OR  CONTRIBUTORS  BE  LIABLE FOR ANY  DIRECT,  INDIRECT,  INCIDENTAL,
#  SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL DAMAGES  (INCLUDING,  BUT NOT
#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
#  DATA OR PROFITS; OR BUSINESS INTERRUPTION)  HOWEVER CAUSED AND ON ANY
#  THEORY OF LIABILITY, WHETHER IN CONTRACT,  STRICT LIABILITY,  OR TORT
#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
# ######################################################################
#  
# ----------------------------------------------------------------------
# - shell --------------------------------------------------------------
# ----------------------------------------------------------------------
#
SHELL        = /bin/sh
#
CD           = cd
CP           = cp
LN_S         = ln -s
MKDIR        = mkdir
RM           = /bin/rm -f
TOUCH        = touch
#
# ----------------------------------------------------------------------
# - Platform identifier ------------------------------------------------
# ----------------------------------------------------------------------
#
ARCH         = linux
#
# ----------------------------------------------------------------------
# - HPL Directory Structure / HPL library ------------------------------
# ----------------------------------------------------------------------
#
TOPdir       = $(HOME)/Documents/Stage_LIG_2017/hpl
INCdir       = $(TOPdir)/include
BINdir       = $(TOPdir)/bin/$(ARCH)
LIBdir       = $(TOPdir)/lib/$(ARCH)
#
HPLlib       = $(LIBdir)/libhpl.a 
#
# ----------------------------------------------------------------------
# - Message Passing library (MPI) --------------------------------------
# ----------------------------------------------------------------------
# MPinc tells the  C  compiler where to find the Message Passing library
# header files,  MPlib  is defined  to be the name of  the library to be 
# used. The variable MPdir is only used for defining MPinc and MPlib.
#
MPdir        = /usr/lib/mpich
MPinc        = -I $(MPdir)/include
MPlib        = -L $(MPdir)/lib
#
# ----------------------------------------------------------------------
# - Linear Algebra library (BLAS or VSIPL) -----------------------------
# ----------------------------------------------------------------------
# LAinc tells the  C  compiler where to find the Linear Algebra  library
# header files,  LAlib  is defined  to be the name of  the library to be 
# used. The variable LAdir is only used for defining LAinc and LAlib.
#
LAdir        = /usr/lib/atlas-base
LAinc        = 
LAlib        = $(LAdir)/libf77blas.a $(LAdir)/libatlas.a -lblas
#
# ----------------------------------------------------------------------
# - F77 / C interface --------------------------------------------------
# ----------------------------------------------------------------------
# You can skip this section  if and only if  you are not planning to use
# a  BLAS  library featuring a Fortran 77 interface.  Otherwise,  it  is
# necessary  to  fill out the  F2CDEFS  variable  with  the  appropriate
# options.  **One and only one**  option should be chosen in **each** of
# the 3 following categories:
#
# 1) name space (How C calls a Fortran 77 routine)
#
# -DAdd_              : all lower case and a suffixed underscore  (Suns,
#                       Intel, ...),                           [default]
# -DNoChange          : all lower case (IBM RS6000),
# -DUpCase            : all upper case (Cray),
# -DAdd__             : the FORTRAN compiler in use is f2c.
#
# 2) C and Fortran 77 integer mapping
#
# -DF77_INTEGER=int   : Fortran 77 INTEGER is a C int,         [default]
# -DF77_INTEGER=long  : Fortran 77 INTEGER is a C long,
# -DF77_INTEGER=short : Fortran 77 INTEGER is a C short.
#
# 3) Fortran 77 string handling
#
# -DStringSunStyle    : The string address is passed at the string loca-
#                       tion on the stack, and the string length is then
#                       passed as  an  F77_INTEGER  after  all  explicit
#                       stack arguments,                       [default]
# -DStringStructPtr   : The address  of  a  structure  is  passed  by  a
#                       Fortran 77  string,  and the structure is of the
#                       form: struct {char *cp; F77_INTEGER len;},
# -DStringStructVal   : A structure is passed by value for each  Fortran
#                       77 string,  and  the  structure is  of the form:
#                       struct {char *cp; F77_INTEGER len;},
# -DStringCrayStyle   : Special option for  Cray  machines,  which  uses
#                       Cray  fcd  (fortran  character  descriptor)  for
#                       interoperation.
#
F2CDEFS      = -DAdd_ -DF77_INTEGER=int -DStringSunStyle
#
# ----------------------------------------------------------------------
# - HPL includes / libraries / specifics -------------------------------
# ----------------------------------------------------------------------
#
HPL_INCLUDES = -I$(INCdir) -I$(INCdir)/$(ARCH) $(LAinc) $(MPinc)
HPL_LIBS     = $(HPLlib) $(LAlib) $(MPlib) -lmpl
#
# - Compile time options -----------------------------------------------
#
# -DHPL_COPY_L           force the copy of the panel L before bcast;
# -DHPL_CALL_CBLAS       call the cblas interface;
# -DHPL_CALL_VSIPL       call the vsip  library;
# -DHPL_DETAILED_TIMING  enable detailed timers;
#
# By default HPL will:
#    *) not copy L before broadcast,
#    *) call the BLAS Fortran 77 interface,
#    *) not display detailed timing information.
#
HPL_OPTS     = -DHPL_CALL_CBLAS
# 
# ----------------------------------------------------------------------
#
HPL_DEFS     = $(F2CDEFS) $(HPL_OPTS) $(HPL_INCLUDES) 
#
# ----------------------------------------------------------------------
# - Compilers / linkers - Optimization flags ---------------------------
# ----------------------------------------------------------------------
#
CC           = /usr/bin/mpicc
CCNOOPT      = $(HPL_DEFS) 
CCFLAGS      = $(HPL_DEFS) -fomit-frame-pointer -O3 -funroll-loops
#
LINKER       = /usr/bin/mpicc
LINKFLAGS    = $(CCFLAGS) -pthread
#
ARCHIVER     = ar
ARFLAGS      = r
RANLIB       = echo
#
# ----------------------------------------------------------------------
#+END_EXAMPLE

Le soucis avait l'air de se trouver dans la configuration de LPlib. En
effet, dans tous les exemple vu c'était de la sorte :
~$(MPdir)/lib/libmpich.a~ hors en le changeant en ~MPlib= -L $(MPdir)/lib~
l'erreur disparait et le programme tourne.

**** Lancement de HPL
Je vais utiliser ce [[http://www.advancedclustering.com/act_kb/tune-hpl-dat-file/][site]] pour m'aider à faire les paramètres sur
ma machine.

***** Premier test
Pour voir si tout fonctionne je fais sur un seul thread.

****** HPL.dat
#+begin_example
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any)
6            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
14208        Ns
1            # of NBs
192          NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
1            Ps
1            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
1            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
#+end_example

avec la sortie suivante :
#+begin_example
================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       14208   192     1     1             201.68              9.482e+00
HPL_pdgesv() start time Fri May 19 10:17:35 2017

HPL_pdgesv() end time   Fri May 19 10:20:57 2017

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=        0.0018179 ...... PASSED
================================================================================

Finished      1 tests with the following results:
              1 tests completed and passed residual checks,
              0 tests completed and failed residual checks,
              0 tests skipped because of illegal input values.
--------------------------------------------------------------------------------

End of Tests.
================================================================================
#+end_example

***** Second test

#+begin_src sh :results output :exports both
lscpu
#+end_src

#+RESULTS:
#+begin_example
Architecture:          x86_64
Mode(s) opératoire(s) des processeurs :32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                4
On-line CPU(s) list:   0-3
Thread(s) par cœur : 2
Cœur(s) par socket : 2
Socket(s):             1
Nœud(s) NUMA :       1
Identifiant constructeur :GenuineIntel
Famille de processeur :6
Modèle :             69
Model name:            Intel(R) Core(TM) i5-4210U CPU @ 1.70GHz
Révision :           1
Vitesse du processeur en MHz :1853.613
CPU max MHz:           2700,0000
CPU min MHz:           800,0000
BogoMIPS:              4789.03
Virtualisation :      VT-x
Cache L1d :           32K
Cache L1i :           32K
Cache L2 :            256K
Cache L3 :            3072K
NUMA node0 CPU(s):     0-3
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts
#+end_example

J'ai donc un socket, avec deux coeurs et 2 threads par coeurs. Je
pense donc pouvoir faire peut être du 2x2

J'ai 8Go de RAM. Ils conseillent d'en alouer au maximum 80% (le reste
pour l'OS &co).

Je vais donc faire le test avec ce fichier :
#+BEGIN_EXAMPLE
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any) 
6            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
18048         Ns
1            # of NBs
192           NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
2            Ps
2            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
1            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
#+END_EXAMPLE

#+begin_src sh :results output :exports both
mpiexec -n 4 ./xhpl 
#+end_src

#+RESULTS:
#+BEGIN_EXAMPLE
================================================================================
HPLinpack 2.2  --  High-Performance Linpack benchmark  --   February 24, 2016
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :   18048 
NB     :     192 
PMAP   : Row-major process mapping
P      :       2 
Q      :       2 
PFACT  :   Right 
NBMIN  :       4 
NDIV   :       2 
RFACT  :   Crout 
BCAST  :  1ringM 
DEPTH  :       1 
SWAP   : Mix (threshold = 64)
L1     : transposed form
U      : transposed form
EQUIL  : yes
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0

================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4       18048   192     2     2             309.73              1.265e+01
HPL_pdgesv() start time Fri May 19 10:59:50 2017

HPL_pdgesv() end time   Fri May 19 11:04:59 2017

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=        0.0014193 ...... PASSED
================================================================================

Finished      1 tests with the following results:
              1 tests completed and passed residual checks,
              0 tests completed and failed residual checks,
              0 tests skipped because of illegal input values.
--------------------------------------------------------------------------------

End of Tests.
================================================================================
#+END_EXAMPLE

C'est la première fois que je vois mes 4 "CPU" à 100% !

Entre les deux tests je suis passé de 9.482 à 12.65 Gflops.
Je pense qu'il est possible d'augmenter encoreun peu, mais le but est
de faire le lien avec DUMPI.


**** Lien avec DUMPI
Il semblerais que j'ai installé seulement undumpi, vu que dumpi ne
fonctionne pas avec les dernières version de mpich...
J'ai peut être trouvé un moyen : [[https://xgitlab.cels.anl.gov/codes/codes/blob/db787d9f77786b8682ab8746942e3e6037cacfd7/doc/BUILD_STEPS][ici]]
- DUMPI :
Je décide de l'installer sur ma machine virtuelle (vu que mon dumpi
est installé seulement avec undumpi sur ma machine et qu'il est lié à CODES).

#+begin_src sh
../configure --enable-libdumpi --prefix=/home/chevamax/sst-dumpi/install CC=mpicc CXX=mpiCC CFLAGS="-DMPICH_SUPPRESS_PROTOTYPES=1 -DHAVE_PRAGMA_HP_SEC_DEF=1"
#+end_src
Permet de passe le make

TODO : faire un test avec mpi tuto helloworld.
#+BEGIN_EXAMPLE
Linking with C/C++

We can use libdumpi that we generated above:

user@machine:~/play/mpi$ mpicc hello.c -c
user@machine:~/play/mpi$ mpicc hello.o -L$HOME/dumpi_inst/lib -ldumpi -o hello
user@machine:~/play/mpi$ LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/dumpi_inst/lib ./hello 
Hello!
user@machine:~/play/mpi$ ls
dumpi-2011.01.25.15.46.10-0000.bin  hello    hello.o
dumpi-2011.01.25.15.46.10.meta      hello.c

In this instance there is a single MPI rank created. There are two files created: a metafile and a file for our MPI rank zero. The date and time are encoded into the filename to prevent accidental overwrite of DUMPI trace files.

We can print the trace file using dumpi2ascii to verify a simple 'hello world' that does, an init, comm_rank, comm_size, and a finalize as its only MPI calls:

user@machine:~/play/mpi$ $HOME/dumpi_inst/bin/dumpi2ascii dumpi-2011.01.25.15.46.10-0000.bin 
MPI_Init entering at walltime 20120.589738643, cputime 0.023382031 seconds in thread 0.
int argc=1
string argv[1]=["./hello"]
MPI_Init returning at walltime 20120.589752472, cputime 0.023385336 seconds in thread 0.
MPI_Comm_rank entering at walltime 20120.589793260, cputime 0.023437127 seconds in thread 0.
MPI_Comm comm=2 (MPI_COMM_WORLD)
int rank=0
MPI_Comm_rank returning at walltime 20120.589796683, cputime 0.023440692 seconds in thread 0.
MPI_Comm_size entering at walltime 20120.589810022, cputime 0.023453983 seconds in thread 0.
MPI_Comm comm=2 (MPI_COMM_WORLD)
int size=1
MPI_Comm_size returning at walltime 20120.589813095, cputime 0.023457038 seconds in thread 0.
MPI_Finalize entering at walltime 20120.589841172, cputime 0.023484981 seconds in thread 0.
MPI_Finalize returning at walltime 20120.590067322, cputime 0.023710703 seconds in thread 0.

Libdumpi uses the PMPI interface, so you shouldn't have to use any included header files, or make any subroutine/function calls to DUMPI for most use cases of DUMPI. Of course, when you use libdumpi, you can't use or link against any other tools that use the PMPI interface in the same executable.

By default, DUMPI will emit trace information for every MPI call, with a timestamp for entry and exit to each routine.
#+END_EXAMPLE

En suivant la méthode du dessus, j'ai été capable de générer des
traces dumpi pour un programme helloWorld disponible [[http://mpitutorial.com/tutorials/mpi-hello-world/][ici]].

***** HPL + DUMPI
En ajoutant ~-L path/to/dumpi_inst/lib -ldumpi~ à la fin de la ligne
MPlib dans le Make.linux de HPL, j'ai réussi à générer une trâce!
/!\ Ca ne fonctionne pas si on met cette ligne sur *HPL_LIBS* ou sur le *LINKFLAGS*

Je n'ai pas encore compris comment on utilise =LD_PRELOAD= j'ai donc
compilé directement HPL avec le lien DUMPI. Je pense donc installer
deux versions, une avec les TRACES et l'autre sans. Christian m'a dit
de faire un lien entre les deux dossiers pour s'assurer que le HPL.dat
soit toujours le même.
***** Rencontre avec Christian
Florence m'a présenté Christian aujourd'hui qui travail sur
SimGrid. Il est très intéréssé par l'objectif de mon stage et aimerait
ajouter SST à la comparaison. En effet, il a écrit un papier sur ses
traveaux et on lui a reproché de ne pas avoir de comparaison avec
SST. J'espère avancer vite pour qu'il puisse se servir de mon travail!

***** Recherche
En faisant des recherche pour installer dumpi, j'ai vu qu'il y a un
convertisseur vers l'ASCII, mais aussi un convertisseur (experimental)
vers du DOT.



